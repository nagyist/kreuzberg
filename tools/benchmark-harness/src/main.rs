//! Benchmark harness CLI

#[cfg(feature = "memory-profiling")]
#[global_allocator]
static ALLOC: tikv_jemallocator::Jemalloc = tikv_jemallocator::Jemalloc;

use benchmark_harness::{BenchmarkConfig, BenchmarkMode, FixtureManager, Result};
use clap::{Parser, Subcommand, ValueEnum};
use std::path::PathBuf;

/// CLI enum for benchmark mode
#[derive(Debug, Clone, Copy, ValueEnum)]
enum CliMode {
    /// Single-file mode: Sequential execution for fair latency comparison
    SingleFile,
    /// Batch mode: Concurrent execution for throughput measurement
    Batch,
}

/// CLI enum for output format
#[derive(Debug, Clone, Copy, ValueEnum)]
enum OutputFormat {
    /// JSON format (default)
    Json,
    /// HTML format with interactive visualizations
    Html,
    /// Both JSON and HTML formats
    Both,
}

impl From<CliMode> for BenchmarkMode {
    fn from(mode: CliMode) -> Self {
        match mode {
            CliMode::SingleFile => BenchmarkMode::SingleFile,
            CliMode::Batch => BenchmarkMode::Batch,
        }
    }
}

#[derive(Parser)]
#[command(name = "benchmark-harness")]
#[command(about = "Benchmark harness for document extraction frameworks", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// List all fixtures from a directory
    ListFixtures {
        /// Directory or file pattern to search for fixtures
        #[arg(short, long)]
        fixtures: PathBuf,
    },

    /// Validate fixtures without running benchmarks
    Validate {
        /// Directory or file pattern to search for fixtures
        #[arg(short, long)]
        fixtures: PathBuf,
    },

    /// Generate an HTML index gallery for flamegraphs
    GenerateFlamegraphIndex {
        /// Directory containing flamegraph SVG files
        #[arg(long)]
        flamegraphs: PathBuf,

        /// Output path for the HTML gallery file
        #[arg(long)]
        output: PathBuf,
    },

    /// Run benchmarks
    Run {
        /// Directory or file pattern to search for fixtures
        #[arg(short, long)]
        fixtures: PathBuf,

        /// Frameworks to benchmark (comma-separated)
        #[arg(short = 'F', long, value_delimiter = ',')]
        frameworks: Vec<String>,

        /// Output directory for results
        #[arg(short, long, default_value = "results")]
        output: PathBuf,

        /// Maximum concurrent extractions
        #[arg(short = 'c', long)]
        max_concurrent: Option<usize>,

        /// Timeout in seconds
        #[arg(short = 't', long)]
        timeout: Option<u64>,

        /// Benchmark mode: single-file (sequential) or batch (concurrent)
        #[arg(short = 'm', long, value_enum, default_value = "batch")]
        mode: CliMode,

        /// Number of warmup iterations (discarded from statistics)
        #[arg(short = 'w', long, default_value = "1")]
        warmup: usize,

        /// Number of benchmark iterations for statistical analysis
        #[arg(short = 'i', long, default_value = "3")]
        iterations: usize,

        /// Enable OCR for image extraction
        #[arg(long, default_value = "true")]
        ocr: bool,

        /// Enable quality assessment
        #[arg(long, default_value = "true")]
        measure_quality: bool,

        /// Output format: json, html, or both
        #[arg(long, value_enum, default_value = "json")]
        format: OutputFormat,

        /// Benchmark execution date (e.g., "2025-12-13 14:30:00 UTC")
        /// Used for marking when the benchmark was run in the HTML output
        #[arg(long)]
        benchmark_date: Option<String>,
    },

    /// Consolidate multiple benchmark runs
    Consolidate {
        /// Input directories containing benchmark results
        #[arg(short, long, value_delimiter = ',')]
        inputs: Vec<PathBuf>,

        /// Output directory for consolidated results
        #[arg(short, long)]
        output: PathBuf,

        /// Output format: json, html, or both
        #[arg(long, value_enum, default_value = "both")]
        format: OutputFormat,

        /// Baseline framework for delta calculations (not used but provided for compatibility)
        #[arg(long, default_value = "kreuzberg-native")]
        baseline: String,
    },

    /// Visualize benchmark results from existing runs
    Visualize {
        /// Input directories containing benchmark results
        #[arg(short, long, value_delimiter = ',')]
        inputs: Vec<PathBuf>,

        /// Output directory for visualization assets
        #[arg(short, long)]
        output: PathBuf,

        /// Output format: json, html, or both
        #[arg(long, value_enum, default_value = "html")]
        format: OutputFormat,

        /// Benchmark execution date (e.g., "2025-12-13 14:30:00 UTC")
        #[arg(long)]
        benchmark_date: Option<String>,
    },
}

#[tokio::main]
async fn main() -> Result<()> {
    let cli = Cli::parse();

    match cli.command {
        Commands::ListFixtures { fixtures } => {
            let mut manager = FixtureManager::new();

            if fixtures.is_dir() {
                manager.load_fixtures_from_dir(&fixtures)?;
            } else {
                manager.load_fixture(&fixtures)?;
            }

            println!("Loaded {} fixture(s)", manager.len());
            for (path, fixture) in manager.fixtures() {
                println!(
                    "  {} - {} ({} bytes)",
                    path.display(),
                    fixture.document.display(),
                    fixture.file_size
                );
            }

            Ok(())
        }

        Commands::Validate { fixtures } => {
            let mut manager = FixtureManager::new();

            if fixtures.is_dir() {
                manager.load_fixtures_from_dir(&fixtures)?;
            } else {
                manager.load_fixture(&fixtures)?;
            }

            println!("✓ All {} fixture(s) are valid", manager.len());
            Ok(())
        }

        Commands::GenerateFlamegraphIndex { flamegraphs, output } => {
            use benchmark_harness::generate_flamegraph_index;
            generate_flamegraph_index(&flamegraphs, &output)?;
            println!("✓ Flamegraph index generated: {}", output.display());
            Ok(())
        }

        Commands::Run {
            fixtures,
            frameworks,
            output,
            max_concurrent,
            timeout,
            mode,
            warmup,
            iterations,
            ocr,
            measure_quality,
            format,
            benchmark_date,
        } => {
            use benchmark_harness::{AdapterRegistry, BenchmarkRunner, NativeAdapter};
            use kreuzberg::{ExtractionConfig, OcrConfig};
            use std::sync::Arc;

            let config = BenchmarkConfig {
                output_dir: output.clone(),
                max_concurrent: max_concurrent.unwrap_or_else(num_cpus::get),
                timeout: std::time::Duration::from_secs(timeout.unwrap_or(1800)),
                benchmark_mode: mode.into(),
                warmup_iterations: warmup,
                benchmark_iterations: iterations,
                measure_quality,
                ..Default::default()
            };

            config.validate()?;

            let mut extraction_config = if ocr {
                ExtractionConfig {
                    ocr: Some(OcrConfig {
                        backend: "tesseract".to_string(),
                        language: "eng".to_string(),
                        tesseract_config: None,
                    }),
                    ..Default::default()
                }
            } else {
                ExtractionConfig::default()
            };
            extraction_config.max_concurrent_extractions = Some(config.max_concurrent);

            let mut registry = AdapterRegistry::new();

            // Helper to check if a framework should be initialized
            // When specific frameworks are requested, only initialize those
            // When no frameworks are specified (empty list), initialize all available
            let should_init = |name: &str| -> bool { frameworks.is_empty() || frameworks.iter().any(|f| f == name) };

            // Helper macro for registering adapters conditionally
            macro_rules! try_register {
                ($name:expr, $create_fn:expr, $count:expr) => {
                    if should_init($name) {
                        match $create_fn() {
                            Ok(adapter) => {
                                if let Err(err) = registry.register(Arc::new(adapter)) {
                                    eprintln!("[adapter] ✗ {} (registration failed: {})", $name, err);
                                } else {
                                    eprintln!("[adapter] ✓ {} (registered)", $name);
                                    $count += 1;
                                }
                            }
                            Err(err) => eprintln!("[adapter] ✗ {} (initialization failed: {})", $name, err),
                        }
                    }
                };
            }

            // Always register native adapter if requested or no specific frameworks specified
            let mut kreuzberg_count = 0;
            if should_init("kreuzberg-native") {
                registry.register(Arc::new(NativeAdapter::with_config(extraction_config)))?;
                eprintln!("[adapter] ✓ kreuzberg-native (registered)");
                kreuzberg_count += 1;
            }

            use benchmark_harness::adapters::{
                create_csharp_sync_adapter, create_elixir_batch_adapter, create_elixir_sync_adapter,
                create_go_batch_adapter, create_go_sync_adapter, create_java_sync_adapter, create_node_async_adapter,
                create_node_batch_adapter, create_php_batch_adapter, create_php_sync_adapter,
                create_python_async_adapter, create_python_batch_adapter, create_python_sync_adapter,
                create_ruby_batch_adapter, create_ruby_sync_adapter, create_wasm_async_adapter,
                create_wasm_batch_adapter,
            };

            try_register!("kreuzberg-python-sync", create_python_sync_adapter, kreuzberg_count);
            try_register!("kreuzberg-python-async", create_python_async_adapter, kreuzberg_count);
            try_register!("kreuzberg-python-batch", create_python_batch_adapter, kreuzberg_count);
            try_register!("kreuzberg-go-sync", create_go_sync_adapter, kreuzberg_count);
            try_register!("kreuzberg-go-batch", create_go_batch_adapter, kreuzberg_count);
            try_register!("kreuzberg-node-async", create_node_async_adapter, kreuzberg_count);
            try_register!("kreuzberg-node-batch", create_node_batch_adapter, kreuzberg_count);
            try_register!("kreuzberg-wasm-async", create_wasm_async_adapter, kreuzberg_count);
            try_register!("kreuzberg-wasm-batch", create_wasm_batch_adapter, kreuzberg_count);
            try_register!("kreuzberg-ruby-sync", create_ruby_sync_adapter, kreuzberg_count);
            try_register!("kreuzberg-ruby-batch", create_ruby_batch_adapter, kreuzberg_count);
            try_register!("kreuzberg-java-sync", create_java_sync_adapter, kreuzberg_count);
            try_register!("kreuzberg-csharp-sync", create_csharp_sync_adapter, kreuzberg_count);
            try_register!("kreuzberg-php-sync", create_php_sync_adapter, kreuzberg_count);
            try_register!("kreuzberg-php-batch", create_php_batch_adapter, kreuzberg_count);
            try_register!("kreuzberg-elixir-sync", create_elixir_sync_adapter, kreuzberg_count);
            try_register!("kreuzberg-elixir-batch", create_elixir_batch_adapter, kreuzberg_count);

            let total_requested = if frameworks.is_empty() { 18 } else { frameworks.len() };
            eprintln!(
                "[adapter] Kreuzberg bindings: {}/{} available",
                kreuzberg_count, total_requested
            );

            use benchmark_harness::adapters::{
                create_docling_adapter, create_docling_batch_adapter, create_markitdown_adapter, create_mineru_adapter,
                create_mineru_batch_adapter, create_pandoc_adapter, create_pdfplumber_adapter,
                create_pdfplumber_batch_adapter, create_pymupdf4llm_adapter, create_tika_batch_adapter,
                create_tika_sync_adapter, create_unstructured_adapter,
            };

            let mut external_count = 0;

            try_register!("docling", create_docling_adapter, external_count);
            try_register!("docling-batch", create_docling_batch_adapter, external_count);
            try_register!("markitdown", create_markitdown_adapter, external_count);
            try_register!("pandoc", create_pandoc_adapter, external_count);
            try_register!("unstructured", create_unstructured_adapter, external_count);
            try_register!("tika-sync", create_tika_sync_adapter, external_count);
            try_register!("tika-batch", create_tika_batch_adapter, external_count);
            try_register!("pymupdf4llm", create_pymupdf4llm_adapter, external_count);
            try_register!("pdfplumber", create_pdfplumber_adapter, external_count);
            try_register!("pdfplumber-batch", create_pdfplumber_batch_adapter, external_count);
            try_register!("mineru", create_mineru_adapter, external_count);
            try_register!("mineru-batch", create_mineru_batch_adapter, external_count);

            eprintln!(
                "[adapter] Open source extraction frameworks: {}/12 available",
                external_count
            );
            eprintln!(
                "[adapter] Total adapters: {} available",
                kreuzberg_count + external_count
            );

            let mut runner = BenchmarkRunner::new(config, registry);
            runner.load_fixtures(&fixtures)?;

            println!("Loaded {} fixture(s)", runner.fixture_count());
            println!("Frameworks: {:?}", frameworks);
            println!("Configuration: {:?}", runner.config());

            if runner.fixture_count() == 0 {
                println!("No fixtures to benchmark");
                return Ok(());
            }

            println!("\nRunning benchmarks...");
            let results = runner.run(&frameworks).await?;

            println!("\nCompleted {} benchmark(s)", results.len());

            let mut success_count = 0;
            let mut failure_count = 0;

            for result in &results {
                if result.success {
                    success_count += 1;
                } else {
                    failure_count += 1;
                }
            }

            println!("\nSummary:");
            println!("  Successful: {}", success_count);
            println!("  Failed: {}", failure_count);
            println!("  Total: {}", results.len());

            use benchmark_harness::{write_by_extension_analysis, write_html, write_json};

            match format {
                OutputFormat::Json => {
                    let output_file = output.join("results.json");
                    write_json(&results, &output_file)?;
                    println!("\nResults written to: {}", output_file.display());

                    let by_ext_file = output.join("by-extension.json");
                    write_by_extension_analysis(&results, &by_ext_file)?;
                    println!("Per-extension analysis written to: {}", by_ext_file.display());
                }
                OutputFormat::Html => {
                    let html_file = output.join("index.html");
                    write_html(&results, &html_file, benchmark_date.as_deref())?;
                    println!("\nHTML report written to: {}", html_file.display());
                }
                OutputFormat::Both => {
                    let output_file = output.join("results.json");
                    write_json(&results, &output_file)?;
                    println!("\nResults written to: {}", output_file.display());

                    let by_ext_file = output.join("by-extension.json");
                    write_by_extension_analysis(&results, &by_ext_file)?;
                    println!("Per-extension analysis written to: {}", by_ext_file.display());

                    let html_file = output.join("index.html");
                    write_html(&results, &html_file, benchmark_date.as_deref())?;
                    println!("HTML report written to: {}", html_file.display());
                }
            }

            Ok(())
        }
        Commands::Consolidate {
            inputs,
            output,
            format,
            baseline: _baseline,
        } => {
            use benchmark_harness::{consolidate_runs, load_run_results, write_consolidated_json};

            if inputs.is_empty() {
                return Err(benchmark_harness::Error::Benchmark(
                    "No input directories specified".to_string(),
                ));
            }

            println!("Loading benchmark results from {} directory(ies)...", inputs.len());
            let mut all_runs = Vec::new();

            for input in &inputs {
                if !input.is_dir() {
                    return Err(benchmark_harness::Error::Benchmark(format!(
                        "Input path is not a directory: {}",
                        input.display()
                    )));
                }
                println!("  Loading from: {}", input.display());
                let run_results = load_run_results(input)?;
                println!("    Loaded {} results", run_results.len());
                all_runs.push(run_results);
            }

            println!("\nConsolidating {} run(s)...", all_runs.len());
            let consolidated = consolidate_runs(all_runs)?;

            println!("\nConsolidation Summary:");
            println!("  Total files processed: {}", consolidated.total_files);
            println!("  Number of runs: {}", consolidated.run_count);
            println!("  Frameworks analyzed: {}", consolidated.framework_count);

            eprintln!("\nFramework Summary:");
            for (framework, agg) in &consolidated.by_framework {
                eprintln!("  {}:", framework);
                eprintln!("    Files processed: {}", agg.total_files);
                eprintln!("    Mean duration: {:.2} ms", agg.mean_duration_ms);
                eprintln!("    Std dev: {:.2} ms", agg.duration_std_dev_ms);
                eprintln!("    Success rate: {:.1}%", agg.success_rate * 100.0);
            }

            std::fs::create_dir_all(&output).map_err(benchmark_harness::Error::Io)?;

            match format {
                OutputFormat::Json => {
                    let output_file = output.join("consolidated.json");
                    write_consolidated_json(&consolidated, &output_file)?;
                    println!("\nConsolidated results written to: {}", output_file.display());
                }
                OutputFormat::Html => {
                    let html_file = output.join("consolidated.html");
                    write_simple_html(&consolidated, &html_file)?;
                    println!("\nConsolidated HTML report written to: {}", html_file.display());
                }
                OutputFormat::Both => {
                    let output_file = output.join("consolidated.json");
                    write_consolidated_json(&consolidated, &output_file)?;
                    println!("\nConsolidated results written to: {}", output_file.display());

                    let html_file = output.join("consolidated.html");
                    write_simple_html(&consolidated, &html_file)?;
                    println!("Consolidated HTML report written to: {}", html_file.display());
                }
            }

            Ok(())
        }

        Commands::Visualize {
            inputs,
            output,
            format,
            benchmark_date,
        } => {
            use benchmark_harness::{load_run_results, write_by_extension_analysis, write_html, write_json};

            if inputs.is_empty() {
                return Err(benchmark_harness::Error::Benchmark(
                    "No input directories specified".to_string(),
                ));
            }

            let mut results = Vec::new();
            for input in &inputs {
                if !input.is_dir() {
                    return Err(benchmark_harness::Error::Benchmark(format!(
                        "Input path is not a directory: {}",
                        input.display()
                    )));
                }
                let mut run_results = load_run_results(input)?;
                results.append(&mut run_results);
            }

            if results.is_empty() {
                return Err(benchmark_harness::Error::Benchmark(
                    "No benchmark results found to visualize".to_string(),
                ));
            }

            std::fs::create_dir_all(&output).map_err(benchmark_harness::Error::Io)?;

            match format {
                OutputFormat::Json => {
                    let output_file = output.join("results.json");
                    write_json(&results, &output_file)?;
                    println!("\nResults written to: {}", output_file.display());

                    let by_ext_file = output.join("by-extension.json");
                    write_by_extension_analysis(&results, &by_ext_file)?;
                    println!("Per-extension analysis written to: {}", by_ext_file.display());
                }
                OutputFormat::Html => {
                    let html_file = output.join("index.html");
                    write_html(&results, &html_file, benchmark_date.as_deref())?;
                    println!("\nHTML report written to: {}", html_file.display());
                }
                OutputFormat::Both => {
                    let output_file = output.join("results.json");
                    write_json(&results, &output_file)?;
                    println!("\nResults written to: {}", output_file.display());

                    let by_ext_file = output.join("by-extension.json");
                    write_by_extension_analysis(&results, &by_ext_file)?;
                    println!("Per-extension analysis written to: {}", by_ext_file.display());

                    let html_file = output.join("index.html");
                    write_html(&results, &html_file, benchmark_date.as_deref())?;
                    println!("HTML report written to: {}", html_file.display());
                }
            }

            Ok(())
        }
    }
}

/// Write simple consolidated HTML report
fn write_simple_html(consolidated: &benchmark_harness::ConsolidatedResults, path: &PathBuf) -> Result<()> {
    let mut html = String::from(
        r#"<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Consolidated Benchmark Report</title>
    <style>
        body { font-family: system-ui, -apple-system, sans-serif; margin: 20px; line-height: 1.6; }
        h1 { color: #333; border-bottom: 2px solid #0066cc; padding-bottom: 10px; }
        h2 { color: #0066cc; margin-top: 30px; }
        table { border-collapse: collapse; width: 100%; margin: 15px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background-color: #0066cc; color: white; }
        tr:nth-child(even) { background-color: #f9f9f9; }
        .summary { background-color: #e7f3ff; padding: 15px; border-radius: 5px; margin: 15px 0; }
    </style>
</head>
<body>
    <h1>Consolidated Benchmark Report</h1>
"#,
    );

    html.push_str(&format!(
        r#"    <div class="summary">
        <h3>Summary</h3>
        <p>Total files processed: <strong>{}</strong></p>
        <p>Number of runs: <strong>{}</strong></p>
        <p>Frameworks analyzed: <strong>{}</strong></p>
    </div>
"#,
        consolidated.total_files, consolidated.run_count, consolidated.framework_count
    ));

    html.push_str("<h2>Framework Performance</h2>\n");
    html.push_str("<table>\n");
    html.push_str("    <tr><th>Framework</th><th>Files</th><th>Avg Duration (ms)</th><th>Std Dev (ms)</th><th>Success Rate</th><th>Avg Throughput (MB/s)</th></tr>\n");

    for (framework, agg) in &consolidated.by_framework {
        html.push_str(&format!(
            r#"    <tr>
        <td>{}</td>
        <td>{}</td>
        <td>{:.2}</td>
        <td>{:.2}</td>
        <td>{:.1}%</td>
        <td>{:.2}</td>
    </tr>
"#,
            framework,
            agg.total_files,
            agg.mean_duration_ms,
            agg.duration_std_dev_ms,
            agg.success_rate * 100.0,
            agg.mean_throughput_bps / 1_000_000.0
        ));
    }

    html.push_str("</table>\n");
    html.push_str("</body>\n</html>");

    std::fs::write(path, html).map_err(benchmark_harness::Error::Io)?;
    Ok(())
}
