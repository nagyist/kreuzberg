TableFormer: Table Structure Understanding with Transformers

Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, Peter Staar
IBM Research
{ahn,nli,mly,taa}@zurich.ibm.com

Abstract

Tables organize valuable content in a concise and compact representation. This content is extremely valuable for systems such as search engines, Knowledge Graph's, etc. since they enhance their predictive capabilities. Unfortunately, tables come in a large variety of shapes and sizes. Furthermore, they can have complex column/row-header configurations, multi-line rows, altered variety of separation lines, missing entries, etc. As such, the correct identification of the table structure from an image is a non-trivial task. In this paper, we present a new table-structure identification model. The latter improves the latest end-to-end deep learning model (i.e. encoder-and decoder from PubTabNet) in two significant ways. First, we introduce a new object detection decoder for table-cells. In this way, we can obtain the content of the table-cells from programmatic PDF's directly from the PDF source and avoid the training of the usual OCR decoders. This architectural change leads to more accurate table-content extraction and allows us to handle non-english tables. Second, we replace the LSTM decoders with transformer based decoders. This upgrade improves significantly the previous state-of-the-art tree-editing-distance-score (TEDS) from 91% to 98.5% on simple tables and from 88.7% to 95% on complex tables.

1. Introduction

The occurrence of tables in documents is ubiquitous. They often summarize quantitative or factual data, which is cumbersome to describe in verbose text but nevertheless extremely valuable. Unfortunately, this compact representation is often not easy to parse by machines. There are many implicit conventions used to obtain a compact table representation. For example, tables often have complex column-and row-headers in order to reduce duplicated cell content. Lines of different shapes and sizes are leveraged to separate content or indicate a tree structure. Additionally, tables can also have empty/missing table-entries or multi-row textual table-entries. Fig. 1 shows a table which presents all these issues.

Recently, significant progress has been made with vision based approaches to extract tables in documents. For the sake of completeness, the issue of table extraction from documents is typically decomposed into two separate challenges, i.e. (1) finding the location of the table(s) on a document-page and (2) finding the structure of a given table in the document.

The first problem is called table-location and has been previously addressed [40, 38, 19, 21, 23, 26, 8] with state-of-the-art object-detection networks (e.g. YOLO and later on Mask-RCNN [9]). For all practical purposes, it can be considered as a solved problem, given enough ground-truth data to train on.

The second problem is called table-structure decomposition. The latter is a long-standing problem in the document understanding community [6, 4, 14]. Contrary to the table-location problem, there are no commonly used approaches that can easily be re-proposed to solve this problem. Lately, a set of new model-architectures have been proposed by the community to address table-structure decomposition [37, 36, 18]. All these models have some weaknesses (See. 2). The common denominator here is the reliance on textual features and/or the inability to provide the bounding box of each table-cell in the original image.

In this paper we want to address these weaknesses and present a robust table-structure decomposition algorithm. The design criteria for our model are the following. First, we want our algorithm to be language agnostic. In this way, we can obtain the structure of any table, irrespective of the language. Second, we want our algorithm to leverage as much data as possible from the original PDF document. For pragmatic PDF documents, the text-cells can often be extracted much faster and with higher accuracy compared to OCR methods. Last but not least, we want to have a direct link between the table-cell and its bounding box in the original image.

To meet the design criteria listed above, we developed a new model called TableFormer and a synthetically generated table structure dataset called SynthTabNet. In particular, our contributions in this work can be summarised as follows:

• We propose TableFormer, a transformer based model that predicts table structure and bounding boxes for the table content simultaneously in an end-to-end approach.

• Across all benchmark datasets TableFormer significantly outperforms existing state-of-the-art metrics, while being much more efficient in training and inference to existing works.

• We present SynthTabNet a synthetically generated dataset, with various appearance styles and complexity.

• An augmented dataset based on PubTabNet [37], FinTabNet [36], and TableBank [17] with generated ground-truth for reproducibility.

The paper is structured as follows. In Sec. 2, we give a brief overview of the current state-of-the-art. In Sec. 3, we describe the datasets on which we train. In Sec. 4, we introduce the TableFormer model-architecture and describe the model. In Sec. 5, we describe how the model can be re-purposed for other tasks in the computer-vision community.

https://github.com/IBM/SynthTabNet

considered as a solved problem, given enough ground-truth data to train on.

The second problem is called table-structure decomposition. The latter is a long standing problem in the document understanding community [6, 4, 14]. Contrary to the table-location problem, there are no commonly used approaches that can easily be re-proposed to solve this problem. Lately, a set of new model-architectures have been proposed by the community to address table-structure decomposition [37, 36, 18]. All these models have some weaknesses (See. 2). The common denominator here is the reliance on textual features and/or the inability to provide the bounding box of each table-cell in the original image.

In this paper we want to address these weaknesses and present a robust table-structure decomposition algorithm. The design criteria for our model are the following. First, we want our algorithm to be language agnostic. In this way, we can obtain the structure of any table, irrespective of the language. Second, we want our algorithm to leverage as much data as possible from the original PDF document. For pragmatic PDF documents, the text-cells can often be extracted much faster and with higher accuracy compared to OCR methods. Last but not least, we want to have a direct link between the table-cell and its bounding box in the original image.

To meet the design criteria listed above, we developed a new model called TableFormer and a synthetically generated table structure dataset called SynthTabNet. In particular, our contributions in this work can be summarised as follows:

• We propose TableFormer, a transformer based model that predicts table structure and bounding boxes for the table content simultaneously in an end-to-end approach.

• Across all benchmark datasets TableFormer significantly outperforms existing state-of-the-art metrics, while being much more efficient in training and inference to existing works.

• We present SynthTabNet a synthetically generated dataset, with various appearance styles and complexity.

• An augmented dataset based on PubTabNet [37], FinTabNet [36], and TableBank [17] with generated ground-truth for reproducibility.

The paper is structured as follows. In Sec. 2, we give a brief overview of the current state-of-the-art. In Sec. 3, we describe the datasets on which we train. In Sec. 4, we introduce the TableFormer model-architecture and describe the model. In Sec. 5, we describe how the model can be re-purposed for other tasks in the computer-vision community.

2. Previous work and State of the Art

Identifying the structure of a table has been an outstanding problem in the document understanding community, that motivates many organised public challenges [6, 4, 14]. The difficulty of the problem can be attributed to a number of factors. First, there is a large variety in the shapes and sizes of tables. Such large variety requires a flexible method. This is especially true for complex column- and row headers, which can be extremely intricate and demanding. A second factor of complexity is the lack of data with regard to table-structure information. For the publications of PubTabNet [37], FinTabNet [36], TableBank [17] etc. there were no large datasets (i.e. > 100K tables) that provided structure information. This happens primarily due to the fact that tables are notoriously time-consuming to annotate by hand. However, this has definitely changed in recent years with the adherence of PubTabNet [37], FinTabNet [36], TableBank [17] etc.

Before the rising popularity of deep neural networks, the community relied heavily on heuristic and/or statistical methods to do table structure identification [3, 7, 11, 5, 13, 28]. Although such methods work well on constrained tables [12], a more data-driven approach can be applied due to the advent of computational neural networks (CNNs) and the availability of large datasets. To the best-of our knowledge, there are currently two different types of network architectures that are being pursued for state-of-the-art table-structure identification.

Image-to-Text networks: In this type of network, one predicts a sequence of tokens starting from an encoded image. Such sequences of tokens are often the HTML table tags [37, 17] or LaTeX symbols[10]. The choice of symbols is ultimately not very important, since one can transform into the other. There are however subtle variations in the Image-to-Text networks. The easiest network architectures are "image-encoder -> text decoder" (IETD), similar to network architectures that try to provide captions to images [32]. In these IETD networks, one expects as output the LaTeX/HTML string of the entire table, i.e. the symbols necessary for creating the table with the content of the table. Another approach is 'image-encoder -> dual decoder' (IEDD) networks. In these type of networks, one has two consecutive decoders with different purposes. The first decoder is the tag-decoder, i.e. it produces the HTML/LaTeX tags which construct an empty table. The second content-decoder uses the encoding of the image in combination with the output encoding of each cell (from the tag-decoder) to generate the textual content of each table cell. The network architecture of IEDD is certainly more elaborate, but it has the advantage that one can pre-train the tag-decoder which is constrained to the table-tags.

In practice, both network architectures (IETD and IEDD) require an implicit, custom trained object-character-recognition (OCR) to obtain the content of the table-cells. In the case of IETD, this OCR engine is implicit in the decoder similar to [24]. For the IEDD method, the OCR is solely embedded in the content-decoder. This reliance on a custom, implicit OCR decoder is of course problematic. OCR is a well known and extremely tough problem, that often needs custom training for each individual language. However, the limited availability for non-english content in the current datasets, makes it impractical to apply the IETD and IEDD methods on tables with other languages. Additionally, OCR can be completely controlled out of the tables originate from programmatic PDF documents with known positions of each cell. The latter was the inspiration for the work of this paper.

Graph Neural networks: Graph Neural networks (GNNs) take a radically different approach to table-structure extraction. Note that one table cell can constitute one of multiple text-cells. To obtain the table-structure, one creates an initial graph, where each of the text-cells becomes a node in the graph similar to [33, 34, 2]. Each node is then associated with an embedded vector coming from the encoded image, its coordinates and the encoded text. Furthermore, nodes that represent adjacent text-cells are linked. Graph Convolutional Networks (GCN's) based methods take the image as an input, but also the position of the text-cells and their coordinates [18]. The purpose of a GCN is to transform the input graph into a new graph, with which replaces the old links with new ones. The new links then represent the table-structure. With this approach, one can avoid the need to build custom OCR decoders. However, the quality of the reconstructed structure is not comparable to the current state-of-the-art [18].

Hybrid Deep Learning-Rule-Based approach: A popular current model for table-structure identification is the use of a deep learning-based approach similar to [27, 29]. In this approach, one first detects the position of the table-cells with object detection (e.g. YoloV5 or Mask-RCNN), then uses different rule-sets to obtain table-structure. Currently, this approach achieves state-of-the-art results, but is not an end-to-end deep learning method. As such, new rules need to be written if different types of tables are encountered.

3. Datasets

We rely on large-scale datasets such as PubTabNet [37], FinTabNet [36], and TableBank [17] datasets to train and evaluate our models. These datasets span over various appearance styles and content. We also introduce our own synthetically generated SynthTabNet dataset to fix an imbalance of the previous datasets.

As it is illustrated in Fig. 2, the table distributions from all datasets are skewed towards simpler structures with fewer number of rows/columns. Additionally, there is very limited variance in the use of the table styles. In case of PubTabNet and FinTabNet means one styling format for the majority of the tables. Similar limitations appear also in the type of table content, which in some cases (e.g. FinTabNet) is restricted to a certain domain. Ultimately, the lack of diversity in the training dataset damages the ability of the models to generalize well on unseen data.

Motivated by those observations we aimed at generating a synthetic table dataset named SynthTabNet. This approach offers control over: 1) the size of the dataset, 2) the table structure, 3) the style of content. The complexity of the table structure is described by the size of the header and the table body, as well as the percentage of the table area covered by row spans and column spans. A set of carefully designed styling templates provides the basis to build a wide range of table appearances. Lastly, the table content is generated out of a curated collection of text corpora. By controlling the size and scope of the synthetic datasets we are able to train and evaluate our models in a variety of different conditions. For example, we can first generate a highly diverse dataset to train our models, then evaluate their performance on other synthetic datasets which are more focused on a specific domain.

In this regard, we have prepared four synthetic datasets, each one containing 150k examples. The corpora to generate the table text consists of the most frequent terms appearing in PubTabNet and FinTabNet together with randomly generated text. The first two synthetic datasets have been fine-tuned to mimic the appearance of the original datasets but encompass more complicated table structures. The third and fourth synthetic datasets, each one containing 150k examples. The corpora to generate the table text consists of the most frequent terms appearing in PubTabNet and FinTabNet together with randomly generated text. The first two synthetic datasets have been fine-tuned to mimic the appearance of the original datasets but encompass more complicated table structures. The third and

PubTabNet
FinTabNet
TableBank
Combined-Tabnet(*)
Combined-Tabnet(**)
SynthTabNet

Tags
✓
✓
✓
✓
✓
✓

Bbox
✓
✓
✓
✓
✓
✓

Size
509k
112k
145k
400k
500k
600k

Format
PNG
PDF
JPEG
PNG
PNG
PNG

Table 1: Both "Combined-Tabnet" and "Combined-Tabnet" are variations of the following: (*) The Combined-Tabnet dataset is the processed combination of PubTabNet and FinTabnet. (**) The combined dataset is the processed combination of PubTabNet, FinTabnet and TableBank.

one adopts a colorful appearance with high contrast and the last one contains tables with sparse content. Lastly, we have combined all synthetic datasets into one big unified synthetic dataset.

Tab. 1 summarizes the various attributes of the datasets.

4. The TableFormer model

Given the image of a table, TableFormer is able to predict: 1) a sequence of tokens that represent the structure of a table, and 2) a bounding box coupled to a subset of those tokens. The conversion of an image into a sequence of tokens is a well known technique [15, 14], while table attention is often used as an implicit method to associate each token of the sequence with a position in the original image, an explicit association between the individual table-cells and the image bounding boxes is also required.

4.1. Model architecture.

We now describe in detail the proposed method, which is composed of three main components, see Fig. 4. Our CNN Backbone Network encodes the input as a feature vector of predefined length. The input feature vector of the encoded image is passed to the Structure Decoder to produce a sequence of HTML tags that represent the structure of the table. With each prediction of an HTML standard data cell ('<td>') the hidden state of that cell is passed to the Cell BBox Decoder. A shared feed forward network (FFN) receives the hidden state, and to provide the final detection predictions of the bounding box coordinates and their classification.

CNN Backbone Network. A ResNet-18 CNN is the backbone that receives the table image and encodes it as a vector of predefined length. The network has been modified by removing the linear and pooling layer, as we are not performing any classification. Thus the network produces a vector of predefined length.

Structure Decoder. The transformer architecture of this component is based on the work proposed in [31]. After extensive experimentation, the Structure Decoder is modeled as a transformer decoder with two decoder layers and a transformer encoder with 4 decoder layers that comprise mainly of multi-head attention and feed forward layers. This configuration uses fewer layers and heads in comparison to networks applied to other problems (e.g. "Scene Understanding", "Image Captioning"), something which we relate to the simplicity of table images, which contain much simpler visual content and have a much more elaborate content present in other scenes (e.g. the COCO dataset). Moreover, we have added ResNet blocks to the Structure Decoder and Cell BBox Decoder. This prevents a decoder having a stronger influence over the learned weights which would damage the other prediction task (structure vs bounding boxes), but learn task specific weights instead. Lastly our dropout layers are set to 0.5.

For training, TableFormer is trained with 3 Adam optimizers, each one for the CNN Backbone Network, Structure Decoder, and Cell BBox Decoder. Taking the PubTabNet as an example for our parameter setup, the initializing learning rate is 0.001 for 12 epochs with a batch size of 24, and λ set to 0.5. Afterwards, we reduce the learning rate to 0.0001, the batch size to 18 and train for 12 more epochs or convergence.

Loss Functions. We formulate a multi-task loss Eq. 2 to train our network. The Cross-Entropy loss (denoted as lce) is used to train the Structure Decoder which predicts the structure tokens. As for the Cell BBox Decoder it is trained with a combination of losses denoted as lbox, lce, consists of the generality used l1 loss for object detection and the IoU loss (loiou) to be scale invariant as explained in [25]. We do not use the Hungarian algorithm [15] to match the predicted bounding boxes with the ground truth boxes, as we have already achieved a one-to-one mapping between the cell content and its bounding box for all post processed datasets.

The loss used to train the TableFormer can be defined as following:

lbox = λion lion + λl1
l = λs + (1 − λ)lbox

where λ ∈ [0, 1], and λion, λl1 ∈ ℜ are hyper-parameters.

5. Experimental Results

5.1. Implementation Details

TableFormer uses ResNet-18 as the CNN Backbone Network. The input images are resized to 448*448 pixels and the feature map has a dimension of 28*28. Additionally, we enforce the following input constraints:

Image width and height ≤ 1024 pixels
Structural tags length ≤ 512 tokens.

Although input constraints are used also by other methods, such as EDD, ours are less restrictive due to the improved decoder.

5.2. Generalization

TableFormer is evaluated on three major publicly available datasets of different nature to prove the generalization and effectiveness of our model. The datasets used for evaluation are the PubTabNet, FinTabNet and TableBank which stem from the scientific, financial and general domains respectively.

We also share our baseline results on the challenging SynthTabNet dataset. Throughout our experiments, the same parameters stated in Sec. 5.1 are utilized.

5.3. Datasets and Metrics

Tree-Edit-Distance-Based Similarity (TEDS) metric was introduced in [37]. It represents the prediction, and ground-truth as a tree structure of HTML tags. This similarity is calculated as:

TEDS (Ts, Tt) = 1 - EditDist(Ts, Tt) / max (|Ts|, |Tt|)

where Ts and Tt represent tables in tree structure HTML format. EditDist denotes the tree-edit distance, and |T| represents the number of nodes in T.

5.4. Quantitative Analysis

Structure. As shown in Tab. 2, TableFormer outputs all SOTA methods across different datasets by a large margin for predicting the table structure from an image. All the more, our model outperforms pre-trained methods. During the evaluation we do not apply any table filtering. We also provide our baseline results on the SynthTabNet dataset. It has been observed that large tables that occupy half of the page or more yield predictions. We attribute this issue to the image resizing during the pre-processing step, that produces downsampled images with indistinguishable features. This problem can be addressed by treating such big tables with a separate model which accepts a large input size.

Model
EDD
GTE
TableFormer
EDD
GTE
TableFormer
EDD
GTE
TableFormer
TableFormer
TableFormer

Dataset
PTN
PTN
PTN
FTN
FTN
FTN
TB
TB
TB
STN

TEDS
Simple
91.1
88.4
98.5
88.4
97.3
86.0
89.6
96.9

Complex
88.7
92.08
95.0
92.08
96.0
95.7

All
89.9
93.01
90.6
87.14
91.02
96.8
86.0
89.6
96.7

Table 2: Structure results on PubTabNet (PTN), FinTabNet (FTN), TableBank (TB) and SynthTabNet (STN).
FT: Model was trained on PubTabNet then finetuned.

Cell Detection. Like any object detector, our Cell BBox Detector provides bounding boxes that can be improved with post-processing during inference. We make use of the grid-like structure of tables to refine the predictions. A detailed explanation on the post-processing is available in the supplementary material. As shown in Tab. 3, we evaluate our Cell BBox Decoder accuracy for cells with a class label of 'content' only using the PASCAL VOC mAP metric for pre-processing and post-processing. Note that we do not have predictions for PubTabNet as images are only provided. To compare the performance of our proposed approach, we've integrated TableFormer's Cell BBox Decoder into EDD architecture. As mentioned previously, the Structure Decoder provides the Cell BBox Decoder with the features needed to predict the bounding box predictions. Therefore, the accuracy of the Structure Decoder directly influences the accuracy of the Cell BBox Decoder. If the Structure Decoder predicts an extra column, this will result in an extra column of predicted bounding boxes.

Model
EDD+BBox
TableFormer
TableFormer

Dataset
PubTabNet
PubTabNet
SynthTabNet

mAP
79.2
82.1
87.7

mAP (PP)
82.7
86.8

Table 3: Cell Bounding Box detection results on PubTabNet, and FinTabNet. PP: Post-processing.

Cell Content. In this section, we evaluate the entire pipeline of recovering a table with content. Here we put our approach to test by capitalizing on extracting content from the PDF cells rather than decoding from images. Tab. 4 shows the TEDS score of HTML code representing the structure of the table along with the content inserted in the data cell and compared with the ground-truth. Our method achieved a 5.3% increase over the state-of-the-art, and commercial solutions. We believe our scores would be higher if the HTML ground truth matched the extracted PDF cell content. Unfortunately, there are small discrepancies such as spacings around words or special characters with various unicode representations.

Model
Tabula
Traprange
Camelot
Acrobat Pro
EDD
TableFormer

Sample
78.0
60.8
60.0
68.9
91.2
95.4

TEDS
Complex
57.8
49.9
66.0
61.8
85.4
90.1

All
67.9
55.4
73.0
65.3
88.3
93.6

Table 4: Results of structure with content retrieved using cell detection on PubTabNet. In all cases the input is PDF documents with cropped tables.

5.5. Qualitative Analysis

We showcase several visualizations for the different components of our network on various "complex" tables within datasets presented in this work in Fig. 5 and Fig. 6. As it is shown, our post-processing technique can extract the cell content by matching the predicted bounding boxes to the PDF cells based on their overlap and spatial proximity. The left part of Fig. 5 demonstrates the adaptability of our method to any language, as it successfully extract Japanese text, although the training set contained only English content. We provide more visualizations including the intermediate steps in the supplementary material. Overall these illustrations justify the versatility of our method across a diverse range of table appearances and content type.

6. Future Work & Conclusion

In this paper, we presented TableFormer an end-to-end transformer based approach to predict table structures and bounding boxes of cells from an image. This approach enables us to recreate the table structure, and extract the cell content from PDF or OCR by using bounding boxes. Additionally, it provides the versatility required in real-world scenarios when dealing with various types of PDF documents, and languages. Furthermore, our method outperforms all state-of-the-art with a wide margin. Finally, we introduce "SynthTabNet" a challenging synthetically generated dataset that reinforces missing characteristics from other datasets.

References

[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagorusko. End-to-end object detection with transformers. In Andrea Vedaldi, Horst Bischofm, Thomas Brox, and Jan-Michael Frahin, editors, Computer Vision – ECCV 2020, pages 213–229, Cham, 2020. Springer International Publishing. 5

[2] Zewten Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanxuan Liu, and Xia-Liang Mao. Complicated table structure recognition. arXiv preprint arXiv:1908.04729, 2019. 3

[3] Bertrand Couasnon and Aurelie Lemaitre. Recognition of tables and forms. pages 647–677. Springer London, 2014. 2

[4] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian Kleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and Recognition (cTDaR), Apr. 2019. http://sac.foundtext.org/. 2

[5] Basilios Gatos, Dimitrios Danesis, Ioannis Pratakis, and Stavros J Peratonis. Automatic table detection in document images. In International Conference on Pattern Recognition and Image Analysis, pages 609–618. Springer, 2005. 2

[6] Mas Ghosh, Tabriz Hasam. Table Detection and Recognition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1449–1453, 2013. 2

[7] EA Green and M Krishnamoorthy. Recognition of tables using stochastic techniques. In Proceedings of the Eleventh International Conference on Document Analysis and Recognition (SDAR 93), pages 261–277. 2

[8] Khurram Azhar, Shuail Pham, Mohrir Zeshan Afzal. Castledefense: Cascade network for table detection in document images. Journal of Imaging, 7(10), 2021. 1

[9] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Oct 2017. 1

[10] Yelin He, X. Qi, Jiaquan Ye, Peter Gao, Yihan Chen, Binglu Li, Xu Tang, and Rong Xiao. Pingan-pagegroup's solution for icdar 2021 competition on scientific table image recognition to latex. arXiv preprint, 2021. 2

[11] Jianiying Hu, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE Conference on Computer Vision (CVPR), Oct 2017. 1

[12] Matthew Hurst. A constraint-based approach to table structure derivation. In Proceedings of the Seventh International Conference on Document Analysis and Recognition - Volume 2, ICDAR '03, page 911, USA, 2003. IEEE Computer Society. 2

[13] Thoerangam Kasar, Phillippe Barlas, Sebastian Adam, Clement Chatelain, and Thierry Paquet. Learning to detect tables in scanned document images using line information. In 2013 12th International Conference on Document Analysis and Recognition, pages 1183–1189. IEEE, 2013. 2

[14] Pratik Kayal, Mrinal Anand, Harsh Desai, and Mayank Singh. Icdar 2021 competition on scientific table image recognition to latex. arXiv preprint, 2021. 2

[15] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83–97, 1955. 6

[16] Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L. Berg. Babytak: Understanding and generating simple image descriptions. IEEE Transactions on Pattern Recognition and Machine Intelligence, 35(12):2891–2903, 2013. 4

[17] Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhongqia Li. Tablebank: A benchmark dataset for table detection and recognition. In 2019 International Conference on Document Analysis and Recognition, 2019. 2, 3

[18] Yiroji Li, Zheng Huanshi, Iiangli Nan, Yi Zhou, Fan Ye, and Xianhui Liu. Gfte: Graph-based financial table extraction. In Alberto Del Bimbo, Rita Cucchiara, Stan Scaroff, Giovanna Maria Farioli, Hugo Jair Escalante, and Roberto Vezzani, editors, Pattern Recognition: ICPR International Workshops and Challenges, pages 644–658, Cham, 2021. Springer International Publishing. 2, 3

[19] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmad Nassar, Anita Carvalho, Michele Dolfi, Christoph Auer, Kasper Dakla, and Peter Staar. Robust pdf document layout analysis. In Proceedings of the AAAI Conference on Artificial Intelligence, 35(17):15137–15145, May 2021. 1

[20] Rujiao Luo, Wen Wang, Nan Xue, Heyiu Guo, Zhibo Yang, Yongyan Wang, and Gue-Song Xu. Parsing table structures in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 944–952, 2021. 2

[21] Shubham Singh, Pallavi D Vishwanadh, Rohit Rahul, Monika Sharma, and Lovekesh Vg. Tablener: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 128–133. IEEE, 2019. 1

[22] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Susank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Wallach, H. Larochelle, A. Alcheli-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019. 6

[23] Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visaye, and Kavina Subramagne. An approach for end to end table detection and structure recognition from image-based pdf documents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 572–575, 2020. 1

[24] Shah Rukh Qasim, Hassan Mahmood, and Faisal Shafait. Rethinking table recognition using graph neural networks. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 142–147. IEEE, 2019. 3

[25] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savanese. Generalized intersection over union: A metric and a loss for bounding box regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 658–666, 2019. 6

[26] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdest: Deep learning for detection and structure recognition of tables in document images. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 01, pages 1162–1167, 2017. 1

[27] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdest: Deep learning for detection and structure recognition of tables in document images. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 1162–1167. IEEE, 2017. 3

[28] Faisal Shafait and Ray Smith. Table detection in heterogeneous documents. In Proceedings of the 9th IAPR International Workshop on Document Analysis Systems, pages 65–72, 2010. 2

[29] Shoaib Ahmed Siddiqui, Imran Ali Fateh, Syed Tahseen Raza Rizvi, Andreas Dengel, and Sheraz Ahmed. Deeptabstr: Deep learning based table structure recognition. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1403–1409. IEEE, 2019. 3

[30] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus conversion service: A machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD, KDD '18, pages 774–782, New York, NY, USA, 2018. ACM. 1

[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Ilia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. 5

[32] Oriol Vinyals, Alexander Toshev, Saniy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 2

[33] Wenyuan Xue, Qingyong Li, and Dacheng Tao. Res2lim: reconstruct syntactic structures from table images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 749–755. IEEE, 2019. 3

[34] Wenyuan Xue, Baosheng Yu, Wen Wang, Dacheng Tao, and Qingyong Li. Tgret: A table graph reconstruction network for table structure recognition. arXiv preprint arXiv:2106.10598, 2021. 3

[35] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebs Luo. Image captioning with semantic attention. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4651–4659, 2016. 4

[36] Xinyi Zheng, Doug Burdick, Lucian Popa, Peter Zhong, and Nancy Xin Ru Wang. Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. Winter Conference for Applications in Computer Vision (WACV), 2021. 2, 3

[37] Xu Zhong, Eliaheh ShafeiBavani, and Antonio Jimeno Yepes. Image-based table recognition: Data, model, and evaluation. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1015–1022, 2019. 1

TableFormer: Table Structure Understanding with Transformers

Supplementary Material

1. Details on the datasets

1.1. Data preparation

As a first step of our data preparation process, we have calculated statistics over the datasets across the following dimensions: (1) table size measured in the number of rows and columns, (2) complexity of the structure, (3) structures of the HTML structure and (4) completeness (i.e. no omitted bounding boxes). A table is considered to be simple if it does not contain row spans or column spans. Additionally, a table has a strict HTML structure if every row has the same number of columns after taking into account any row or column spans. Therefore a strict HTML structure looks always rectangular. However, HTML is a lenient encoding format, i.e. tables with rows of different sizes might still be regarded as correct due to implicit display rules. These implicit rules leave room for ambiguity, which we want to avoid. As such, we prefer to have "strict" tables, i.e. tables where every row has exactly the same length.

We have developed a technique that tries to derive a missing bounding box out of its neighbours. As a first step, we use the annotation data to generate the most fine grained grid that covers the table structure. In case of strict HTML tables, all grid squares are associated with some table cell and in the presence of table-page and if table rows have unequal number of columns and the structure is non-strict. The generation of missing bounding boxes for non-strict HTML tables is ambiguous and therefore quite challenging. Thus, we have decided to simply discard those tables. In case of PubTabNet we have computed missing bounding boxes for 48% of the simple and 69% of the complex tables. Regarding FinTabNet, 68% of the simple and 98% of the complex tables require the generation of bounding boxes.

As it is illustrated in Fig. 7 illustrates the distribution of the tables across different dimensions per dataset.

1.2. Synthetic datasets

Aiming to train and evaluate our models in a broader spectrum of table data we have synthesized four types of datasets. Each one contains tables with different appearances in regard to their size, structure, style and content. Every synthetic dataset can be decomposed into the following steps:

1. Prepare styling and content templates: The styling templates have been manually designed and organized into groups of scope specific appearances (e.g. financial data, marketing data, etc.). Additionally, we have prepared curated collections of content templates by extracting the most frequently used items from non-synthetic datasets (e.g. PubTabNet, FinTabNet, etc.).

2. Generate table structures: The structure of each synthetic dataset assumes a horizontal table header which potentially spans over multiple rows and a table body that may contain a combination of row spans and column spans. However, spans are not allowed to cross the header - body boundary. The table structure is described by the parameters: Total number of rows and columns, number of header rows, type of spans (header only spans, row only spans, column only spans, both row and column spans), maximum span size and the ratio of the table area covered by spans.

3. Generate content: Based on the dataset theme, a set of suitable content templates is chosen first. Then, this content can be combined with purely random text to produce the synthetic content.

4. Apply styling templates: Depending on the domain of the synthetic dataset, a set of styling templates is first manually selected. Then, a style is randomly selected to format the appearance of the synthesized table.

5. Render the complete tables: The synthetic table is finally rendered by a web browser engine to generate the bounding boxes for each table cell. A batching technique is utilized to optimize the runtime overhead of the rendering process.

The availability of the bounding boxes for all table cells is essential to train our models. In order to distinguish between empty and non-empty bounding boxes, we have introduced a binary class in the annotation. Unfortunately, the original datasets discounted the bounding boxes for all cells. This forced us to develop a data pre-processing procedure that generates the missing bounding boxes out of the annotation information. This procedure first passes the provided table structure and calculates the dimensions of the most fine-grained grid that covers the table structure. Notice that each table cell can occupy multiple grid squares due to row or column spans. In the case of PubTabNet we had to compute missing bounding boxes for 48% of the simple and 69% of the complex tables. Regarding FinTabNet, 68% of the simple and 98% of the complex tables require the generation of bounding boxes.

As it is illustrated in Fig. 2, the table distributions from all datasets are skewed towards simpler structures with fewer number of rows/columns. Additionally, there is very limited variance in the use of styling format for the majority of the tables. Similar limitations appear also in the type of table content, which in some cases (e.g. FinTabNet) is restricted to a certain domain. Ultimately, the lack of diversity in the training dataset damages the ability of the models to generalize well on unseen data.

balance in the previous datasets.

The PubTabNet dataset contains 509k tables delivered as annotated PNG images. The annotations consist of the table structure encoded in HTML format, the tokenized text and its bounding boxes per table cell. Fig. 1 shows the appearance style of PubTabNet. Depending on its complexity, a table is characterized as "simple" if it does not contain row spans or column spans, otherwise it is "complex". The dataset is divided into Train and Val split, roughly 98% and 2%. The Train split consists of 54% simple and 46% complex tables and the Val split of 51% and 49% respectively. The FinTabNet dataset contains 112k tables delivered as single-page PDF documents with mixed table structures and text content. Similarly to the PubTabNet, the annotations of the FinTabNet include the table structure in HTML, the tokenized text and the bounding boxes on a table cell basis. The dataset is divided into Train, Test and Val splits (81%, 9.5%, 9.5%), and each one is almost equally divided into simple and complex tables (Train: 48% simple, 52% complex, Test: 53% simple, 47% complex). Finally, the TableBank dataset consists of simple tables and it is divided into 90% Train, 3% Test and 7% Val splits.

Due to the heterogeneity across the dataset formats, it was necessary to combine all available data into one homogenized dataset before we could train our models for practical purposes. Given the size of PubTabNet, we adopted its annotation format and we extracted and converted all tables as PNG images with a resolution of 72 dpi. Additionally, we have filtered out tables with extreme sizes due to small amount of such tables and kept only those ones ranging between 1 and 20 (rows/columns).

The availability of the bounding boxes for all table cells is essential to train our models. In order to distinguish between empty and non-empty bounding boxes, we have introduced a binary class in the annotation. Unfortunately, the original datasets discounted the bounding boxes for all cells. This forced us to develop a data pre-processing procedure that generates the missing bounding boxes out of the annotation information. This procedure first passes the provided table structure and calculates the dimensions of the most fine-grained grid that covers the table structure. Notice that each table cell can occupy multiple grid squares due to row or column spans. In the case of PubTabNet we had to to compute missing bounding boxes for 48% of the simple and 69% of the complex tables. Regarding FinTabNet, 68% of the simple and 98% of the complex tables require the generation of bounding boxes.

As it is illustrated in Fig. 7 illustrates the distribution of the tables across different dimensions per dataset.

1.2. Synthetic datasets

Aiming to train and evaluate our models in a broader spectrum of table data we have synthesized four types of datasets. Each one contains tables with different appearances in regard to their size, structure, style and content. Every synthetic dataset can be decomposed into the following steps:

1. Prepare styling and content templates: The styling templates have been manually designed and organized into groups of scope specific appearances (e.g. financial data, marketing data, etc.). Additionally, we have prepared curated collections of content templates by extracting the most frequently used items from non-synthetic datasets (e.g. PubTabNet, FinTabNet, etc.).

2. Generate table structures: The structure of each synthetic dataset assumes a horizontal table header which potentially spans over multiple rows and a table body that may contain a combination of row spans and column spans. However, spans are not allowed to cross the header - body boundary. The table structure is described by the parameters: Total number of rows and columns, number of header rows, type of spans (header only spans, row only spans, column only spans, both row and column spans), maximum span size and the ratio of the table area covered by spans.

3. Generate content: Based on the dataset theme, a set of suitable content templates is chosen first. Then, this content can be combined with purely random text to produce the synthetic content.

4. Apply styling templates: Depending on the domain of the synthetic dataset, a set of styling templates is first manually selected. Then, a style is randomly selected to format the appearance of the synthesized table.

5. Render the complete tables: The synthetic table is finally rendered by a web browser engine to generate the bounding boxes for each table cell. A batching technique is utilized to optimize the runtime overhead of the rendering process.

The availability of the bounding boxes for all table cells is essential to train our models. In order to distinguish between empty and non-empty bounding boxes, we have introduced a binary class in the annotation. Unfortunately, the original datasets discounted the bounding boxes for all cells. This forced us to develop a data pre-processing procedure that generates the missing bounding boxes out of the annotation information. This procedure first passes the provided table structure and calculates the dimensions of the most fine-grained grid that covers the table structure. Notice that each table cell can occupy multiple grid squares due to row or column spans. In the case of PubTabNet we had to compute missing bounding boxes for 48% of the simple and 69% of the complex tables. Regarding FinTabNet, 68% of the simple and 98% of the complex tables require the generation of bounding boxes.

As it is illustrated in Fig. 2, the table distributions from all datasets are skewed towards simpler structures with fewer number of rows/columns. Additionally, there is very limited variance in the use of styling format for the majority of the tables. Similar limitations appear also in the type of table content, which in some cases (e.g. FinTabNet) is restricted to a certain domain. Ultimately, the lack of diversity in the training dataset damages the ability of the models to generalize well on unseen data.

2. Prediction post-processing for PDF documents

Although TableFormer can predict the table structure and the bounding boxes for tables recognized inside PDF documents, this is not enough when a full reconstruction of the original table is required. This happens mainly due to the following reasons:

TableFormer output does not include the table cell content.

There are occasional inaccuracies in the predictions of the bounding boxes.

However, it is possible to mitigate those limitations by combining the TableFormer predictions with the information already present inside a programmatic PDF document. More specifically, PDF documents can be used as a sequence of PDF cells where each cell is described by its content and bounding box. If we are able to associate the PDF cells with the predicted cells, we can directly link the PDF cell content to the table cell structure and use the PDF bounding boxes to correct misalignments in the predicted table cell bounding boxes.

Here is a step-by-step description of the prediction post-processing:

1. Get the minimal grid dimensions - number of rows and columns for the predefined table structure. This approach is used for the structure size of the underlying table structure.

2. Generate pair-wise matches between the bounding boxes of the PDF cells and the predicted cells. The Intersection Over Union (IOU) metric is used to evaluate the quality of the matches.

3. Use a carefully selected IOU threshold to designate the matches as "good" ones and "bad" ones.

3a. If all IOU scores in a column are below the threshold, discard all predictions (structure and bounding boxes) for that column.

4. Find the best-fitting content alignment for the predicted cells with good IOU per each column. The alignment of the column can be identified by the following formula:

alignment = arg min{Dc}

Dc = max{xc} - min{xc}

where c is one of {left, centroid, right} and xc is the x-coordinate for the corresponding point.

5. Use the alignment computed in step 4, to compute the median x-coordinate for all table columns and the median y-coordinate for all columns and the median y-coordinate for all table cells. The usage of median during the computations, helps to eliminate outliers caused by occasional column spans which are usually wider than normal.

6. Snap all cells with bad IOU to their corresponding median x-coordinates and cell sizes.

7. Generate a new set of pair-wise matches between the corrected bounding boxes and PDF cells. This time we use a modified version of the IOU metric, where the area of the intersection between the predicted and PDF cells is divided by the PDF cell area.

8. In some rare occasions, we have noticed that TableFormer can confuse a single column as two. When the post-processing steps are applied, this results with two predicted columns pointing to the same PDF column. In such case we must de-duplicate the columns according to highest total column intersection score.

9. Pick up the remaining orphan cells. There could be cases, when after applying the previous post-processing steps, some PDF cells could still remain without any match to predicted cells. However, it is still possible to deduce the correct matching for an orphan PDF cell by mapping its bounding box on the geometry of the grid. This mapping decides if the orphan cell will be appended to an already matched table cell, or a new table cell should be created to match with the orphan.

9a. Compute the top and bottom boundary of the horizontal band for each grid row (min/max y coordinates per row).

9b. Intersect the orphan's bounding box with the row bands, and map the cell to the closest grid row.

9c. Compute the left and right boundary of the vertical band for each grid column (min/max x coordinates per column).

9d. Intersect the orphan's bounding box with the column bands, and map the cell to the closest grid column.

9e. If the table cell under the identified row and column is not empty, extend its content with the content of the orphan cell. Otherwise create a new structural cell and match it with the orphan.

phan cell.

9f. Otherwise create a new structural cell and match it with the orphan cell.

Appendix: images with examples of TableFormer predictions and post-processing can be found below.
