     A Course in
Machine Learning




         Hal Daumé III
Copyright © 2015 Hal Daumé III

Published by TODO

http://hal3.name/courseml/


TODO. . . .

First printing, September 2015
For my students and teachers.

 Often the same.
                                                   TABLE OF C ONTENTS




     About this Book        6


1    Decision Trees     8


2    Geometry and Nearest Neighbors           26


3    The Perceptron      39


4    Practical Issues    53


5    Beyond Binary Classification        70


6    Linear Models      86


7    Probabilistic Modeling        103


8    Neural Networks         116


9    Kernel Methods      128


10   Learning Theory        141
                                               5



11   Ensemble Methods            152

12   Efficient Learning         159

13   Unsupervised Learning             166

14   Expectation Maximization            175

15   Semi-Supervised Learning            181

16   Graphical Models           183

17   Online Learning        184

18   Structured Learning Tasks           186

19   Bayesian Learning           187

     Code and Datasets           188

     Notation       189

     Bibliography         190

     Index      191
                                                                          A BOUT THIS B OOK




      Machine learning is a broad and fascinating field. It has
      been called one of the attractive fields to work in1 . It has applications   1


      in an incredibly wide variety of application areas, from medicine to
      advertising, from military to pedestrian. Its importance is likely to
      grow, as more and more areas turn to it as a way of dealing with the
      massive amounts of data available.




0.1   How to Use this Book

0.2   Why Another Textbook?

      The purpose of this book is to provide a gentle and pedagogically orga-
      nized introduction to the field. This is in contrast to most existing ma-
      chine learning texts, which tend to organize things topically, rather
      than pedagogically (an exception is Mitchell’s book2 , but unfortu-          2
                                                                                       Mitchell 1997
      nately that is getting more and more outdated). This makes sense for
      researchers in the field, but less sense for learners. A second goal of
      this book is to provide a view of machine learning that focuses on
      ideas and models, not on math. It is not possible (or even advisable)
      to avoid math. But math should be there to aid understanding, not
      hinder it. Finally, this book attempts to have minimal dependencies,
      so that one can fairly easily pick and choose chapters to read. When
      dependencies exist, they are listed at the start of the chapter, as well
      as the list of dependencies at the end of this chapter.
         The audience of this book is anyone who knows differential calcu-
      lus and discrete math, and can program reasonably well. (A little bit
      of linear algebra and probability will not hurt.) An undergraduate in
      their fourth or fifth semester should be fully capable of understand-
      ing this material. However, it should also be suitable for first year
      graduate students, perhaps at a slightly faster pace.
                                                                                    7



0.3   Organization and Auxilary Material

      There is an associated web page, http://hal3.name/courseml/, which
      contains an online copy of this book, as well as associated code and
      data. It also contains errate. For instructors, there is the ability to get
      a solutions manual.
         This book is suitable for a single-semester undergraduate course,
      graduate course or two semester course (perhaps the latter supple-
      mented with readings decided upon by the instructor). Here are
      suggested course plans for the first two courses; a year-long course
      could be obtained simply by covering the entire book.


0.4   Acknowledgements
                                                                       1 | D ECISION T REES

        The words printed here are concepts.                                        Learning Objectives:
        You must go through the experiences.                  – Carl Frederick      • Explain the difference between
                                                                                      memorization and generalization.
                                                                                    • Define “inductive bias” and recog-
                                                                                      nize the role of inductive bias in
                                                                                      learning.
                                                                                    • Take a concrete task and cast it as a
                                                                                      learning problem, with a formal no-
                                                                                      tion of input space, features, output
                                                                                      space, generating distribution and
      At a basic level, machine learning is about predicting the fu-
                                                                                      loss function.
      ture based on the past. For instance, you might wish to predict how
                                                                                    • Illustrate how regularization trades
      much a user Alice will like a movie that she hasn’t seen, based on              off between underfitting and overfit-
      her ratings of movies that she has seen. This means making informed             ting.

      guesses about some unobserved property of some object, based on               • Evaluate whether a use of test data
                                                                                      is “cheating” or not.
      observed properties of that object.
         The first question we’ll ask is: what does it mean to learn? In
      order to develop learning machines, we must know what learning
      actually means, and how to determine success (or failure). You’ll see
      this question answered in a very limited learning setting, which will
      be progressively loosened and adapted throughout the rest of this
      book. For concreteness, our focus will be on a very simple model of
                                                                                    Dependencies: None.
      learning called a decision tree.

         V IGNETTE : A LICE D ECIDES WHICH C LASSES TO TAKE
        todo




1.1   What Does it Mean to Learn?

      Alice has just begun taking a course on machine learning. She knows
      that at the end of the course, she will be expected to have “learned”
      all about this topic. A common way of gauging whether or not she
      has learned is for her teacher, Bob, to give her a exam. She has done
      well at learning if she does well on the exam.
         But what makes a reasonable exam? If Bob spends the entire
      semester talking about machine learning, and then gives Alice an
      exam on History of Pottery, then Alice’s performance on this exam
      will not be representative of her learning. On the other hand, if the
      exam only asks questions that Bob has answered exactly during lec-
      tures, then this is also a bad test of Alice’s learning, especially if it’s
      an “open notes” exam. What is desired is that Alice observes specific
                                                                                         decision trees              9



examples from the course, and then has to answer new, but related
questions on the exam. This tests whether Alice has the ability to
generalize. Generalization is perhaps the most central concept in
machine learning.
    As a running concrete example in this book, we will use that of a
course recommendation system for undergraduate computer science
students. We have a collection of students and a collection of courses.
Each student has taken, and evaluated, a subset of the courses. The
evaluation is simply a score from −2 (terrible) to +2 (awesome). The
job of the recommender system is to predict how much a particular
student (say, Alice) will like a particular course (say, Algorithms).
    Given historical data from course ratings (i.e., the past) we are
trying to predict unseen ratings (i.e., the future). Now, we could
be unfair to this system as well. We could ask it whether Alice is
likely to enjoy the History of Pottery course. This is unfair because
the system has no idea what History of Pottery even is, and has no
prior experience with this course. On the other hand, we could ask
it how much Alice will like Artificial Intelligence, which she took
last year and rated as +2 (awesome). We would expect the system to
predict that she would really like it, but this isn’t demonstrating that
the system has learned: it’s simply recalling its past experience. In
the former case, we’re expecting the system to generalize beyond its
experience, which is unfair. In the latter case, we’re not expecting it
to generalize at all.
    This general set up of predicting the future based on the past is
at the core of most machine learning. The objects that our algorithm
will make predictions about are examples. In the recommender sys-
tem setting, an example would be some particular Student/Course
pair (such as Alice/Algorithms). The desired prediction would be the
rating that Alice would give to Algorithms.
    To make this concrete, Figure 1.1 shows the general framework of
induction. We are given training data on which our algorithm is ex-
pected to learn. This training data is the examples that Alice observes
in her machine learning course, or the historical ratings data for         Figure 1.1: The general supervised ap-
                                                                           proach to machine learning: a learning
the recommender system. Based on this training data, our learning          algorithm reads in training data and
algorithm induces a function f that will map a new example to a cor-       computes a learned function f . This
                                                                           function can then automatically label
responding prediction. For example, our function might guess that          future text examples.
 f (Alice/Machine Learning) might be high because our training data
said that Alice liked Artificial Intelligence. We want our algorithm
to be able to make lots of predictions, so we refer to the collection
of examples on which we will evaluate our algorithm as the test set.
The test set is a closely guarded secret: it is the final exam on which
our learning algorithm is being tested. If our algorithm gets to peek
at it ahead of time, it’s going to cheat and do better than it should.         Why is it bad if the learning algo-
                                                                            ? rithm gets to peek at the test data?
      10   a course in machine learning



          The goal of inductive machine learning is to take some training
      data and use it to induce a function f . This function f will be evalu-
      ated on the test data. The machine learning algorithm has succeeded
      if its performance on the test data is high.


1.2   Some Canonical Learning Problems

      There are a large number of typical inductive learning problems.
      The primary difference between them is in what type of thing they’re
      trying to predict. Here are some examples:
      Regression: trying to predict a real value. For instance, predict the
        value of a stock tomorrow given its past performance. Or predict
        Alice’s score on the machine learning final exam based on her
        homework scores.

      Binary Classification: trying to predict a simple yes/no response.
         For instance, predict whether Alice will enjoy a course or not.
         Or predict whether a user review of the newest Apple product is
         positive or negative about the product.

      Multiclass Classification: trying to put an example into one of a num-
       ber of classes. For instance, predict whether a news story is about
       entertainment, sports, politics, religion, etc. Or predict whether a
       CS course is Systems, Theory, AI or Other.

      Ranking: trying to put a set of objects in order of relevance. For in-
        stance, predicting what order to put web pages in, in response to a
        user query. Or predict Alice’s ranked preferences over courses she
        hasn’t taken.
                                                                                    For each of these types of canon-
         The reason that it is convenient to break machine learning prob-           ical machine learning problems,
                                                                                ?   come up with one or two concrete
      lems down by the type of object that they’re trying to predict has to         examples.
      do with measuring error. Recall that our goal is to build a system
      that can make “good predictions.” This begs the question: what does
      it mean for a prediction to be “good?” The different types of learning
      problems differ in how they define goodness. For instance, in regres-
      sion, predicting a stock price that is off by $0.05 is perhaps much
      better than being off by $200.00. The same does not hold of multi-
      class classification. There, accidentally predicting “entertainment”
      instead of “sports” is no better or worse than predicting “politics.”


1.3   The Decision Tree Model of Learning

      The decision tree is a classic and natural model of learning. It is
      closely related to the fundamental computer science notion of “di-
      vide and conquer.” Although decision trees can be applied to many
                                                                                          decision trees            11



learning problems, we will begin with the simplest case: binary clas-
sification.
    Suppose that your goal is to predict whether some unknown user
will enjoy some unknown course. You must simply answer “yes” or
“no.” In order to make a guess, you’re allowed to ask binary ques-
tions about the user/course under consideration. For example:
    You: Is the course under consideration in Systems?
    Me: Yes
    You: Has this student taken any other Systems courses?
    Me: Yes                                                                 Figure 1.2: A decision tree for a course
    You: Has this student liked most previous Systems courses?              recommender system, from which the
                                                                            in-text “dialog” is drawn.
    Me: No
    You: I predict this student will not like this course.
    The goal in learning is to figure out what questions to ask, in what
order to ask them, and what answer to predict once you have asked
enough questions.
    The decision tree is so-called because we can write our set of ques-
tions and guesses in a tree format, such as that in Figure 1.2. In this
figure, the questions are written in the internal tree nodes (rectangles)
and the guesses are written in the leaves (ovals). Each non-terminal
node has two children: the left child specifies what to do if the an-
swer to the question is “no” and the right child specifies what to do if
it is “yes.”
    In order to learn, I will give you training data. This data consists
of a set of user/course examples, paired with the correct answer for
these examples (did the given user enjoy the given course?). From
this, you must construct your questions. For concreteness, there is a
small data set in Table ?? in the Appendix of this book. This training
data consists of 20 course rating examples, with course ratings and
answers to questions that you might ask about this pair. We will
interpret ratings of 0, +1 and +2 as “liked” and ratings of −2 and −1
as “hated.”
    In what follows, we will refer to the questions that you can ask as
features and the responses to these questions as feature values. The
rating is called the label. An example is just a set of feature values.
And our training data is a set of examples, paired with labels.
    There are a lot of logically possible trees that you could build,
even over just this small number of features (the number is in the
millions). It is computationally infeasible to consider all of these to
try to choose the “best” one. Instead, we will build our decision tree
greedily. We will begin by asking:
    If I could only ask one question, what question would I ask?
    You want to find a feature that is most useful in helping you guess
whether this student will enjoy this course.1 A useful way to think

                                                                            Figure 1.3: A histogram of labels for (a)
                                                                            the entire data set; (b-e) the examples
                                                                            in the data set for each value of the first
                                                                            four features.
                                                                            1
                                                                              A colleague related the story of
                                                                            getting his 8-year old nephew to
                                                                            guess a number between 1 and 100.
                                                                            His nephew’s first four questions
                                                                            were: Is it bigger than 20? (YES) Is
12   a course in machine learning



about this is to look at the histogram of labels for each feature. This
is shown for the first four features in Figure 1.3. Each histogram
shows the frequency of “like”/“hate” labels for each possible value
of an associated feature. From this figure, you can see that asking
the first feature is not useful: if the value is “no” then it’s hard to
guess the label; similarly if the answer is “yes.” On the other hand,
asking the second feature is useful: if the value is “no,” you can be
pretty confident that this student will hate this course; if the answer
is “yes,” you can be pretty confident that this student will like this
course.
    More formally, you will consider each feature in turn. You might
consider the feature “Is this a System’s course?” This feature has two
possible value: no and yes. Some of the training examples have an
answer of “no” – let’s call that the “NO” set. Some of the training
examples have an answer of “yes” – let’s call that the “YES” set. For
each set (NO and YES) we will build a histogram over the labels.
This is the second histogram in Figure 1.3. Now, suppose you were
to ask this question on a random example and observe a value of
“no.” Further suppose that you must immediately guess the label for
this example. You will guess “like,” because that’s the more preva-
lent label in the NO set (actually, it’s the only label in the NO set).
Alternatively, if you recieve an answer of “yes,” you will guess “hate”
because that is more prevalent in the YES set.
    So, for this single feature, you know what you would guess if you
had to. Now you can ask yourself: if I made that guess on the train-
ing data, how well would I have done? In particular, how many ex-
amples would I classify correctly? In the NO set (where you guessed
“like”) you would classify all 10 of them correctly. In the YES set
(where you guessed “hate”) you would classify 8 (out of 10) of them
correctly. So overall you would classify 18 (out of 20) correctly. Thus,
we’ll say that the score of the “Is this a System’s course?” question is
18/20.                                                                         How many training examples
    You will then repeat this computation for each of the available            would you classify correctly for
                                                                           ?   each of the other three features
features to us, compute the scores for each of them. When you must             from Figure 1.3?
choose which feature consider first, you will want to choose the one
with the highest score.
    But this only lets you choose the first feature to ask about. This
is the feature that goes at the root of the decision tree. How do we
choose subsequent features? This is where the notion of divide and
conquer comes in. You’ve already decided on your first feature: “Is
this a Systems course?” You can now partition the data into two parts:
the NO part and the YES part. The NO part is the subset of the data
on which value for this feature is “no”; the YES half is the rest. This
is the divide step.
                                                                                                          decision trees          13



Algorithm 1 DecisionTreeTrain(data, remaining features)
  1: guess ← most frequent answer in data                  // default answer for this data
  2: if the labels in data are unambiguous then
  3:     return Leaf(guess)                      // base case: no need to split further
  4: else if remaining features is empty then

  5:     return Leaf(guess)                            // base case: cannot split further
  6: else                                           // we need to query more features
  7:     for all f ∈ remaining features do
  8:       NO ← the subset of data on which f =no
  9:       YES ← the subset of data on which f =yes
 10:       score[f ] ← # of majority vote answers in NO
 11:                  + # of majority vote answers in YES
                                    // the accuracy we would get if we only queried on f
 12:   end for
 13:   f ← the feature with maximal score(f )
 14:   NO ← the subset of data on which f =no
 15:   YES ← the subset of data on which f =yes
 16:   left ← DecisionTreeTrain(NO, remaining features \ {f })
 17:   right ← DecisionTreeTrain(YES, remaining features \ {f })
 18:   return Node(f , left, right)
 19: end if




Algorithm 2 DecisionTreeTest(tree, test point)
  1: if tree is of the form Leaf(guess) then
  2:     return guess
  3: else if tree is of the form Node(f , left, right) then

  4:     if f = yes in test point then
  5:         return DecisionTreeTest(left, test point)
  6:     else
  7:         return DecisionTreeTest(right, test point)
  8:     end if
  9: end if




   The conquer step is to recurse, and run the same routine (choosing
the feature with the highest score) on the NO set (to get the left half
of the tree) and then separately on the YES set (to get the right half of
the tree).
   At some point it will become useless to query on additional fea-
tures. For instance, once you know that this is a Systems course,
you know that everyone will hate it. So you can immediately predict
“hate” without asking any additional questions. Similarly, at some
point you might have already queried every available feature and still
not whittled down to a single answer. In both cases, you will need to
create a leaf node and guess the most prevalent answer in the current
piece of the training data that you are looking at.
   Putting this all together, we arrive at the algorithm shown in Al-
gorithm 1.3.2 This function, DecisionTreeTrain takes two argu-                               2
                                                                                              There are more nuanced algorithms
                                                                                             for building decision trees, some of
                                                                                             which are discussed in later chapters of
                                                                                             this book. They primarily differ in how
                                                                                             they compute the score funciton.
      14   a course in machine learning



      ments: our data, and the set of as-yet unused features. It has two
      base cases: either the data is unambiguous, or there are no remaining
      features. In either case, it returns a Leaf node containing the most
      likely guess at this point. Otherwise, it loops over all remaining fea-
      tures to find the one with the highest score. It then partitions the data
      into a NO/YES split based on the best feature. It constructs its left
      and right subtrees by recursing on itself. In each recursive call, it uses
      one of the partitions of the data, and removes the just-selected feature
      from consideration.                                                                Is Algorithm 1.3 guaranteed to
         The corresponding prediction algorithm is shown in Algorithm 1.3.            ? terminate?
      This function recurses down the decision tree, following the edges
      specified by the feature values in some test point. When it reaches a
      leaf, it returns the guess associated with that leaf.
         TODO: define outlier somewhere!


1.4   Formalizing the Learning Problem

      As you’ve seen, there are several issues that we must take into ac-
      count when formalizing the notion of learning.

      • The performance of the learning algorithm should be measured on
        unseen “test” data.

      • The way in which we measure performance should depend on the
        problem we are trying to solve.

      • There should be a strong relationship between the data that our
        algorithm sees at training time and the data it sees at test time.

         In order to accomplish this, let’s assume that someone gives us a
      loss function, `(·, ·), of two arguments. The job of ` is to tell us how
      “bad” a system’s prediction is in comparison to the truth. In particu-
      lar, if y is the truth and ŷ is the system’s prediction, then `(y, ŷ) is a
      measure of error.
         For three of the canonical tasks discussed above, we might use the
      following loss functions:

      Regression: squared loss `(y, ŷ) = (y − ŷ)2
        or absolute loss `(y, ŷ) = |y − ŷ|.
                                                        (
                                                            0   if y = ŷ            This notation means that the loss is zero
      Binary Classification: zero/one loss `(y, ŷ) =                                if the prediction is correct and is one
                                                            1   otherwise
                                                                                     otherwise.
      Multiclass Classification: also zero/one loss.
                                                                                         Why might it be a bad idea to use
        Note that the loss function is something that you must decide on              ? zero/one loss to measure perfor-
      based on the goals of learning.                                                    mance for a regression problem?
                                                                                         decision trees          15



    Now that we have defined our loss function, we need to consider
where the data (training and test) comes from. The model that we
will use is the probabilistic model of learning. Namely, there is a prob-
ability distribution D over input/output pairs. This is often called
the data generating distribution. If we write x for the input (the
user/course pair) and y for the output (the rating), then D is a distri-
bution over ( x, y) pairs.
    A useful way to think about D is that it gives high probability to
reasonable ( x, y) pairs, and low probability to unreasonable ( x, y)
pairs. A ( x, y) pair can be unreasonable in two ways. First, x might
be an unusual input. For example, a x related to an “Intro to Java”
course might be highly probable; a x related to a “Geometric and
Solid Modeling” course might be less probable. Second, y might
be an unusual rating for the paired x. For instance, if Alice were to
take AI 100 times (without remembering that she took it before!),
she would give the course a +2 almost every time. Perhaps some
semesters she might give a slightly lower score, but it would be un-
likely to see x =Alice/AI paired with y = −2.
    It is important to remember that we are not making any assump-
tions about what the distribution D looks like. (For instance, we’re
not assuming it looks like a Gaussian or some other, common distri-
bution.) We are also not assuming that we know what D is. In fact,
if you know a priori what your data generating distribution is, your
learning problem becomes significantly easier. Perhaps the hardest
thing about machine learning is that we don’t know what D is: all we
get is a random sample from it. This random sample is our training
data.
    Our learning problem, then, is defined by two quantities:                   Consider the following prediction
                                                                                task. Given a paragraph written
1. The loss function `, which captures our notion of what is important          about a course, we have to predict
                                                                                whether the paragraph is a positive
   to learn.
                                                                            ?   or negative review of the course.
                                                                                (This is the sentiment analysis prob-
2. The data generating distribution D , which defines what sort of              lem.) What is a reasonable loss
   data we expect to see.                                                       function? How would you define
                                                                                the data generating distribution?
   We are given access to training data, which is a random sample of
input/output pairs drawn from D . Based on this training data, we
need to induce a function f that maps new inputs x̂ to corresponding
prediction ŷ. The key property that f should obey is that it should do
well (as measured by `) on future examples that are also drawn from
D . Formally, it’s expected loss e over D with repsect to ` should be
as small as possible:

  e , E( x,y)∼D `(y, f ( x)) = ∑ D( x, y)`(y, f ( x))
                           
                                                                    (1.1)
                               ( x,y)
      16     a course in machine learning



          The difficulty in minimizing our expected loss from Eq (1.1) is
      that we don’t know what D is! All we have access to is some training
      data sampled from it! Suppose that we denote our training data
      set by D. The training data consists of N-many input/output pairs,
      ( x1 , y1 ), ( x2 , y2 ), . . . , ( x N , y N ). Given a learned function f , we can
      compute our training error, ê:

                  1 N
                  N n∑
           ê ,         `(yn , f ( xn ))                                               (1.8)
                     =1

         That is, our training error is simply our average error over the train-
      ing data.                                                                                    Verify by calculation that we
         Of course, we can drive ê to zero by simply memorizing our train-                        can write our training error as
                                                                                                   E( x,y)∼ D `(y, f ( x)) , by thinking
      ing data. But as Alice might find in memorizing past exams, this
                                                                                               ?   of D as a distribution that places
      might not generalize well to a new exam!                                                     probability 1/N to each example in
         This is the fundamental difficulty in machine learning: the thing                         D and probabiliy 0 on everything
                                                                                                   else.
      we have access to is our training error, ê. But the thing we care about
      minimizing is our expected error e. In order to get the expected error
      down, our learned function needs to generalize beyond the training
      data to some future data that it might not have seen yet!
         So, putting it all together, we get a formal definition of induction
      machine learning: Given (i) a loss function ` and (ii) a sample D
      from some unknown distribution D , you must compute a function
      f that has low expected error e over D with respect to `.


1.5   Inductive Bias: What We Know Before the Data Arrives
                                                                                               decision trees     17



M ATH R EVIEW | E XPECTATED VALUES
In this book, we will often write things like E( x,y)∼D [`(y, f ( x))] for the expected loss. Here, as always,
expectation means “average.” In words, this is saying “if you drew a bunch of ( x, y) pairs indepen-
dently at random from D , what would your average loss be? (More formally, what would be the aver-
age of `(y, f ( x)) be over these random draws?)

More formally, if D is a discrete probability distribution, then this expectation can be expanded as:


   E( x,y)∼D [`(y, f ( x))] =        ∑ [D(x, y)`(y, f (x))]                                                   (1.2)
                                 ( x,y)∈D

This is exactly the weighted average loss over the all ( x, y) pairs in D , weighted by their probability
(namely, D( x, y)) under this distribution D .

In particular, if D is a finite discrete distribution, for instance one defined by a finite data set
{( x1 , y1 ), . . . , ( x N , y N ) that puts equal weight on each example (in this case, equal weight means proba-
bility 1/N), then we get:


   E( x,y)∼ D [`(y, f ( x))] =       ∑ [ D(x, y)`(y, f (x))]                    definition of expectation     (1.3)
                                 ( x,y)∈ D
                                  N
                           = ∑ [ D ( xn , yn )`(yn , f ( xn ))]                  D is discrete and finite     (1.4)
                                 n =1
                                  N
                                      1
                           = ∑[         `(yn , f ( xn ))]                                 definition of D     (1.5)
                                 n =1
                                      N
                                 1 N
                                 N n∑
                           =           [`(yn , f ( xn ))]                             rearranging terms       (1.6)
                                    =1

Which is exactly the average loss on that dataset.

In the case that the distribution is continuous, we need to replace the discrete sum with a continuous
integral over some space Ω:

                                 Z
   E( x,y)∼D [`(y, f ( x))] =            D( x, y)`(y, f ( x))dxdy                                             (1.7)
                                     Ω

This is exactly the same but in continuous space rather than discrete space.
The most important thing to remember is that there are two equivalent ways to think about expections:

1. The expectation of some function g is the weighted average value of g, where the weights are given by
   the underlying probability distribution.

2. The expectation of some function g is your best guess of the value of g if you were to draw a single
   item from the underlying probability distribution.



                                                                                 Figure 1.4:
18   a course in machine learning




   In Figure 1.5 you’ll find training data for a binary classification
problem. The two labels are “A” and “B” and you can see five exam-
ples for each label. Below, in Figure 1.6, you will see some test data.
These images are left unlabeled. Go through quickly and, based on
the training data, label these images. (Really do it before you read
further! I’ll wait!)
   Most likely you produced one of two labelings: either ABBAAB or
ABBABA. Which of these solutions is right?
   The answer is that you cannot tell based on the training data. If
you give this same example to 100 people, 60 − 70 of them come up
with the ABBAAB prediction and 30 − 40 come up with the ABBABA              Figure 1.5: dt:bird: bird training
prediction. Why are they doing this? Presumably because the first           images
group believes that the relevant distinction is between “bird” and
“non-bird” while the second group believes that the relevant distinc-
tion is between “fly” and “no-fly.”
   This preference for one distinction (bird/non-bird) over another
(fly/no-fly) is a bias that different human learners have. In the con-
text of machine learning, it is called inductive bias: in the absense of
data that narrow down the relevant concept, what type of solutions
are we more likely to prefer? Two thirds of people seem to have an
inductive bias in favor of bird/non-bird, and one third seem to have
an inductive bias in favor of fly/no-fly.
   Throughout this book you will learn about several approaches to
                                                                            Figure 1.6: dt:birdtest: bird test
machine learning. The decision tree model is the first such approach.       images
These approaches differ primarily in the sort of inductive bias that             It is also possible that the correct
they exhibit.                                                                    classification on the test data is
   Consider a variant of the decision tree learning algorithm. In this           BABAAA. This corresponds to the
                                                                             ?   bias “is the background in focus.”
variant, we will not allow the trees to grow beyond some pre-defined             Somehow no one seems to come up
maximum depth, d. That is, once we have queried on d-many fea-                   with this classification rule.
tures, we cannot query on any more and must just make the best
guess we can at that point. This variant is called a shallow decision
tree.
   The key question is: What is the inductive bias of shallow decision
trees? Roughly, their bias is that decisions can be made by only look-
ing at a small number of features. For instance, a shallow decision
tree would be very good at learning a function like “students only
like AI courses.” It would be very bad at learning a function like “if
this student has liked an odd number of his past courses, he will like
the next one; otherwise he will not.” This latter is the parity function,
which requires you to inspect every feature to make a prediction. The
inductive bias of a decision tree is that the sorts of things we want
to learn to predict are more like the first example and less like the
second example.
                                                                                 decision trees   19



1.6   Not Everything is Learnable

      Although machine learning works well—perhaps astonishingly
      well—in many cases, it is important to keep in mind that it is not
      magical. There are many reasons why a machine learning algorithm
      might fail on some learning task.
         There could be noise in the training data. Noise can occur both
      at the feature level and at the label level. Some features might corre-
      spond to measurements taken by sensors. For instance, a robot might
      use a laser range finder to compute its distance to a wall. However,
      this sensor might fail and return an incorrect value. In a sentiment
      classification problem, someone might have a typo in their review of
      a course. These would lead to noise at the feature level. There might
      also be noise at the label level. A student might write a scathingly
      negative review of a course, but then accidentally click the wrong
      button for the course rating.
         The features available for learning might simply be insufficient.
      For example, in a medical context, you might wish to diagnose
      whether a patient has cancer or not. You may be able to collect a
      large amount of data about this patient, such as gene expressions,
      X-rays, family histories, etc. But, even knowing all of this information
      exactly, it might still be impossible to judge for sure whether this pa-
      tient has cancer or not. As a more contrived example, you might try
      to classify course reviews as positive or negative. But you may have
      erred when downloading the data and only gotten the first five char-
      acters of each review. If you had the rest of the features you might
      be able to do well. But with this limited feature set, there’s not much
      you can do.
         Some examples may not have a single correct answer. You might
      be building a system for “safe web search,” which removes offen-
      sive web pages from search results. To build this system, you would
      collect a set of web pages and ask people to classify them as “offen-
      sive” or not. However, what one person considers offensive might be
      completely reasonable for another person. It is common to consider
      this as a form of label noise. Nevertheless, since you, as the designer
      of the learning system, have some control over this problem, it is
      sometimes helpful to isolate it as a source of difficulty.
         Finally, learning might fail because the inductive bias of the learn-
      ing algorithm is too far away from the concept that is being learned.
      In the bird/non-bird data, you might think that if you had gotten
      a few more training examples, you might have been able to tell
      whether this was intended to be a bird/non-bird classification or a
      fly/no-fly classification. However, no one I’ve talked to has ever come
      up with the “background is in focus” classification. Even with many
      20   a course in machine learning



      more training points, this is such an unusual distinction that it may
      be hard for anyone to figure out it. In this case, the inductive bias of
      the learner is simply too misaligned with the target classification to
      learn.
          Note that the inductive bias source of error is fundamentally dif-
      ferent than the other three sources of error. In the inductive bias case,
      it is the particular learning algorithm that you are using that cannot
      cope with the data. Maybe if you switched to a different learning
      algorithm, you would be able to learn well. For instance, Neptunians
      might have evolved to care greatly about whether backgrounds are
      in focus, and for them this would be an easy classification to learn.
      For the other three sources of error, it is not an issue to do with the
      particular learning algorithm. The error is a fundamental part of the
      learning problem.


1.7   Underfitting and Overfitting

      As with many problems, it is useful to think about the extreme cases
      of learning algorithms. In particular, the extreme cases of decision
      trees. In one extreme, the tree is “empty” and we do not ask any
      questions at all. We simply immediately make a prediction. In the
      other extreme, the tree is “full.” That is, every possible question
      is asked along every branch. In the full tree, there may be leaves
      with no associated training data. For these we must simply choose
      arbitrarily whether to say “yes” or “no.”
         Consider the course recommendation data from Table ??. Sup-
      pose we were to build an “empty” decision tree on this data. Such a
      decision tree will make the same prediction regardless of its input,
      because it is not allowed to ask any questions about its input. Since
      there are more “likes” than “hates” in the training data (12 versus
      8), our empty decision tree will simply always predict “likes.” The
      training error, ê, is 8/20 = 40%.
         On the other hand, we could build a “full” decision tree. Since
      each row in this data is unique, we can guarantee that any leaf in a
      full decision tree will have either 0 or 1 examples assigned to it (20
      of the leaves will have one example; the rest will have none). For the
      leaves corresponding to training points, the full decision tree will
      always make the correct prediction. Given this, the training error, ê, is
      0/20 = 0%.
         Of course our goal is not to build a model that gets 0% error on
      the training data. This would be easy! Our goal is a model that will
      do well on future, unseen data. How well might we expect these two
      models to do on future data? The “empty” tree is likely to do not
      much better and not much worse on future data. We might expect
                                                                                              decision trees           21



      that it would continue to get around 40% error.
          Life is more complicated for the “full” decision tree. Certainly
      if it is given a test example that is identical to one of the training
      examples, it will do the right thing (assuming no noise). But for
      everything else, it will only get about 50% error. This means that
      even if every other test point happens to be identical to one of the
      training points, it would only get about 25% error. In practice, this is
      probably optimistic, and maybe only one in every 10 examples would
      match a training example, yielding a 35% error.                                Convince yourself (either by proof
          So, in one case (empty tree) we’ve achieved about 40% error and            or by simulation) that even in the
                                                                                     case of imbalanced data – for in-
      in the other case (full tree) we’ve achieved 35% error. This is not            stance data that is on average 80%
      very promising! One would hope to do better! In fact, you might            ?   positive and 20% negative – a pre-
      notice that if you simply queried on a single feature for this data, you       dictor that guesses randomly (50/50
                                                                                     positive/negative) will get about
      would be able to get very low training error, but wouldn’t be forced           50% error.
      to “guess” randomly.
          This example illustrates the key concepts of underfitting and
      overfitting. Underfitting is when you had the opportunity to learn
      something but didn’t. A student who hasn’t studied much for an up-
      coming exam will be underfit to the exam, and consequently will not
                                                                                     Which feature is it, and what is it’s
      do well. This is also what the empty tree does. Overfitting is when        ? training error?
      you pay too much attention to idiosyncracies of the training data,
      and aren’t able to generalize well. Often this means that your model
      is fitting noise, rather than whatever it is supposed to fit. A student
      who memorizes answers to past exam questions without understand-
      ing them has overfit the training data. Like the full tree, this student
      also will not do well on the exam. A model that is neither overfit nor
      underfit is the one that is expected to do best in the future.


1.8   Separation of Training and Test Data

      Suppose that, after graduating, you get a job working for a company
      that provides personalized recommendations for pottery. You go in
      and implement new algorithms based on what you learned in your
      machine learning class (you have learned the power of generaliza-
      tion!). All you need to do now is convince your boss that you have
      done a good job and deserve a raise!
         How can you convince your boss that your fancy learning algo-
      rithms are really working?
         Based on what we’ve talked about already with underfitting and
      overfitting, it is not enough to just tell your boss what your training
      error is. Noise notwithstanding, it is easy to get a training error of
      zero using a simple database query (or grep, if you prefer). Your boss
      will not fall for that.
         The easiest approach is to set aside some of your available data as
      22   a course in machine learning



      “test data” and use this to evaluate the performance of your learning
      algorithm. For instance, the pottery recommendation service that you
      work for might have collected 1000 examples of pottery ratings. You
      will select 800 of these as training data and set aside the final 200
      as test data. You will run your learning algorithms only on the 800
      training points. Only once you’re done will you apply your learned
      model to the 200 test points, and report your test error on those 200
      points to your boss.
         The hope in this process is that however well you do on the 200
      test points will be indicative of how well you are likely to do in the
      future. This is analogous to estimating support for a presidential
      candidate by asking a small (random!) sample of people for their
      opinions. Statistics (specifically, concentration bounds of which the
      “Central limit theorem” is a famous example) tells us that if the sam-
      ple is large enough, it will be a good representative. The 80/20 split
      is not magic: it’s simply fairly well established. Occasionally people
      use a 90/10 split instead, especially if they have a lot of data.              If you have more data at your dis-
         The cardinal rule of machine learning is: never touch your test         ?   posal, why might a 90/10 split be
                                                                                     preferable to an 80/20 split?
      data. Ever. If that’s not clear enough:

       Never ever touch your test data!
         If there is only one thing you learn from this book, let it be that.
      Do not look at your test data. Even once. Even a tiny peek. Once
      you do that, it is not test data any more. Yes, perhaps your algorithm
      hasn’t seen it. But you have. And you are likely a better learner than
      your learning algorithm. Consciously or otherwise, you might make
      decisions based on whatever you might have seen. Once you look at
      the test data, your model’s performance on it is no longer indicative
      of it’s performance on future unseen data. This is simply because
      future data is unseen, but your “test” data no longer is.



1.9   Models, Parameters and Hyperparameters

      The general approach to machine learning, which captures many ex-
      isting learning algorithms, is the modeling approach. The idea is that
      we come up with some formal model of our data. For instance, we
      might model the classification decision of a student/course pair as a
      decision tree. The choice of using a tree to represent this model is our
      choice. We also could have used an arithmetic circuit or a polynomial
      or some other function. The model tells us what sort of things we can
      learn, and also tells us what our inductive bias is.
          For most models, there will be associated parameters. These are
      the things that we use the data to decide on. Parameters in a decision
                                                                                                 decision trees         23



tree include: the specific questions we asked, the order in which we
asked them, and the classification decisions at the leaves. The job of
our decision tree learning algorithm DecisionTreeTrain is to take
data and figure out a good set of parameters.
    Many learning algorithms will have additional knobs that you can
adjust. In most cases, these knobs amount to tuning the inductive
bias of the algorithm. In the case of the decision tree, an obvious
knob that one can tune is the maximum depth of the decision tree.
That is, we could modify the DecisionTreeTrain function so that
it stops recursing once it reaches some pre-defined maximum depth.
By playing with this depth knob, we can adjust between underfitting
(the empty tree, depth= 0) and overfitting (the full tree, depth= ∞).                    Go back to the DecisionTree-
    Such a knob is called a hyperparameter. It is so called because it                   Train algorithm and modify it so
                                                                                         that it takes a maximum depth pa-
is a parameter that controls other parameters of the model. The exact                ?   rameter. This should require adding
definition of hyperparameter is hard to pin down: it’s one of those                      two lines of code and modifying
things that are easier to identify than define. However, one of the                      three others.

key identifiers for hyperparameters (and the main reason that they
cause consternation) is that they cannot be naively adjusted using the
training data.
    In DecisionTreeTrain, as in most machine learning, the learn-
ing algorithm is essentially trying to adjust the parameters of the
model so as to minimize training error. This suggests an idea for
choosing hyperparameters: choose them so that they minimize train-
ing error.
    What is wrong with this suggestion? Suppose that you were to
treat “maximum depth” as a hyperparameter and tried to tune it on
your training data. To do this, maybe you simply build a collection
of decision trees, tree0 , tree1 , tree2 , . . . , tree100 , where treed is a tree
of maximum depth d. We then computed the training error of each
of these trees and chose the “ideal” maximum depth as that which
minimizes training error? Which one would it pick?
    The answer is that it would pick d = 100. Or, in general, it would
pick d as large as possible. Why? Because choosing a bigger d will
never hurt on the training data. By making d larger, you are simply
encouraging overfitting. But by evaluating on the training data, over-
fitting actually looks like a good idea!
    An alternative idea would be to tune the maximum depth on test
data. This is promising because test data peformance is what we
really want to optimize, so tuning this knob on the test data seems
like a good idea. That is, it won’t accidentally reward overfitting. Of
course, it breaks our cardinal rule about test data: that you should
never touch your test data. So that idea is immediately off the table.
    However, our “test data” wasn’t magic. We simply took our 1000
examples, called 800 of them “training” data and called the other 200
     24    a course in machine learning



     “test” data. So instead, let’s do the following. Let’s take our original
     1000 data points, and select 700 of them as training data. From the
     remainder, take 100 as development data3 and the remaining 200               3
                                                                                   Some people call this “validation
     as test data. The job of the development data is to allow us to tune         data” or “held-out data.”

     hyperparameters. The general approach is as follows:

     1. Split your data into 70% training data, 10% development data and
        20% test data.

     2. For each possible setting of your hyperparameters:

          (a) Train a model using that setting of hyperparameters on the
             training data.
          (b) Compute this model’s error rate on the development data.

     3. From the above collection of models, choose the one that achieved
        the lowest error rate on development data.

     4. Evaluate that model on the test data to estimate future test perfor-
        mance.
                                                                                          In step 3, you could either choose
                                                                                          the model (trained on the 70% train-
                                                                                          ing data) that did the best on the
1.10 Chapter Summary and Outlook                                                          development data. Or you could
                                                                                      ?   choose the hyperparameter settings
                                                                                          that did best and retrain the model
     At this point, you should be able to use decision trees to do machine                on the 80% union of training and
     learning. Someone will give you data. You’ll split it into training,                 development data. Is either of these
                                                                                          options obviously better or worse?
     development and test portions. Using the training and development
     data, you’ll find a good value for maximum depth that trades off
     between underfitting and overfitting. You’ll then run the resulting
     decision tree model on the test data to get an estimate of how well
     you are likely to do in the future.
        You might think: why should I read the rest of this book? Aside
     from the fact that machine learning is just an awesome fun field to
     learn about, there’s a lot left to cover. In the next two chapters, you’ll
     learn about two models that have very different inductive biases than
     decision trees. You’ll also get to see a very useful way of thinking
     about learning: the geometric view of data. This will guide much of
     what follows. After that, you’ll learn how to solve problems more
     complicated that simple binary classification. (Machine learning
     people like binary classification a lot because it’s one of the simplest
     non-trivial problems that we can work on.) After that, things will
     diverge: you’ll learn about ways to think about learning as a formal
     optimization problem, ways to speed up learning, ways to learn
     without labeled data (or with very little labeled data) and all sorts of
     other fun topics.
                                                                               decision trees   25



         But throughout, we will focus on the view of machine learning
     that you’ve seen here. You select a model (and its associated induc-
     tive biases). You use data to find parameters of that model that work
     well on the training data. You use development data to avoid under-
     fitting and overfitting. And you use test data (which you’ll never look
     at or touch, right?) to estimate future model performance. Then you
     conquer the world.


1.11 Exercises

     Exercise 1.1. TODO. . .
                   2 | G EOMETRY AND N EAREST N EIGHBORS

        Our brains have evolved to get us out of the rain, find where the        Learning Objectives:
        berries are, and keep us from getting killed. Our brains did not         • Describe a data set as points in a
                                                                                   high dimensional space.
        evolve to help us grasp really large numbers or to look at things in
                                                                                 • Explain the curse of dimensionality.
        a hundred thousand dimensions.                      – Ronald Graham
                                                                                 • Compute distances between points
                                                                                   in high dimensional space.
                                                                                 • Implement a K-nearest neighbor
                                                                                   model of learning.
                                                                                 • Draw decision boundaries.
      You can think of prediction tasks as mapping inputs (course
                                                                                 • Implement the K-means algorithm
      reviews) to outputs (course ratings). As you learned in the previ-           for clustering.
      ous chapter, decomposing an input into a collection of features (e.g.,
      words that occur in the review) forms a useful abstraction for learn-
      ing. Therefore, inputs are nothing more than lists of feature values.
      This suggests a geometric view of data, where we have one dimen-
      sion for every feature. In this view, examples are points in a high-
      dimensional space.
          Once we think of a data set as a collection of points in high dimen-
      sional space, we can start performing geometric operations on this
      data. For instance, suppose you need to predict whether Alice will
      like Algorithms. Perhaps we can try to find another student who is
                                                                                 Dependencies: Chapter 1
      most “similar” to Alice, in terms of favorite courses. Say this student
      is Jeremy. If Jeremy liked Algorithms, then we might guess that Alice
      will as well. This is an example of a nearest neighbor model of learn-
      ing. By inspecting this model, we’ll see a completely different set of
      answers to the key learning questions we discovered in Chapter 1.


2.1   From Data to Feature Vectors

      An example is just a collection of feature values about that example,
      for instance the data in Table ?? from the Appendix. To a person,
      these features have meaning. One feature might count how many
      times the reviewer wrote “excellent” in a course review. Another
      might count the number of exclamation points. A third might tell us
      if any text is underlined in the review.
          To a machine, the features themselves have no meaning. Only
      the feature values, and how they vary across examples, mean some-
      thing to the machine. From this perspective, you can think about an
      example as being represented by a feature vector consisting of one
      “dimension” for each feature, where each dimenion is simply some
      real value.
          Consider a review that said “excellent” three times, had one excla-
                                                                        geometry and nearest neighbors                 27



mation point and no underlined text. This could be represented by
the feature vector h3, 1, 0i. An almost identical review that happened
to have underlined text would have the feature vector h3, 1, 1i.
   Note, here, that we have imposed the convention that for binary
features (yes/no features), the corresponding feature values are 0
and 1, respectively. This was an arbitrary choice. We could have
made them 0.92 and −16.1 if we wanted. But 0/1 is convenient and
helps us interpret the feature values. When we discuss practical
issues in Chapter 4, you will see other reasons why 0/1 is a good
choice.
   Figure 2.1 shows the data from Table ?? in three views. These
three views are constructed by considering two features at a time in
different pairs. In all cases, the plusses denote positive examples and
the minuses denote negative examples. In some cases, the points fall
on top of each other, which is why you cannot see 20 unique points
in all figures.
   The mapping from feature values to vectors is straighforward in               Figure 2.1: A figure showing projections
the case of real valued features (trivial) and binary features (mapped           of data in two dimension in three
                                                                                 ways – see text. Top: horizontal axis
to zero or one). It is less clear what to do with categorical features.
                                                                                 corresponds to the first feature (TODO)
For example, if our goal is to identify whether an object in an image            and the vertical axis corresponds to
is a tomato, blueberry, cucumber or cockroach, we might want to                  the second feature (TODO); Middle:
                                                                                 horizonal is second feature and vertical
know its color: is it Red, Blue, Green or Black?                                 is third; Bottom: horizonal is first and
   One option would be to map Red to a value of 0, Blue to a value               vertical is third.
of 1, Green to a value of 2 and Black to a value of 3. The problem                    Match the example ids from Ta-
                                                                                  ? ble ?? with the points in Figure 2.1.
with this mapping is that it turns an unordered set (the set of colors)
into an ordered set (the set {0, 1, 2, 3}). In itself, this is not necessarily
a bad thing. But when we go to use these features, we will measure
examples based on their distances to each other. By doing this map-
ping, we are essentially saying that Red and Blue are more similar
(distance of 1) than Red and Black (distance of 3). This is probably
not what we want to say!
   A solution is to turn a categorical feature that can take four dif-
ferent values (say: Red, Blue, Green and Black) into four binary
features (say: IsItRed?, IsItBlue?, IsItGreen? and IsItBlack?). In gen-
eral, if we start from a categorical feature that takes V values, we can
map it to V-many binary indicator features.                                           The computer scientist in you might
   With that, you should be able to take a data set and map each                      be saying: actually we could map it
                                                                                  ?   to log2 V-many binary features! Is
example to a feature vector through the following mapping:                            this a good idea or not?

• Real-valued features get copied directly.

• Binary features become 0 (for false) or 1 (for true).

• Categorical features with V possible values get mapped to V-many
  binary indicator features.
      28     a course in machine learning



         After this mapping, you can think of a single example as a vec-
      tor in a high-dimensional feature space. If you have D-many fea-
      tures (after expanding categorical features), then this feature vector
      will have D-many components. We will denote feature vectors as
      x = h x1 , x2 , . . . , x D i, so that xd denotes the value of the dth fea-
      ture of x. Since these are vectors with real-valued components in
      D-dimensions, we say that they belong to the space RD .
         For D = 2, our feature vectors are just points in the plane, like in
      Figure 2.1. For D = 3 this is three dimensional space. For D > 3 it
      becomes quite hard to visualize. (You should resist the temptation
      to think of D = 4 as “time” – this will just make things confusing.)
      Unfortunately, for the sorts of problems you will encounter in ma-
      chine learning, D ≈ 20 is considered “low dimensional,” D ≈ 1000 is
      “medium dimensional” and D ≈ 100000 is “high dimensional.”                          Can you think of problems (per-
                                                                                          haps ones already mentioned in this
                                                                                      ?   book!) that are low dimensional?
                                                                                          That are medium dimensional?
2.2   K-Nearest Neighbors                                                                 That are high dimensional?

      The biggest advantage to thinking of examples as vectors in a high
      dimensional space is that it allows us to apply geometric concepts
      to machine learning. For instance, one of the most basic things
      that one can do in a vector space is compute distances. In two-
      dimensional space, the distance between h2, 3i and h6, 1i is given
         p                         √
      by (2 − 6)2 + (3 − 1)2 = 18 ≈ 4.24. In general, in D-dimensional
      space, the Euclidean distance between vectors a and b is given by
      Eq (2.1) (see Figure 2.2 for geometric intuition in three dimensions):


                        "                        #1
                             D                    2
           d( a, b) =       ∑ ( a d − bd )   2
                                                                             (2.1)
                            d =1

         Now that you have access to distances between examples, you
      can start thinking about what it means to learn again. Consider Fig-
      ure 2.3. We have a collection of training data consisting of positive
      examples and negative examples. There is a test point marked by a
      question mark. Your job is to guess the correct label for that point.          Figure 2.2: A figure showing Euclidean
                                                                                     distance in three dimensions
         Most likely, you decided that the label of this test point is positive.
                                                                                          Verify that d from Eq (2.1) gives the
      One reason why you might have thought that is that you believe
                                                                                      ? same result (4.24) for the previous
      that the label for an example should be similar to the label of nearby              computation.
      points. This is an example of a new form of inductive bias.
         The nearest neighbor classifier is build upon this insight. In com-
      parison to decision trees, the algorithm is ridiculously simple. At
      training time, we simply store the entire training set. At test time,
      we get a test example x̂. To predict its label, we find the training ex-
      ample x that is most similar to x̂. In particular, we find the training
                                                                                  geometry and nearest neighbors                   29



Algorithm 3 KNN-Predict(D, K, x̂)
  1: S ← [ ]

  2: for n = 1 to N do

  3:    S ← S ⊕ hd(xn , x̂), ni                 // store distance to training example n
  4: end for

  5: S ← sort(S)                                     // put lowest-distance objects first
  6: ŷ ← 0

  7: for k = 1 to K do

  8:    hdist,ni ← Sk                             // n this is the kth closest data point
  9:    ŷ ← ŷ + yn            // vote according to the label for the nth training point
 10: end for

 11: return sign(ŷ)                             // return +1 if ŷ > 0 and −1 if ŷ < 0



example x that minimizes d( x, x̂). Since x is a training example, it has
a corresponding label, y. We predict that the label of x̂ is also y.
    Despite its simplicity, this nearest neighbor classifier is incred-
ibly effective. (Some might say frustratingly effective.) However, it
is particularly prone to overfitting label noise. Consider the data in
Figure 2.4. You would probably want to label the test point positive.
Unfortunately, it’s nearest neighbor happens to be negative. Since the
nearest neighbor algorithm only looks at the single nearest neighbor,
it cannot consider the “preponderance of evidence” that this point
should probably actually be a positive example. It will make an un-
                                                                                            Figure 2.4: A figure showing an easy
necessary error.                                                                            NN classification problem where the
    A solution to this problem is to consider more than just the single                     test point is a ? and should be positive,
                                                                                            but its NN is actually a negative point
nearest neighbor when making a classification decision. We can con-                         that’s noisy.
sider the K-nearest neighbors and let them vote on the correct class
for this test point. If you consider the 3-nearest neighbors of the test
point in Figure 2.4, you will see that two of them are positive and one
is negative. Through voting, positive would win.                                                 Why is it a good idea to use an odd
    The full algorithm for K-nearest neighbor classification is given                        ? number for K?
in Algorithm 2.2. Note that there actually is no “training” phase for
K-nearest neighbors. In this algorithm we have introduced five new
conventions:

1. The training data is denoted by D.

2. We assume that there are N-many training examples.

3. These examples are pairs ( x1 , y1 ), ( x2 , y2 ), . . . , ( x N , y N ).
   (Warning: do not confuse xn , the nth training example, with xd ,
   the dth feature for example x.)

4. We use [ ]to denote an empty list and ⊕ · to append · to that list.

5. Our prediction on x̂ is called ŷ.
30   a course in machine learning



   The first step in this algorithm is to compute distances from the
test point to all training points (lines 2-4). The data points are then
sorted according to distance. We then apply a clever trick of summing
the class labels for each of the K nearest neighbors (lines 6-10) and
using the sign of this sum as our prediction.                                     Why is the sign of the sum com-
   The big question, of course, is how to choose K. As we’ve seen,                puted in lines 2-4 the same as the
                                                                              ?   majority vote of the associated
with K = 1, we run the risk of overfitting. On the other hand, if                 training examples?
K is large (for instance, K = N), then KNN-Predict will always
predict the majority class. Clearly that is underfitting. So, K is a
hyperparameter of the KNN algorithm that allows us to trade-off
between overfitting (small value of K) and underfitting (large value of
K).
   One aspect of inductive bias that we’ve seen for KNN is that it
assumes that nearby points should have the same label. Another                    Why can’t you simply pick the
aspect, which is quite different from decision trees, is that all features        value of K that does best on the
                                                                                  training data? In other words, why
are equally important! Recall that for decision trees, the key question       ?   do we have to treat it like a hy-
was which features are most useful for classification? The whole learning         perparameter rather than just a
                                                                                  parameter.
algorithm for a decision tree hinged on finding a small set of good
features. This is all thrown away in KNN classifiers: every feature
is used, and they are all used the same amount. This means that if
you have data with only a few relevant features and lots of irrelevant
features, KNN is likely to do poorly.
   A related issue with KNN is feature scale. Suppose that we are
trying to classify whether some object is a ski or a snowboard (see
Figure 2.5). We are given two features about this data: the width
and height. As is standard in skiing, width is measured in millime-
ters and height is measured in centimeters. Since there are only two
features, we can actually plot the entire training set; see Figure 2.6
where ski is the positive class. Based on this data, you might guess
that a KNN classifier would do well.
   Suppose, however, that our measurement of the width was com-
puted in millimeters (instead of centimeters). This yields the data
shown in Figure 2.7. Since the width values are now tiny, in compar-
ison to the height values, a KNN classifier will effectively ignore the      Figure 2.5: A figure of a ski and snow-
                                                                             board with width (mm) and height
width values and classify almost purely based on height. The pre-            (cm).
dicted class for the displayed test point had changed because of this
feature scaling.
   We will discuss feature scaling more in Chapter 4. For now, it is
just important to keep in mind that KNN does not have the power to
decide which features are important.




                                                                             Figure 2.6: Classification data for ski vs
                                                                             snowboard in 2d
                                                                         geometry and nearest neighbors                    31



2.3   Decision Boundaries

      The standard way that we’ve been thinking about learning algo-
      rithms up to now is in the query model. Based on training data, you
      learn something. I then give you a query example and you have to
      guess it’s label.
          An alternative, less passive, way to think about a learned model
      is to ask: what sort of test examples will it classify as positive, and
      what sort will it classify as negative. In Figure 2.9, we have a set of
      training data. The background of the image is colored blue in regions
      that would be classified as positive (if a query were issued there)         Figure 2.8: decision boundary for 1nn.

      and colored red in regions that would be classified as negative. This
      coloring is based on a 1-nearest neighbor classifier.
          In Figure 2.9, there is a solid line separating the positive regions
      from the negative regions. This line is called the decision boundary
      for this classifier. It is the line with positive land on one side and
      negative land on the other side.
          Decision boundaries are useful ways to visualize the complex-
      ity of a learned model. Intuitively, a learned model with a decision
      boundary that is really jagged (like the coastline of Norway) is really
      complex and prone to overfitting. A learned model with a decision
      boundary that is really simple (like the bounary between Arizona            Figure 2.9: decision boundary for knn
                                                                                  with k=3.
      and Utah) is potentially underfit. In Figure ??, you can see the deci-
      sion boundaries for KNN models with K ∈ {1, 3, 5, 7}. As you can
      see, the boundaries become simpler and simpler as K gets bigger.
          Now that you know about decision boundaries, it is natural to ask:
      what do decision boundaries for decision trees look like? In order
      to answer this question, we have to be a bit more formal about how
      to build a decision tree on real-valued features. (Remember that the
      algorithm you learned in the previous chapter implicitly assumed
      binary feature values.) The idea is to allow the decision tree to ask
      questions of the form: “is the value of feature 5 greater than 0.2?”
      That is, for real-valued features, the decision tree nodes are param-
                                                                                  Figure 2.10: decision tree for ski vs.
      eterized by a feature and a threshold for that feature. An example          snowboard
      decision tree for classifying skis versus snowboards is shown in Fig-
      ure 2.10.
          Now that a decision tree can handle feature vectors, we can talk
      about decision boundaries. By example, the decision boundary for
      the decision tree in Figure 2.10 is shown in Figure 2.11. In the figure,
      space is first split in half according to the first query along one axis.
      Then, depending on which half of the space you look at, it is either
      split again along the other axis, or simply classified.
          Figure 2.11 is a good visualization of decision boundaries for
      decision trees in general. Their decision boundaries are axis-aligned
                                                                                  Figure 2.11: decision boundary for dt in
                                                                                  previous figure
      32   a course in machine learning



      cuts. The cuts must be axis-aligned because nodes can only query on
      a single feature at a time. In this case, since the decision tree was so
      shallow, the decision boundary was relatively simple.                            What sort of data might yield a
                                                                                       very simple decision boundary with
                                                                                       a decision tree and very complex
2.4   K-Means Clustering                                                           ?   decision boundary with 1-nearest
                                                                                       neighbor? What about the other
                                                                                       way around?
      Up through this point, you have learned all about supervised learn-
      ing (in particular, binary classification). As another example of the
      use of geometric intuitions and data, we are going to temporarily
      consider an unsupervised learning problem. In unsupervised learn-
      ing, our data consists only of examples xn and does not contain corre-
      sponding labels. Your job is to make sense of this data, even though
      no one has provided you with correct labels. The particular notion of
      “making sense of” that we will talk about now is the clustering task.
          Consider the data shown in Figure 2.12. Since this is unsupervised
      learning and we do not have access to labels, the data points are
      simply drawn as black dots. Your job is to split this data set into
      three clusters. That is, you should label each data point as A, B or C
      in whatever way you want.
          For this data set, it’s pretty clear what you should do. You prob-
      ably labeled the upper-left set of points A, the upper-right set of
      points B and the bottom set of points C. Or perhaps you permuted
      these labels. But chances are your clusters were the same as mine.          Figure 2.12: simple clustering data...
                                                                                  clusters in UL, UR and BC.
          The K-means clustering algorithm is a particularly simple and
      effective approach to producing clusters on data like you see in Fig-
      ure 2.12. The idea is to represent each cluster by it’s cluster center.
      Given cluster centers, we can simply assign each point to its nearest
      center. Similarly, if we know the assignment of points to clusters, we
      can compute the centers. This introduces a chicken-and-egg problem.
      If we knew the clusters, we could compute the centers. If we knew
      the centers, we could compute the clusters. But we don’t know either.
          The general computer science answer to chicken-and-egg problems
      is iteration. We will start with a guess of the cluster centers. Based
      on that guess, we will assign each data point to its closest center.
      Given these new assignments, we can recompute the cluster centers.
      We repeat this process until clusters stop moving. The first few it-
      erations of the K-means algorithm are shown in Figure 2.13. In this
      example, the clusters converge very quickly.
          Algorithm 2.4 spells out the K-means clustering algorithm in de-
      tail. The cluster centers are initialized randomly. In line 6, data point
      xn is compared against each cluster center µk . It is assigned to cluster
      k if k is the center with the smallest distance. (That is the “argmin”
      step.) The variable zn stores the assignment (a value from 1 to K) of
      example n. In lines 8-12, the cluster centers are re-computed. First, Xk    Figure 2.13: first few iterations of
                                                                                  k-means running on previous data set
                                                                          geometry and nearest neighbors   33



Algorithm 4 K-Means(D, K)
  1: for k = 1 to K do

  2:    µk ← some random location      // randomly initialize mean for kth cluster
  3: end for

  4: repeat

  5:   for n = 1 to N do
  6:       zn ← argmink ||µk − xn ||      // assign example n to closest center
  7:   end for
  8:   for k = 1 to K do
  9:       Xk ← { x n : z n = k }                 // points assigned to cluster k
 10:       µk ← mean(Xk )                       // re-estimate mean of cluster k
 11:   end for
 12: until µs stop changing

 13: return z                                      // return cluster assignments



   M ATH R EVIEW | V ECTOR A RITHMETIC , N ORMS AND M EANS
  define vector addition, scalar addition, subtraction, scalar multiplication and norms. define mean.



                                                                                     Figure 2.14:

stores all examples that have been assigned to cluster k. The center of
cluster k, µk is then computed as the mean of the points assigned to
it. This process repeats until the means converge.
    An obvious question about this algorithm is: does it converge?
A second question is: how long does it take to converge. The first
question is actually easy to answer. Yes, it does. And in practice, it
usually converges quite quickly (usually fewer than 20 iterations). In
Chapter 13, we will actually prove that it converges. The question of
how long it takes to converge is actually a really interesting question.
Even though the K-means algorithm dates back to the mid 1950s, the
best known convergence rates were terrible for a long time. Here, ter-
rible means exponential in the number of data points. This was a sad
situation because empirically we knew that it converged very quickly.
New algorithm analysis techniques called “smoothed analysis” were
invented in 2001 and have been used to show very fast convergence
for K-means (among other algorithms). These techniques are well
beyond the scope of this book (and this author!) but suffice it to say
that K-means is fast in practice and is provably fast in theory.
    It is important to note that although K-means is guaranteed to
converge and guaranteed to converge quickly, it is not guaranteed to
converge to the “right answer.” The key problem with unsupervised
learning is that we have no way of knowing what the “right answer”
is. Convergence to a bad solution is usually due to poor initialization.
For example, poor initialization in the data set from before yields
convergence like that seen in Figure ??. As you can see, the algorithm
      34   a course in machine learning



      has converged. It has just converged to something less than satisfac-
      tory.                                                                                What is the difference between un-
                                                                                           supervised and supervised learning
                                                                                           that means that we know what the
2.5   Warning: High Dimensions are Scary                                               ?   “right answer” is for supervised
                                                                                           learning but not for unsupervised
                                                                                           learning?
      Visualizing one hundred dimensional space is incredibly difficult for
      humans. After huge amounts of training, some people have reported
      that they can visualize four dimensional space in their heads. But
      beyond that seems impossible.1
         In addition to being hard to visualize, there are at least two addi-
      tional problems in high dimensions, both refered to as the curse of
                                                                                   1
                                                                                     If you want to try to get an intu-
      dimensionality. One is computational, the other is mathematical.             itive sense of what four dimensions
         From a computational perspective, consider the following prob-            looks like, I highly recommend the
      lem. For K-nearest neighbors, the speed of prediction is slow for a          short 1884 book Flatland: A Romance
                                                                                   of Many Dimensions by Edwin Abbott
      very large data set. At the very least you have to look at every train-      Abbott. You can even read it online at
      ing example every time you want to make a prediction. To speed               gutenberg.org/ebooks/201.

      things up you might want to create an indexing data structure. You
      can break the plane up into a grid like that shown in Figure 2.15.
      Now, when the test point comes in, you can quickly identify the grid
      cell in which it lies. Now, instead of considering all training points,
      you can limit yourself to training points in that grid cell (and perhaps
      the neighboring cells). This can potentially lead to huge computa-
      tional savings.
         In two dimensions, this procedure is effective. If we want to break
      space up into a grid whose cells are 0.2×0.2, we can clearly do this
      with 25 grid cells in two dimensions (assuming the range of the
      features is 0 to 1 for simplicity). In three dimensions, we’ll need          Figure 2.15: 2d knn with an overlaid
                                                                                   grid, cell with test point highlighted
      125 = 5×5×5 grid cells. In four dimensions, we’ll need 625. By the
      time we get to “low dimensional” data in 20 dimensions, we’ll need
      95, 367, 431, 640, 625 grid cells (that’s 95 trillion, which is about 6 to
      7 times the US national debt as of January 2011). So if you’re in 20
      dimensions, this gridding technique will only be useful if you have at
      least 95 trillion training examples.
         For “medium dimensional” data (approximately 1000) dimesions,
      the number of grid cells is a 9 followed by 698 numbers before the
      decimal point. For comparison, the number of atoms in the universe
      is approximately 1 followed by 80 zeros. So even if each atom yielded
      a googul training examples, we’d still have far fewer examples than
      grid cells. For “high dimensional” data (approximately 100000) di-
      mensions, we have a 1 followed by just under 70, 000 zeros. Far too
      big a number to even really comprehend.
         Suffice it to say that for even moderately high dimensions, the
      amount of computation involved in these problems is enormous.                        How does the above analysis relate
         In addition to the computational difficulties of working in high                  to the number of data points you
                                                                                           would need to fill out a full decision
                                                                                       ?   tree with D-many features? What
                                                                                           does this say about the importance
                                                                                           of shallow trees?
                                                                 geometry and nearest neighbors                35



dimensions, there are a large number of strange mathematical oc-
curances there. In particular, many of your intuitions that you’ve
built up from working in two and three dimensions just do not carry
over to high dimensions. We will consider two effects, but there are
countless others. The first is that high dimensional spheres look more
like porcupines than like balls.2 The second is that distances between    2
                                                                           This result was related to me by Mark
points in high dimensions are all approximately the same.                 Reid, who heard about it from Marcus
                                                                          Hutter.
    Let’s start in two dimensions as in Figure 2.16. We’ll start with
four green spheres, each of radius one and each touching exactly two
other green spheres. (Remember that in two dimensions a “sphere”
is just a “circle.”) We’ll place a red sphere in the middle so that it
touches all four green spheres. We can easily compute the radius of
this small sphere. The pythagorean theorem says that 12 + 12 = (1 +
                                    √
r )2 , so solving for r we get r = 2 − 1 ≈ 0.41. Thus, by calculation,
the blue sphere lies entirely within the cube (cube = square) that
contains the grey spheres. (Yes, this is also obvious from the picture,
but perhaps you can see where this is going.)
    Now we can do the same experiment in three dimensions, as             Figure 2.16: 2d spheres in spheres
shown in Figure 2.17. Again, we can use the pythagorean theorem
to compute the radius of the blue sphere. Now, we get 12 + 12 + 12 =
                    √
(1 + r )2 , so r = 3 − 1 ≈ 0.73. This is still entirely enclosed in the
cube of width four that holds all eight grey spheres.
    At this point it becomes difficult to produce figures, so you’ll
have to apply your imagination. In four dimensions, we would have
16 green spheres (called hyperspheres), each of radius one. They
would still be inside a cube (called a hypercube) of width four. The
                                              √
blue hypersphere would have radius r = 4 − 1 = 1. Continuing
to five dimensions, the blue hypersphere embedded in 256 green
                                         √
hyperspheres would have radius r = 5 − 1 ≈ 1.23 and so on.                Figure 2.17: 3d spheres in spheres
    In general, in D-dimensional space, there will be 2D green hyper-
spheres of radius one. Each green hypersphere will touch exactly
n-many other hyperspheres. The blue hyperspheres in the middle
                                                 √
will touch them all and will have radius r = D − 1.
    Think about this for a moment. As the number of dimensions
grows, the radius of the blue hypersphere grows without bound!. For
example, in 9-dimensions the radius of the blue hypersphere is now
√
   9 − 1 = 2. But with a radius of two, the blue hypersphere is now
“squeezing” between the green hypersphere and touching the edges
of the hypercube. In 10 dimensional space, the radius is approxi-
mately 2.16 and it pokes outside the cube.
    This is why we say that high dimensional spheres look like por-
cupines and not balls (see Figure 2.18). The moral of this story from
a machine learning perspective is that intuitions you have about space
might not carry over to high dimensions. For example, what you
                                                                          Figure 2.18: porcupine versus ball
36   a course in machine learning



think looks like a “round” cluster in two or three dimensions, might                                                     0.06                                                 1.0
                                                                                                                         0.04
not look so “round” in high dimensions.                                                                                  0.02
                                                                                                                                                                              0.8
                                                                                                                                                                              0.6
                                                                                                                         0.00
   The second strange fact we will consider has to do with the dis-                                                      0.02
                                                                                                                                                                              0.4

                                                                                                                         0.04                                                 0.2
tances between points in high dimensions. We start by considering                                                        0.06
                                                                                                                            0.0      0.2      0.4      0.6     0.8      1.0
                                                                                                                                                                              0.0
                                                                                                                                                                                0.0   0.2      0.4    0.6      0.8     1.0

random points in one dimension. That is, we generate a fake data set                                                                                                   1.0
                                                                                                                                                                      0.8
consisting of 100 random points between zero and one. We can do                                                                                                       0.6
                                                                                                                                                                      0.4
                                                                                                                                                                      0.2
                                                                                                                                                                      0.0
                                                                                                                                                                  0.81.0
the same in two dimensions and in three dimensions. See Figure 2.19                                                               0.0 0.2
                                                                                                                                          0.4 0.6            0.40.6
                                                                                                                                                  0.8 1.00.0.2

for data distributed uniformly on the unit hypercube in different
dimensions.
                                                                            Figure 2.19: 100 uniform random points
   Now, pick two of these points at random and compute the dis-             in 1, 2 and 3 dimensions
tance between them. Repeat this process for all pairs of points and
average the results. For the data shown in Figure 2.19, the average
distance between points in one dimension is about 0.346; in two di-
mensions is about 0.518; and in three dimensions is 0.615. The fact
that these increase as the dimension increases is not surprising. The
furthest two points can be in a 1-dimensional hypercube (line) is 1;
                                                        √
the furthest in a 2-dimensional hypercube (square) is 2 (opposite
                                            √
corners); the furthest in a 3-d hypercube is 3 and so on. In general,
                                                                √
the furthest two points in a D-dimensional hypercube will be D.
   You can actually compute these values analytically. Write UniD
for the uniform distribution in D dimensions. The quantity we are
interested in computing is:
                           h       h         ii
   avgDist( D ) = Ea∼UniD Eb∼UniD || a − b||                        (2.2)

We can actually compute this in closed form (see Exercise ?? for a bit
                                                    √
of calculus refresher) and arrive at avgDist( D ) = D/3. Because
we know that the maximum distance between two points grows like
√
   D, this says that the ratio between average distance and maximum
distance converges to 1/3.
   What is more interesting, however, is the variance of the distribu-
tion of distances. You can show that in D dimensions, the variance
               √
is constant 1/ 18, independent of D. This means that when you look
at (variance) divided-by (max distance), the variance behaves like
   √
1/ 18D, which means that the effective variance continues to shrink
as D grows 3 .                                                              3
                                                                              Sergey Brin. Near neighbor search in
   When I first saw and re-proved this result, I was skeptical, as I        large metric spaces. In Conference on
                                                                            Very Large Databases (VLDB), 1995
imagine you are. So I implemented it. In Figure 2.20 you can see
the results. This presents a histogram of distances between random                                                      14000                  dimensionality versus uniform point distances
                                                                                                                                                                                                            2 dims
points in D dimensions for D ∈ {1, 2, 3, 10, 20, 100}. As you can see,                                                                                                                                      8 dims
                                                       √                                                                12000
                                                                                                                                                                                                            32 dims
                                                                                                                                                                                                            128 dims
all of these distances begin to concentrate around 0.4 D, even for

                                                                                # of pairs of points at that distance
                                                                                                                        10000                                                                               512 dims

“medium dimension” problems.                                                                                            8000


   You should now be terrified: the only bit of information that KNN                                                    6000

                                                                                                                        4000
gets is distances. And you’ve just seen that in moderately high di-
                                                                                                                        2000
mensions, all distances becomes equal. So then isn’t it the case that
                                                                                                                           0
                                                                                                                           0.0                 0.2                0.4              0.6               0.8               1.0
                                                                                                                                                             distance / sqrt(dimensionality)




                                                                            Figure 2.20: histogram of distances in
                                                                            D=2,8,32,128,512
                                                                         geometry and nearest neighbors                  37



      KNN simply cannot work?
         The answer has to be no. The reason is that the data that we get
      is not uniformly distributed over the unit hypercube. We can see this
      by looking at two real-world data sets. The first is an image data set
      of hand-written digits (zero through nine); see Section ??. Although
      this data is originally in 256 dimensions (16 pixels by 16 pixels), we
      can artifically reduce the dimensionality of this data. In Figure 2.21
      you can see the histogram of average distances between points in this
      data at a number of dimensions.
         As you can see from these histograms, distances have not con-
      centrated around a single value. This is very good news: it means
                                                                                  Figure 2.21: knn:mnist: histogram of
      that there is hope for learning algorithms to work! Nevertheless, the       distances in multiple D for mnist
      moral is that high dimensions are weird.


2.6   Extensions to KNN

      There are several fundamental problems with KNN classifiers. First,
      some neighbors might be “better” than others. Second, test-time per-
      formance scales badly as your number of training examples increases.
      Third, it treats each dimension independently. We will not address
      the third issue, as it has not really been solved (though it makes a
      great thought question!).
         Regarding neighborliness, consider Figure 2.22. Using K = 5 near-
      est neighbors, the test point would be classified as positive. However,
      we might actually believe that it should be classified negative because
      the two negative neighbors are much closer than the three positive          Figure 2.22: data set with 5nn, test point
      neighbors.                                                                  closest to two negatives, then to three
                                                                                  far positives
         There are at least two ways of addressing this issue. The first is the
      e-ball solution. Instead of connecting each data point to some fixed
      number (K) of nearest neighbors, we simply connect it to all neigh-
      bors that fall within some ball of radius e. Then, the majority class of
      all the points in the e ball wins. In the case of a tie, you would have
      to either guess, or report the majority class. Figure 2.23 shows an e
      ball around the test point that happens to yield the proper classifica-
      tion.
         When using e-ball nearest neighbors rather than KNN, the hyper-
      parameter changes from K to e. You would need to set it in the same
      way as you would for KNN.
                                                                                  Figure 2.23: same as previous with e
         An alternative to the e-ball solution is to do weighted nearest          ball
      neighbors. The idea here is to still consider the K-nearest neighbors            One issue with e-balls is that the
      of a test point, but give them uneven votes. Closer points get more              e-ball for some test point might
                                                                                   ?   be empty. How would you handle
      vote than further points. When classifying a point x̂, the usual strat-          this?
      egy is to give a training point xn a vote that decays exponentially in
      the distance between x̂ and xn . Mathematically, the vote that neigh-
      38   a course in machine learning



      bor n gets is:
                            
                1
        exp − || x̂ − xn ||2                                            (2.3)
                2

      Thus, nearby points get a vote very close to 1 and far away points get
      a vote very close to 0. The overall prediction is positive if the sum
      of votes from positive neighbors outweighs the sum of votes from
      negative neighbors.                                                               Could you combine the e-ball idea
         The second issue with KNN is scaling. To predict the label of a                with the weighted voting idea?
                                                                                    ?   Does it make sense, or does one
      single test point, we need to find the K nearest neighbors of that                idea seem to trump the other?
      test point in the training data. With a standard implementation, this
      will take O( ND + K log K ) time4 . For very large data sets, this is
      impractical.
         A first attempt to speed up the computation is to represent each
      class by a representative. A natural choice for a representative would
      be the mean. We would collapse all positive examples down to their
      mean, and all negative examples down to their mean. We could then         4
                                                                                 The ND term comes from computing
                                                                                distances between the test point and
      just run 1-nearest neighbor and check whether a test point is closer
                                                                                all training points. The K log K term
      to the mean of the positive points or the mean of the negative points.    comes from finding the K smallest
      Figure 2.24 shows an example in which this would probably work            values in the list of distances, using a
                                                                                median-finding algorithm. Of course,
      well, and an example in which this would probably work poorly. The        ND almost always dominates K log K in
      problem is that collapsing each class to its mean is too aggressive.      practice.
         A less aggressive approach is to make use of the K-means algo-
      rithm for clustering. You can cluster the positive examples into L
      clusters (we are using L to avoid variable overloading!) and then
      cluster the negative examples into L separate clusters. This is shown
      in Figure 2.25 with L = 2. Instead of storing the entire data set,
      you would only store the means of the L positive clusters and the
      means of the L negative clusters. At test time, you would run the
      K-nearest neighbors algorithm against these means rather than
      against the full training set. This leads to a much faster runtime of
      just O( LD + K log K ), which is probably dominated by LD.


2.7   Exercises                                                                 Figure 2.24: knn:collapse: two figures
                                                                                of points collapsed to mean, one with
                                                                                good results and one with dire results
      Exercise 2.1. TODO. . .




                                                                                Figure 2.25: knn:collapse2: data from
                                                                                previous bad case collapsed into L=2
                                                                                cluster and test point classified based
                                                                                on means and 1-nn
                                                                                        Clustering of classes was intro-
                                                                                        duced as a way of making things
                                                                 3 | T HE P ERCEPTRON
                                                                         –      Learning Objectives:
                                                                                • Describe the biological motivation
                                                                                  behind the perceptron.
                                                                                • Classify learning algorithms based
                                                                                  on whether they are error-driven or
                                                                                  not.
                                                                                • Implement the perceptron algorithm
                                                                                  for binary classification.
                                                                                • Draw perceptron weight vectors
      So far, you’ve seen two types of learning models: in decision               and the corresponding decision
                                                                                  boundaries in two dimensions.
      trees, only a small number of features are used to make decisions; in
                                                                                • Contrast the decision boundaries
      nearest neighbor algorithms, all features are used equally. Neither of
                                                                                  of decision trees, nearest neighbor
      these extremes is always desirable. In some problems, we might want         algorithms and perceptrons.
      to use most of the features, but use some more than others.               • Compute the margin of a given
         In this chapter, we’ll discuss the perceptron algorithm for learn-       weight vector on a given data set.

      ing weights for features. As we’ll see, learning weights for features
      amounts to learning a hyperplane classifier: that is, basically a di-
      vision of space into two halves by a straight line, where one half is
      “positive” and one half is “negative.” In this sense, the perceptron
      can be seen as explicitly finding a good linear decision boundary.

                                                                                Dependencies: Chapter 1, Chapter 2
3.1   Bio-inspired Learning

      Folk biology tells us that our brains are made up of a bunch of little
      units, called neurons, that send electrical signals to one another. The
      rate of firing tells us how “activated” a neuron is. A single neuron,
      like that shown in Figure 3.1 might have three incoming neurons.
      These incoming neurons are firing at different rates (i.e., have dif-
      ferent activations). Based on how much these incoming neurons are
      firing, and how “strong” the neural connections are, our main neu-
      ron will “decide” how strongly it wants to fire. And so on through
      the whole brain. Learning in the brain happens by neurons becom-
      ming connected to other neurons, and the strengths of connections
      adapting over time.                                                       Figure 3.1: a picture of a neuron
          The real biological world is much more complicated than this.
      However, our goal isn’t to build a brain, but to simply be inspired
      by how they work. We are going to think of our learning algorithm
      as a single neuron. It receives input from D-many other neurons,
      one for each input feature. The strength of these inputs are the fea-
      ture values. This is shown schematically in Figure ??. Each incom-
      ing connection has a weight and the neuron simply sums up all the
      weighted inputs. Based on this sum, it decides whether to “fire” or




                                                                                Figure 3.2: figure showing feature
                                                                                vector and weight vector and products
                                                                                and sum
      40    a course in machine learning



      not. Firing is interpreted as being a positive example and not firing is
      interpreted as being a negative example. In particular, if the weighted
      sum is positive, it “fires” and otherwise it doesn’t fire. This is shown
      diagramatically in Figure 3.2.
         Mathematically, an input vector x = h x1 , x2 , . . . , x D i arrives. The
      neuron stores D-many weights, w1 , w2 , . . . , w D . The neuron computes
      the sum:
                  D
           a = ∑ wd xd                                                        (3.1)
                d =1

      to determine it’s amount of “activation.” If this activiation is posi-
      tive (i.e., a > 0) it predicts that this example is a positive example.
      Otherwise it predicts a negative example.
         The weights of this neuron are fairly easy to interpret. Suppose
      that a feature, for instance “is this a System’s class?” gets a zero
      weight. Then the activation is the same regardless of the value of
      this feature. So features with zero weight are ignored. Features with
      positive weights are indicative of positive examples because they
      cause the activation to increase. Features with negative weights are
      indicative of negative examples because they cause the activiation to
      decrease.                                                                           What would happen if we encoded
         It is often convenient to have a non-zero threshold. In other                    binary features like “is this a Sys-
                                                                                      ?   tem’s class” as no=0 and yes=−1
      words, we might want to predict positive if a > θ for some value                    (rather than the standard no=0 and
      θ. The way that is most convenient to achieve this is to introduce a                yes=+1)?
      bias term into the neuron, so that the activation is always increased
      by some fixed value b. Thus, we compute:
               "         #
                      D
           a=     ∑ wd xd + b                                                 (3.2)
                  d =1

                                                                                          If you wanted the activation thresh-
         This is the complete neural model of learning. The model is pa-
                                                                                      ? old to be a > θ instead of a > 0,
      rameterized by D-many weights, w1 , w2 , . . . , w D , and a single scalar          what value would b have to be?
      bias value b.


3.2   Error-Driven Updating: The Perceptron Algorithm


           V IGNETTE : T HE H ISTORY OF THE P ERCEPTRON
           todo



         The perceptron is a classic learning algorithm for the neural model
      of learning. Like K-nearest neighbors, it is one of those frustrating
      algorithms that is incredibly simple and yet works amazingly well,
      for some types of problems.
                                                                                                          the perceptron            41



Algorithm 5 PerceptronTrain(D, MaxIter)
  1: w d ← 0, for all d = 1 . . . D                                   // initialize weights
  2: b ← 0                                                                 // initialize bias
  3: for iter = 1 . . . MaxIter do

  4:    for all (x,y) ∈ D do
  5:       a ← ∑D  d=1 w d x d + b                // compute activation for this example
  6:       if ya ≤ 0 then
  7:          wd ← wd + yxd , for all d = 1 . . . D                     // update weights
  8:          b←b+y                                                           // update bias
  9:       end if
 10:    end for
 11: end for

 12: return w0 , w1 , . . . , w D , b




Algorithm 6 PerceptronTest(w0 , w1 , . . . , w D , b, x̂)
  1: a ← ∑Dd=1 wd x̂d + b                       // compute activation for the test example
  2: return sign(a)




   The algorithm is actually quite different than either the decision
tree algorithm or the KNN algorithm. First, it is online. This means
that instead of considering the entire data set at the same time, it only
ever looks at one example. It processes that example and then goes
on to the next one. Second, it is error driven. This means that, so
long as it is doing well, it doesn’t bother updating its parameters.
   The algorithm maintains a “guess” at good parameters (weights
and bias) as it runs. It processes one example at a time. For a given
example, it makes a prediction. It checks to see if this prediction
is correct (recall that this is training data, so we have access to true
labels). If the prediction is correct, it does nothing. Only when the
prediction is incorrect does it change its parameters, and it changes
them in such a way that it would do better on this example next
time around. It then goes on to the next example. Once it hits the
last example in the training set, it loops back around for a specified
number of iterations.
   The training algorithm for the perceptron is shown in Algo-
rithm 3.2 and the corresponding prediction algorithm is shown in
Algorithm 3.2. There is one “trick” in the training algorithm, which
probably seems silly, but will be useful later. It is in line 6, when we
check to see if we want to make an update or not. We want to make
an update if the current prediction (just sign(a)) is incorrect. The
trick is to multiply the true label y by the activation a and compare
this against zero. Since the label y is either +1 or −1, you just need
to realize that ya is positive whenever a and y have the same sign.
In other words, the product ya is positive if the current prediction is
correct.                                                                                           It is very very important to check
                                                                                                ? ya ≤ 0 rather than ya < 0. Why?
42    a course in machine learning



    The particular form of update for the perceptron is quite simple.
The weight wd is increased by yxd and the bias is increased by y. The
goal of the update is to adjust the parameters so that they are “bet-
ter” for the current example. In other words, if we saw this example
twice in a row, we should do a better job the second time around.
    To see why this particular update achieves this, consider the fol-
lowing scenario. We have some current set of parameters w1 , . . . , w D , b.
We observe an example ( x, y). For simplicity, suppose this is a posi-
tive example, so y = +1. We compute an activation a, and make an
error. Namely, a < 0. We now update our weights and bias. Let’s call
the new weights w10 , . . . , w0D , b0 . Suppose we observe the same exam-
ple again and need to compute a new activation a0 . We proceed by a
little algebra:
           D
     a0 = ∑ wd0 xd + b0                                                (3.3)
         d =1
           D
       = ∑ ( w d + x d ) x d + ( b + 1)                                (3.4)
         d =1
           D                D
       = ∑ wd xd + b + ∑ xd xd + 1                                     (3.5)
         d =1              d =1
                 D
       = a + ∑ xd2 + 1       >    a                                    (3.6)
                d =1

So the difference between the old activation a and the new activa-
tion a0 is ∑d xd2 + 1. But xd2 ≥ 0, since it’s squared. So this value is
always at least one. Thus, the new activation is always at least the old
activation plus one. Since this was a positive example, we have suc-
cessfully moved the activation in the proper direction. (Though note
that there’s no guarantee that we will correctly classify this point the
second, third or even fourth time around!)                                           This analysis hold for the case pos-
    The only hyperparameter of the perceptron algorithm is MaxIter,                  itive examples (y = +1). It should
                                                                                 ?   also hold for negative examples.
the number of passes to make over the training data. If we make                      Work it out.
many many passes over the training data, then the algorithm is likely
to overfit. (This would be like studying too long for an exam and just
confusing yourself.) On the other hand, going over the data only
one time might lead to underfitting. This is shown experimentally in
Figure 3.3. The x-axis shows the number of passes over the data and
the y-axis shows the training error and the test error. As you can see,
there is a “sweet spot” at which test performance begins to degrade
due to overfitting.
    One aspect of the perceptron algorithm that is left underspecified
is line 4, which says: loop over all the training examples. The natural
implementation of this would be to loop over them in a constant
order. The is actually a bad idea.




                                                                                Figure 3.3: training and test error via
                                                                                early stopping
                                                                                                the perceptron               43



          Consider what the perceptron algorithm would do on a data set
      that consisted of 500 positive examples followed by 500 negative
      examples. After seeing the first few positive examples (maybe five),
      it would likely decide that every example is positive, and would stop
      learning anything. It would do well for a while (next 495 examples),
      until it hit the batch of negative examples. Then it would take a while
      (maybe ten examples) before it would start predicting everything as
      negative. By the end of one pass through the data, it would really
      only have learned from a handful of examples (fifteen in this case).
          So one thing you need to avoid is presenting the examples in some
      fixed order. This can easily be accomplished by permuting the order
      of examples once in the beginning and then cycling over the data set
      in the same (permuted) order each iteration. However, it turns out
      that you can actually do better if you re-permute the examples in each       Figure 3.4: training and test error for
      iteration. Figure 3.4 shows the effect of re-permuting on convergence        permuting versus not-permuting
      speed. In practice, permuting each iteration tends to yield about 20%
      savings in number of iterations. In theory, you can actually prove that
      it’s expected to be about twice as fast.                                          If permuting the data each iteration
                                                                                        saves somewhere between 20% and
                                                                                    ?   50% of your time, are there any
3.3   Geometric Intrepretation                                                          cases in which you might not want
                                                                                        to permute the data every iteration?
      A question you should be asking yourself by now is: what does the
      decision boundary of a perceptron look like? You can actually answer
      that question mathematically. For a perceptron, the decision bound-
      ary is precisely where the sign of the activation, a, changes from −1
      to +1. In other words, it is the set of points x that achieve zero ac-
      tivation. The points that are not clearly positive nor negative. For
      simplicity, we’ll first consider the case where there is no “bias” term
      (or, equivalently, the bias is zero). Formally, the decision boundary B
      is:
              (                    )
        B=     x :   ∑ wd xd = 0                                           (3.7)
                     d

      We can now apply some linear algebra. Recall that ∑d wd xd is just
      the dot product between the vector w = hw1 , w2 , . . . , w D i and the
      vector x. We will write this as w · x. Two vectors have a zero dot
      product if and only if they are perpendicular. Thus, if we think of
      the weights as a vector w, then the decision boundary is simply the
      plane perpendicular to w.
44     a course in machine learning



     M ATH R EVIEW | D OT P RODUCTS
     dot products, definition, perpendicular, normalization and projections... think about basis vectors for
     projections. quadratic rule on vectors. also that dot products onto unit vectors are maximized when
     they point in the same direction so a*a >= a*b blah blah blah.



                                                                               Figure 3.5:




   This is shown pictorially in Figure 3.6. Here, the weight vector is
shown, together with it’s perpendicular plane. This plane forms the
decision boundary between positive points and negative points. The
vector points in the direction of the positive examples and away from
the negative examples.
   One thing to notice is that the scale of the weight vector is irrele-
vant from the perspective of classification. Suppose you take a weight
vector w and replace it with 2w. All activations are now doubled.
But their sign does not change. This makes complete sense geometri-
cally, since all that matters is which side of the plane a test point falls
on, now how far it is from that plane. For this reason, it is common
to work with normalized weight vectors, w, that have length one; i.e.,         Figure 3.6: picture of data points with
                                                                               hyperplane and weight vector
||w|| = 1.                                                                         If I give you an arbitrary non-zero
   The geometric intuition can help us even more when we realize                   weight vector w, how do I compute
                                                                                ?  a weight vector w0 that points in the
that dot products compute projections. That is, the value w · x is                 same direction but has a norm of
just the distance of x from the origin when projected onto the vector              one?
w. This is shown in Figure 3.7. In that figure, all the data points are
projected onto w. Below, we can think of this as a one-dimensional
version of the data, where each data point is placed according to its
projection along w. This distance along w is exactly the activiation of
that example, with no bias.
   From here, you can start thinking about the role of the bias term.
Previously, the threshold would be at zero. Any example with a
negative projection onto w would be classified negative; any exam-
ple with a positive projection, positive. The bias simply moves this
threshold. Now, after the projection is computed, b is added to get
the overall activation. The projection plus b is then compared against
zero.
   Thus, from a geometric perspective, the role of the bias is to shift
the decision boundary away from the origin, in the direction of w. It
is shifted exactly −b units. So if b is positive, the boundary is shifted
away from w and if b is negative, the boundary is shifted toward w.
This is shown in Figure ??. This makes intuitive sense: a positive bias
means that more examples should be classified positive. By moving              Figure 3.7: same picture as before, but
the decision boundary in the negative direction, more space yields a           with projections onto weight vector;
                                                                               TODO: then, below, those points along
                                                                               a one-dimensional axis with zero
                                                                               marked.
                                                                                               the perceptron            45



      positive classification.
         The decision boundary for a perceptron is a very magical thing. In
      D dimensional space, it is always a D − 1-dimensional hyperplane.
      (In two dimensions, a 1-d hyperplane is simply a line. In three di-
      mensions, a 2-d hyperplane is like a sheet of paper.) This hyperplane
      divides space in half. In the rest of this book, we’ll refer to the weight
      vector, and to hyperplane it defines, interchangeably.
         The perceptron update can also be considered geometrically. (For
      simplicity, we will consider the unbiased case.) Consider the situ-
      ation in Figure ??. Here, we have a current guess as to the hyper-
      plane, and positive training example comes in that is currently mis-
      classified. The weights are updated: w ← w + yx. This yields the             Figure 3.8: perceptron picture with
      new weight vector, also shown in the Figure. In this case, the weight        update, no bias
      vector changed enough that this training example is now correctly
      classified.


3.4   Interpreting Perceptron Weights

      TODO


3.5   Perceptron Convergence and Linear Separability

      You already have an intuitive feeling for why the perceptron works:
      it moves the decision boundary in the direction of the training exam-
      ples. A question you should be asking yourself is: does the percep-
      tron converge? If so, what does it converge to? And how long does it
      take?
          It is easy to construct data sets on which the perceptron algorithm
      will never converge. In fact, consider the (very uninteresting) learn-
      ing problem with no features. You have a data set consisting of one
      positive example and one negative example. Since there are no fea-
      tures, the only thing the perceptron algorithm will ever do is adjust
      the bias. Given this data, you can run the perceptron for a bajillion
      iterations and it will never settle down. As long as the bias is non-
      negative, the negative example will cause it to decrease. As long as
      it is non-positive, the positive example will cause it to increase. Ad
      infinitum. (Yes, this is a very contrived example.)
          What does it mean for the perceptron to converge? It means that
      it can make an entire pass through the training data without making
      any more updates. In other words, it has correctly classified every
      training example. Geometrically, this means that it was found some
      hyperplane that correctly segregates the data into positive and nega-
                                                                                   Figure 3.9: separable data
      tive examples, like that shown in Figure 3.9.
          In this case, this data is linearly separable. This means that there
46    a course in machine learning



exists some hyperplane that puts all the positive examples on one side
and all the negative examples on the other side. If the training is not
linearly separable, like that shown in Figure 3.10, then the perceptron
has no hope of converging. It could never possibly classify each point
correctly.
   The somewhat surprising thing about the perceptron algorithm is
that if the data is linearly separable, then it will converge to a weight
vector that separates the data. (And if the data is inseparable, then it
will never converge.) This is great news. It means that the perceptron
converges whenever it is even remotely possible to converge.
   The second question is: how long does it take to converge? By
“how long,” what we really mean is “how many updates?” As is the
case for much learning theory, you will not be able to get an answer
of the form “it will converge after 5293 updates.” This is asking too
much. The sort of answer we can hope to get is of the form “it will
converge after at most 5293 updates.”
   What you might expect to see is that the perceptron will con-
verge more quickly for easy learning problems than for hard learning
problems. This certainly fits intuition. The question is how to define
“easy” and “hard” in a meaningful way. One way to make this def-
inition is through the notion of margin. If I give you a data set and
hyperplane that separates it (like that shown in Figure ??) then the
margin is the distance between the hyperplane and the nearest point.
Intuitively, problems with large margins should be easy (there’s lots
of “wiggle room” to find a separating hyperplane); and problems
with small margins should be hard (you really have to get a very
specific well tuned weight vector).
   Formally, given a data set D, a weight vector w and bias b, the
margin of w, b on D is defined as:
                      (                         
                        min( x,y)∈D y w · x + b if w separates D
 margin(D, w, b) =                                                    (3.8)
                        −∞                         otherwise
In words, the margin is only defined if w, b actually separate the data
(otherwise it is just −∞). In the case that it separates the data, we
find the point with the minimum activation, after the activation is
multiplied by the label.                                                          So long as the margin is not −∞,
   For some historical reason (that is unknown to the author), mar-               it is always positive. Geometrically
                                                                              ?   this makes sense, but what does
gins are always denoted by the Greek letter γ (gamma). One often                  Eq (3.8) yeild this?
talks about the margin of a data set. The margin of a data set is the
largest attainable margin on this data. Formally:

     margin(D) = sup margin(D, w, b)                                  (3.9)
                  w,b

In words, to compute the margin of a data set, you “try” every possi-
ble w, b pair. For each pair, you compute its margin. We then take the
                                                                                            the perceptron                47



largest of these as the overall margin of the data.1 If the data is not        1
                                                                                 You can read “sup” as “max” if you
linearly separable, then the value of the sup, and therefore the value         like: the only difference is a technical
                                                                               difference in how the −∞ case is
of the margin, is −∞.                                                          handled.
   There is a famous theorem due to Rosenblatt2 that shows that the            2
                                                                                 Rosenblatt 1958
number of errors that the perceptron algorithm makes is bounded by
γ−2 . More formally:
Theorem 1 (Perceptron Convergence Theorem). Suppose the perceptron
algorithm is run on a linearly separable data set D with margin γ > 0.
Assume that || x|| ≤ 1 for all x ∈ D. Then the algorithm will converge after
at most γ12 updates.
   todo: comment on norm of w and norm of x also some picture
about maximum margins.
   The proof of this theorem is elementary, in the sense that it does
not use any fancy tricks: it’s all just algebra. The idea behind the
proof is as follows. If the data is linearly separable with margin γ,
then there exists some weight vector w∗ that achieves this margin.
Obviously we don’t know what w∗ is, but we know it exists. The
perceptron algorithm is trying to find a weight vector w that points
roughly in the same direction as w∗ . (For large γ, “roughly” can be
very rough. For small γ, “roughly” is quite precise.) Every time the
perceptron makes an update, the angle between w and w∗ changes.
What we prove is that the angle actually decreases. We show this in
two steps. First, the dot product w · w∗ increases a lot. Second, the
norm ||w|| does not increase very much. Since the dot product is
increasing, but w isn’t getting too long, the angle between them has
to be shrinking. The rest is algebra.

Proof of Theorem 1. The margin γ > 0 must be realized by some set
of parameters, say x∗ . Suppose we train a perceptron on this data.
Denote by w(0) the initial weight vector, w(1) the weight vector after
the first update, and w(k) the weight vector after the kth update. (We
are essentially ignoring data points on which the perceptron doesn’t
update itself.) First, we will show that w∗ · w(k) grows quicky as
a function of k. Second, we will show that w(k) does not grow
quickly.
   First, suppose that the kth update happens on example ( x, y). We
are trying to show that w(k) is becoming aligned with w∗ . Because we
updated, know that this example was misclassified: yw(k-1) · x < 0.
After the update, we get w(k) = w(k-1) + yx. We do a little computa-
tion:
  w∗ · w(k) = w∗ · w(k-1) + yx                  definition of w(k)    (3.10)
            = w∗ · w(k-1) + yw∗ · x               vector algebra      (3.11)
                 ∗    (k-1)                      ∗
            ≥ w ·w            +γ               w has margin γ         (3.12)
      48   a course in machine learning



      Thus, every time w(k) is updated, its projection onto w∗ incrases by at
      least γ. Therefore: w∗ · w(k) ≥ kγ.
         Next, we need to show that the increase of γ along w∗ occurs
      because w(k) is getting closer to w∗ , not just because it’s getting ex-
      ceptionally long. To do this, we compute the norm of w(k) :
                  2                      2
           w(k)       = w(k-1) + yx                                             definition of w(k)
                                                                                 (3.13)
                                 2
                      = w(k-1)       + y2 || x||2 + 2yw(k-1) · x        quadratic rule on vectors
                                                                                 (3.14)
                                 2
                      ≤ w(k-1)       +1+0                          assumption on || x|| and a < 0
                                                                                 (3.15)

      Thus, the squared norm of w(k) increases by at most one every up-
                               2
      date. Therefore: w(k) ≤ k.
         Now we put together the two things we have learned before. By
      our first conclusion, we know w∗ · w(k) ≥ kγ. But our second con-
                √            2
      clusion, k ≥ w(k) . Finally, because w∗ is a unit vector, we know
      that w(k) ≥ w∗ · w(k) . Putting this together, we have:
         √
           k ≥        w(k)     ≥ w∗ · w(k) ≥ kγ                      (3.16)
                                                            √
      Taking the left-most and right-most terms, we get that k ≥ kγ.
      Dividing both sides by k, we get √1 ≥ γ and therefore k ≤ √1γ .
                                                  k
      This means that once we’ve made γ12 updates, we cannot make any
      more!
                                                                                                     Perhaps we don’t want to assume
          It is important to keep in mind what this proof shows and what                             that all x have norm at most 1. If
                                                                                                     they have all have norm at most
      it does not show. It shows that if I give the perceptron data that
                                                                                               ?     R, you can achieve a very simi-
      is linearly separable with margin γ > 0, then the perceptron will                              lar bound. Modify the perceptron
      converge to a solution that separates the data. And it will converge                           convergence proof to handle this
                                                                                                     case.
      quickly when γ is large. It does not say anything about the solution,
      other than the fact that it separates the data. In particular, the proof
      makes use of the maximum margin separator. But the perceptron
      is not guaranteed to find this maximum margin separator. The data
      may be separable with margin 0.9 and the perceptron might still
      find a separating hyperplane with a margin of only 0.000001. Later
      (in Chapter ??), we will see algorithms that explicitly try to find the
      maximum margin solution.                                                                       Why does the perceptron conver-
                                                                                                     gence bound not contradict the
                                                                                                     earlier claim that poorly ordered
3.6   Improved Generalization: Voting and Averaging                                            ?     data points (e.g., all positives fol-
                                                                                                     lowed by all negatives) will cause
                                                                                                     the perceptron to take an astronom-
      In the beginning of this chapter, there was a comment that the per-
                                                                                                     ically long time to learn?
      ceptron works amazingly well. This was a half-truth. The “vanilla”
                                                                                                the perceptron            49



perceptron algorithm does well, but not amazingly well. In order to
make it more competitive with other learning algorithms, you need
to modify it a bit to get better generalization. The key issue with the
vanilla perceptron is that it counts later points more than it counts earlier
points.
    To see why, consider a data set with 10, 000 examples. Suppose
that after the first 100 examples, the perceptron has learned a really
good classifier. It’s so good that it goes over the next 9899 exam-
ples without making any updates. It reaches the 10, 000th example
and makes an error. It updates. For all we know, the update on this
10, 000th example completely ruines the weight vector that has done so
well on 99.99% of the data!
    What we would like is for weight vectors that “survive” a long
time to get more say than weight vectors that are overthrown quickly.
One way to achieve this is by voting. As the perceptron learns, it
remembers how long each hyperplane survives. At test time, each
hyperplane encountered during training “votes” on the class of a test
example. If a particular hyperplane survived for 20 examples, then
it gets a vote of 20. If it only survived for one example, it only gets a
vote of 1. In particular, let (w, b)(1) , . . . , (w, b)(K) be the K + 1 weight
vectors encountered during training, and c(1) , . . . , c(K) be the survival
times for each of these weight vectors. (A weight vector that gets
immediately updated gets c = 1; one that survives another round
gets c = 2 and so on.) Then the prediction on a test point is:
                                                  !
                K                             
    ŷ = sign ∑ c(k) sign w(k) · x̂ + b(k)                                  (3.17)
               k =1

This algorithm, known as the voted perceptron works quite well in
practice, and there is some nice theory showing that it is guaranteed
to generalize better than the vanilla perceptron. Unfortunately, it is
also completely impractical. If there are 1000 updates made during
perceptron learning, the voted perceptron requires that you store
1000 weight vectors, together with their counts. This requires an
absurd amount of storage, and makes prediction 1000 times slower
than the vanilla perceptron.                                                             The training algorithm for the voted
   A much more practical alternative is the averaged perceptron.                         perceptron is the same as the
                                                                                         vanilla perceptron. In particular,
The idea is similar: you maintain a collection of weight vectors and                     in line 5 of Algorithm 3.2, the ac-
survival times. However, at test time, you predict according to the                  ?   tivation on a training example is
average weight vector, rather than the voting. In particular, the predic-                computed based on the current
                                                                                         weight vector, not based on the voted
tion is:                                                                                 prediction. Why?
                                       !
               K                    
   ŷ = sign ∑ c(k) w(k) · x̂ + b(k)                                (3.18)
               k =1

The only difference between the voted prediction, Eq (??), and the
50     a course in machine learning



Algorithm 7 AveragedPerceptronTrain(D, MaxIter)
 1:  w ← h0, 0, . . . 0i , b ← 0                                    // initialize weights and bias
 2:  u ← h0, 0, . . . 0i , β ← 0                         // initialize cached weights and bias
  3: c ← 1                                                 // initialize example counter to one
  4: for iter = 1 . . . MaxIter do

  5:   for all (x,y) ∈ D do
  6:       if y(w · x + b) ≤ 0 then
  7:          w←w+yx                                                         // update weights
  8:          b←b+y                                                              // update bias
  9:          u←u+ycx                                               // update cached weights
 10:          β←β+yc                                                     // update cached bias
 11:       end if
 12:       c←c+1                                    // increment counter regardless of update
 13:   end for
 14: end for
                    1        1
 15: return w -
                    c u, b - c β                          // return averaged weights and bias



averaged prediction, Eq (3.18), is the presense of the interior sign
operator. With a little bit of algebra, we can rewrite the test-time
prediction as:
                                     !                        !
                   K                          K
      ŷ = sign   ∑c w   (k)   (k)
                                         · x̂ + ∑ c b
                                                    (k) (k)
                                                                                           (3.19)
                  k =1                       k =1

The advantage of the averaged perceptron is that we can simply
maintain a running sum of the averaged weight vector (the blue term)
and averaged bias (the red term). Test-time prediction is then just as
efficient as it is with the vanilla perceptron.
   The full training algorithm for the averaged perceptron is shown
in Algorithm 3.6. Some of the notation is changed from the original
perceptron: namely, vector operations are written as vector opera-
tions, and the activation computation is folded into the error check-
ing.
   It is probably not immediately apparent from Algorithm 3.6 that
the computation unfolding is precisely the calculation of the averaged
weights and bias. The most natural implementation would be to keep
track of an averaged weight vector u. At the end of every example,
you would increase u ← u + w (and similarly for the bias). However,
such an implementation would require that you updated the aver-
aged vector on every example, rather than just on the examples that
were incorrectly classified! Since we hope that eventually the per-
ceptron learns to do a good job, we would hope that it will not make
updates on every example. So, ideally, you would like to only update
the averaged weight vector when the actual weight vector changes.
The slightly clever computation in Algorithm 3.6 achieves this.                                          By writing out the computation of
   The averaged perceptron is almost always better than the per-                                         the averaged weights from Eq (??)
                                                                                                     ?   as a telescoping sum, derive the
                                                                                                         computation from Algorithm 3.6.
                                                                                              the perceptron            51



      ceptron, in the sense that it generalizes better to test data. However,
      that does not free you from having to do early stopping. It will,
      eventually, overfit. Figure 3.11 shows the performance of the vanilla
      perceptron and the averaged perceptron on the same data set, with
      both training and test performance. As you can see, the averaged
      perceptron does generalize better. But it also does begin to overfit
      eventually.


3.7   Limitations of the Perceptron

      Although the perceptron is very useful, it is fundamentally limited in
      a way that neither decision trees nor KNN are. Its limitation is that
      its decision boundaries can only be linear. The classic way of showing
      this limitation is through the XOR problem (XOR = exclusive or). The
      XOR problem is shown graphically in Figure 3.12. It consists of four
      data points, each at a corner of the unit square. The labels for these      Figure 3.12: picture of xor problem
      points are the same, along the diagonals. You can try, but you will
      not be able to find a linear decision boundary that perfectly separates
      these data points.
          One question you might ask is: do XOR-like problems exist in
      the real world? Unfortunately for the perceptron, the answer is yes.
      Consider a sentiment classification problem that has three features
      that simply say whether a given word is contained in a review of
      a course. These features are: excellent, terrible and not. The
      excellent feature is indicative of positive reviews and the terrible
      feature is indicative of negative reviews. But in the presence of the
      not feature, this categorization flips.
          One way to address this problem is by adding feature combina-
      tions. We could add two additional features: excellent-and-not
      and terrible-and-not that indicate a conjunction of these base
      features. By assigning weights as follows, you can achieve the desired
      effect:

                wexecellent = +1              wterrible = −1      wnot = 0
        wexecllent-and-not = −2       wterrible-and-not = +2

      In this particular case, we have addressed the problem. However, if
      we start with D-many features, if we want to add all pairs, we’ll blow
      up to ( D2 ) = O( D2 ) features through this feature mapping. And
      there’s no guarantee that pairs of features is enough. We might need
      triples of features, and now we’re up to ( D3 ) = O( D2 ) features. These
      additional features will drastically increase computation and will
      often result in a stronger propensity to overfitting.                            Suppose that you took the XOR
         In fact, the “XOR problem” is so significant that it basically killed         problem and added one new fea-
                                                                                       ture: x3 = x1 ∧ x2 (the logical and
      research in classifiers with linear decision boundaries for a decade
                                                                                   ?   of the two existing features). Write
                                                                                       out feature weights and a bias that
                                                                                       would achieve perfect classification
                                                                                       on this data.
      52   a course in machine learning



      or two. Later in this book, we will see two alternative approaches to
      taking key ideas from the perceptron and generating classifiers with
      non-linear decision boundaries. One approach is to combine multi-
      ple perceptrons in a single framework: this is the neural networks
      approach (see Chapter 8). The second approach is to find computa-
      tionally efficient ways of doing feature mapping in a computationally
      and statistically efficient way: this is the kernels approach (see Chap-
      ter 9).


3.8   Exercises

      Exercise 3.1. TODO. . .
                                                                4 | P RACTICAL I SSUES

        In theory, there is no difference between theory and practice.          Learning Objectives:
        But, in practice, there is.             – Jan L.A. van de Snepscheut    • Translate between a problem de-
                                                                                  scription and a concrete learning
                                                                                  problem.
                                                                                • Perform basic feature engineering on
                                                                                  image and text data.
                                                                                • Explain how to use cross-validation
                                                                                  to tune hyperparameters and esti-
        TODO: one two two examples per feature                                    mate future performance.
                                                                                • Compare and contrast the differ-
                                                                                  ences between several evaluation
      At this point, you have seen three qualitatively different models
                                                                                  metrics.
      for learning: decision trees, nearest neighbors, and perceptrons. You
                                                                                • Explain why feature combinations
      have also learned about clustering with the K-means algorithm. You          are important for learning with
      will shortly learn about more complex models, most of which are             some models but not others.

      variants on things you already know. However, before attempting           • Explain the relationship between the
                                                                                  three learning techniques you have
      to understand more complex models of learning, it is important to           seen so far.
      have a firm grasp on how to use machine learning in practice. This        • Apply several debugging techniques
      chapter is all about how to go from an abstract learning problem            to learning algorithms.
      to a concrete implementation. You will see some examples of “best
      practices” along with justifications of these practices.
         In many ways, going from an abstract problem to a concrete learn-
      ing task is more of an art than a science. However, this art can have     Dependencies: Chap-
                                                                                ter ??,Chapter ??,Chapter ??
      a huge impact on the practical performance of learning systems. In
      many cases, moving to a more complicated learning algorithm will
      gain you a few percent improvement. Going to a better representa-
      tion will gain you an order of magnitude improvement. To this end,
      we will discuss several high level ideas to help you develop a better
      artistic sensibility.


4.1   The Importance of Good Features

      Machine learning is magical. You give it data and it manages to
      classify that data. For many, it can actually outperform a human! But,
      like so many problems in the world, there is a significant “garbage
      in, garbage out” aspect to machine learning. If the data you give it is
      trash, the learning algorithm is unlikely to be able to overcome it.
         Consider a problem of object recognition from images. If you start
      with a 100×100 pixel image, a very easy feature representation of
      this image is as a 30, 000 dimensional vector, where each dimension
      corresponds to the red, green or blue component of some pixel in
      the image. So perhaps feature 1 is the amount of red in pixel (1, 1);
      feature 2 is the amount of green in that pixel; and so on. This is the
      54   a course in machine learning



      pixel representation of images.
         One thing to keep in mind is that the pixel representation throws
      away all locality information in the image. Learning algorithms don’t
      care about features: they only care about feature values. So I can
      permute all of the features, with no effect on the learning algorithm
      (so long as I apply the same permutation to all training and test
      examples). Figure 4.1 shows some images whos pixels have been
      randomly permuted (in this case only the pixels are permuted, not
      the colors). All of these objects are things that you’ve seen plenty of
      examples of; can you identify them? Should you expect a machine to
      be able to?
                                                                                 Figure 4.1: prac:imagepix: object
         An alternative representation of images is the patch represen-          recognition in pixels
      tation, where the unit of interest is a small rectangular block of an
      image, rather than a single pixel. Again, permuting the patches has
      no effect on the classifier. Figure 4.2 shows the same images in patch
      representation. Can you identify them? A final representation is a
      shape representation. Here, we throw out all color and pixel infor-
      mation and simply provide a bounding polygon. Figure 4.3 shows
      the same images in this representation. Is this now enough to iden-
      tify them? (If not, you can find the answers at the end of this chap-
      ter.)
         In the context of text categorization (for instance, the sentiment
      recognition task), one standard representation is the bag of words
      representation. Here, we have one feature for each unique word that
                                                                                 Figure 4.2: prac:imagepatch: object
      appears in a document. For the feature happy, the feature value is         recognition in patches
      the number of times that the word “happy” appears in the document.
      The bag of words (BOW) representation throws away all position
      information. Figure 4.4 shows a BOW representation for two docu-
      ments: one positive and one negative. Can you tell which is which?


4.2   Irrelevant and Redundant Features

      One big difference between learning models is how robust they are to
      the addition of noisy or irrelevant features. Intuitively, an irrelevant
      feature is one that is completely uncorrelated with the prediction
      task. A feature f whose expectation does not depend on the label
      E[ f | Y ] = E[ f ] might be irrelevant. For instance, the presence of     Figure 4.3: prac:imageshape: object
      the word “the” might be largely irrelevant for predicting whether a        recognition in shapes
      course review is positive or negative.
         A secondary issue is how well these algorithms deal with redun-
      dant features. Two features are redundant if they are highly cor-
      related, regardless of whether they are correlated with the task or
      not. For example, having a bright red pixel in an image at position
      (20, 93) is probably highly redundant with having a bright red pixel




                                                                                 Figure 4.4: prac:bow: BOW repr of one
                                                                                 positive and one negative review
                                                                                         practical issues           55



at position (21, 93). Both might be useful (e.g., for identifying fire hy-
drants), but because of how images are structured, these two features
are likely to co-occur frequently.
    When thinking about robustness to irrelevant or redundant fea-
tures, it is usually not worthwhile thinking of the case where one has
999 great features and 1 bad feature. The interesting case is when the
bad features outnumber the good features, and often outnumber by
a large degree. For instance, perhaps the number of good features is
something like log D out of a set of D total features. The question is
how robust are algorithms in this case.1                                       1
                                                                                You might think it’s crazy to have so
    For shallow decision trees, the model explicitly selects features          many irrelevant features, but the cases
                                                                               you’ve seen so far (bag of words, bag
that are highly correlated with the label. In particular, by limiting the      of pixels) are both reasonable examples
depth of the decision tree, one can at least hope that the model will be       of this! How many words, out of the
                                                                               entire English vocabulary (roughly
able to throw away irrelevant features. Redundant features are almost          10, 000 − 100, 000 words), are actually
certainly thrown out: once you select one feature, the second feature          useful for predicting positive and
now looks mostly useless. The only possible issue with irrelevant              negative course reviews?

features is that even though they’re irrelevant, they happen to correlate
with the class label on the training data, but chance.
    As a thought experiment, suppose that we have N training ex-
amples, and exactly half are positive examples and half are negative
examples. Suppose there’s some binary feature, f , that is completely
uncorrelated with the label. This feature has a 50/50 chance of ap-
pearing in any example, regardless of the label. In principle, the deci-
sion tree should not select this feature. But, by chance, especially if N
is small, the feature might look correlated with the label. This is anal-
ogous to flipping two coins simultaneously N times. Even though the
coins are independent, it’s entirely possible that you will observe a
sequence like ( H, H ), ( T, T ), ( H, H ), ( H, H ), which makes them look
entirely correlated! The hope is that as N grows, this becomes less
and less likely. In fact, we can explicitly compute how likely this is to
happen.
    To do this, let’s fix the sequence of N labels. We now flip a coin N
times and consider how likely it is that it exactly matches the label.
This is easy: the probability is 0.5 N . Now, we would also be confused
if it exactly matched not the label, which has the same probability. So
the chance that it looks perfectly correlated is 0.5 N + 0.5 N = 0.5 N −1 .
Thankfully, this shrinks down very small (e.g., 10−6 ) after only 21
data points.
    This makes us happy. The problem is that we don’t have one ir-
relevant feature: we have D − log D irrelevant features! If we ran-
domly pick two irrelevant feature values, each has the same prob-
ability of perfectly correlating: 0.5 N −1 . But since there are two and
they’re independent coins, the chance that either correlates perfectly
is 2×0.5 N −1 = 0.5 N −2 . In general, if we have K irrelevant features, all
      56   a course in machine learning



      of which are random independent coins, the chance that at least one
      of them perfectly correlates is 0.5 N −K . This suggests that if we have
      a sizeable number K of irrelevant features, we’d better have at least
      K + 21 training examples.
         Unfortunately, the situation is actually worse than this. In the
      above analysis we only considered the case of perfect correlation. We
      could also consider the case of partial correlation, which would yield
      even higher probabilities. (This is left as Exercise ?? for those who
      want some practice with probabilistic analysis.) Suffice it to say that
      even decision trees can become confused.
         In the case of K-nearest neighbors, the situation is perhaps more
      dire. Since KNN weighs each feature just as much as another feature,
      the introduction of irrelevant features can completely mess up KNN
      prediction. In fact, as you saw, in high dimensional space, randomly
      distributed points all look approximately the same distance apart.         Figure 4.5: prac:addirel: data from
      If we add lots and lots of randomly distributed features to a data         high dimensional warning, interpolated

      set, then all distances still converge. This is shown experimentally in
      Figure ??, where we start with the digit categorization data and con-
      tinually add irrelevant, uniformly distributed features, and generate a
      histogram of distances. Eventually, all distances converge.
         In the case of the perceptron, one can hope that it might learn to
      assign zero weight to irrelevant features. For instance, consider a
      binary feature is randomly one or zero independent of the label. If
      the perceptron makes just as many updates for positive examples
      as for negative examples, there is a reasonable chance this feature
      weight will be zero. At the very least, it should be small.                       What happens with the perceptron
         To get a better practical sense of how sensitive these algorithms           ? with truly redundant features (i.e.,
                                                                                        one is literally a copy of the other)?
      are to irrelevant features, Figure 4.6 shows the test performance of
      the three algorithms with an increasing number of compltely noisy
      features. In all cases, the hyperparameters were tuned on validation
      data. TODO...


4.3   Feature Pruning and Normalization

      In text categorization problems, some words simply do not appear
      very often. Perhaps the word “groovy”2 appears in exactly one train-
      ing document, which is positive. Is it really worth keeping this word
      around as a feature? It’s a dangerous endeavor because it’s hard to
      tell with just one training example if it is really correlated with the
      positive class, or is it just noise. You could hope that your learning
      algorithm is smart enough to figure it out. Or you could just remove
      it. That means that (a) the learning algorithm won’t have to figure it
      out, and (b) you’ve reduced the number of dimensions you have, so
      things should be more efficient, and less “scary.”

                                                                                 Figure 4.6: prac:noisy: dt,knn,perc on
                                                                                 increasing amounts of noise
                                                                                 2
                                                                                  This is typically positive indicator,
                                                                                 or at least it was back in the US in the
                                                                                 1970s.
                                                                                         practical issues             57



   M ATH R EVIEW | DATA S TATISTICS : M EANS AND VARIANCES
  data mean, variance, moments, expectations, etc...



                                                                           Figure 4.8:

   This idea of feature pruning is very useful and applied in many
applications. It is easiest in the case of binary features. If a binary
feature only appears some small number K times (in the training
data: no fair looking at the test data!), you simply remove it from
consideration. (You might also want to remove features that appear
in all-but-K many documents, for instance the word “the” appears in
pretty much every English document ever written.) Typical choices
for K are 1, 2, 5, 10, 20, 50, mostly depending on the size of the data.
On a text data set with 1000 documents, a cutoff of 5 is probably
reasonable. On a text data set the size of the web, a cut of of 50 or
even 100 or 200 is probably reasonable3 . Figure 4.7 shows the effect      3
                                                                            According to Google, the following
of pruning on a sentiment analysis task. In the beginning, pruning         words (among many others) appear
                                                                           200 times on the web: moudlings, agag-
does not hurt (and sometimes helps!) but eventually we prune away          gagctg, setgravity, rogov, prosomeric,
all the interesting words and performance suffers.                         spunlaid, piyushtwok, telelesson, nes-
                                                                           mysl, brighnasa. For comparison, the
   In the case of real-valued features, the question is how to extend      word “the” appears 19, 401, 194, 714 (19
the idea of “does not occur much” to real values. A reasonable def-        billion) times.
inition is to look for features with low variance. In fact, for binary
features, ones that almost never appear or almost always appear will
also have low variance. Figure 4.9 shows the result of pruning low-
variance features on the digit recognition task. Again, at first pruning
does not hurt (and sometimes helps!) but eventually we have thrown
out all the useful features.
   Once you have pruned away irrelevant features, it is often useful
to normalize the data so that it is consistent in some way. There are
two basic types of normalization: feature normalization and exam-
ple normalization. In feature normalization, you go through each
feature and adjust it the same way across all examples. In example
normalization, each example is adjusted individually.                      Figure 4.9: prac:variance: effect of
   The goal of both types of normalization is to make it easier for your   pruning on vision
learning algorithm to learn. In feature normalization, there are two               Earlier we discussed the problem
                                                                                   of scale of features (e.g., millimeters
standard things to do:
                                                                               ?   versus centimeters). Does this have
                                                                                   an impact on variance-based feature
1. Centering: moving the entire data set so that it is centered around
                                                                                   pruning?
   the origin.

2. Scaling: rescaling each feature so that one of the following holds:

  (a) Each feature has variance 1 across the training data.
  (b) Each feature has maximum absolute value 1 across the train-
     ing data.
58    a course in machine learning



These transformations are shown geometrically in Figure 4.10. The
goal of centering is to make sure that no features are arbitrarily large.
The goal of scaling is to make sure that all features have roughly the
same scale (to avoid the issue of centimeters versus millimeters).               For the three models you know
   These computations are fairly straightforward. Here, xn,d refers              about (KNN, DT, Perceptron),
to the dth feature of example n. Since it is very rare to apply scaling      ?   which are most sensitive to center-
                                                                                 ing? Which are most sensitive to
without previously applying centering, the expressions below for                 scaling?
scaling assume that the data is already centered.

           Centering:               xn,d ← xn,d − µd                (4.1)
     Variance Scaling:              xn,d ← xn,d /σd                 (4.2)
     Absolute Scaling:              xn,d ← xn,d /rd                 (4.3)
                                           1
                                           N∑
                         where:       µd =      xn,d                (4.4)
                                              n
                                           s
                                             1
                                             N∑
                                      σd =        ( xn,d − µd )2    (4.5)
                                                n

                                      rd = max xn,d                 (4.6)
                                             n

   In practice, if the dynamic range of your features is already some
subset of [−2, 2] or [−3, 3], then it is probably not worth the effort of
centering and scaling. (It’s an effort because you have to keep around
your centering and scaling calculations so that you can apply them
to the test data as well!) However, if some of your features are orders
of magnitude larger than others, it might be helpful. Remember that
you might know best: if the difference in scale is actually significant
for your problem, then rescaling might throw away useful informa-
tion.
   One thing to be wary of is centering binary data. In many cases,
binary data is very sparse: for a given example, only a few of the
features are “on.” For instance, out of a vocabulary of 10, 000 or
100, 000 words, a given document probably only contains about 100.
From a storage and computation perspective, this is very useful.
However, after centering, the data will no longer sparse and you will
pay dearly with outrageously slow implementations.
   In example normalization, you view examples one at a time. The
most standard normalization is to ensure that the length of each
example vector is one: namely, each example lies somewhere on the
unit hypersphere. This is a simple transformation:
                                                                            Figure 4.11: prac:exnorm: example of
     Example Normalization:                xn ← xn / || xn ||       (4.7)   example normalization

This transformation is depicted in Figure 4.11.
  The main advantage to example normalization is that it makes
comparisons more straightforward across data sets. If I hand you
                                                                                             practical issues          59



      two data sets that differ only in the norm of the feature vectors (i.e.,
      one is just a scaled version of the other), it is difficult to compare the
      learned models. Example normalization makes this more straightfor-
      ward. Moreover, as you saw in the perceptron convergence proof, it is
      often just mathematically easier to assume normalized data.


4.4   Combinatorial Feature Explosion

      You learned in Chapter 3 that linear models (like the perceptron)
      cannot solve the XOR problem. You also learned that by performing
      a combinatorial feature explosion, they could. But that came at the
      computational expense of gigantic feature vectors.
          Of the algorithms that you’ve seen so far, the perceptron is the one
      that has the most to gain by feature combination. And the decision
      tree is the one that has the least to gain. In fact, the decision tree
      construction is essentially building meta features for you. (Or, at
      least, it is building meta features constructed purely through “logical
      ands.”)
          This observation leads to a heuristic for constructing meta features
      for perceptrons from decision trees. The idea is to train a decision
      tree on the training data. From that decision tree, you can extract
      meta features by looking at feature combinations along branches. You
      can then add only those feature combinations as meta features to the         Figure 4.12: prac:dttoperc: turning a
      feature set for the perceptron. Figure 4.12 shows a small decision tree      DT into a set of meta features

      and a set of meta features that you might extract from it. There is a
      hyperparameter here of what length paths to extract from the tree: in
      this case, only paths of length two are extracted. For bigger trees, or
      if you have more data, you might benefit from longer paths.
          In addition to combinatorial transformations, the logarithmic
      transformation can be quite useful in practice. It seems like a strange
      thing to be useful, since it doesn’t seem to fundamentally change
      the data. However, since many learning algorithms operate by linear
      operations on the features (both perceptron and KNN do this), the
      log-transform is a way to get product-like operations. The question is
      which of the following feels more applicable to your data: (1) every
      time this feature increases by one, I’m equally more likely to predict       Figure 4.13: prac:log: performance on
                                                                                   text categ with word counts versus log
      a positive label; (2) every time this feature doubles, I’m equally more      word counts
      like to predict a positive label. In the first case, you should stick
      with linear features and in the second case you should switch to
      a log-transform. This is an important transformation in text data,
      where the presence of the word “excellent” once is a good indicator
      of a positive review; seeing “excellent” twice is a better indicator;
      but the difference between seeing “excellent” 10 times and seeing it
      11 times really isn’t a big deal any more. A log-transform achieves
      60    a course in machine learning



      this. Experimentally, you can see the difference in test performance
      between word count data and log-word count data in Figure 4.13.
      Here, the transformation is actually xd 7→ log2 ( xd + 1) to ensure that
      zeros remain zero and sparsity is retained.


4.5   Evaluating Model Performance

      So far, our focus has been on classifiers that achieve high accuracy.
      In some cases, this is not what you might want. For instance, if you
      are trying to predict whether a patient has cancer or not, it might be
      better to err on one side (saying they have cancer when they don’t)
      than the other (because then they die). Similarly, letting a little spam
      slip through might be better than accidentally blocking one email
      from your boss.
         There are two major types of binary classification problems. One
      is “X versus Y.” For instance, positive versus negative sentiment.
      Another is “X versus not-X.” For instance, spam versus non-spam.
      (The argument being that there are lots of types of non-spam.) Or
      in the context of web search, relevant document versus irrelevant
      document. This is a subtle and subjective decision. But “X versus not-
      X” problems often have more of the feel of “X spotting” rather than
      a true distinction between X and Y. (Can you spot the spam? can you
      spot the relevant documents?)
         For spotting problems (X versus not-X), there are often more ap-
      propriate success metrics than accuracy. A very popular one from
      information retrieval is the precision/recall metric. Precision asks
      the question: of all the X’s that you found, how many of them were
      actually X’s? Recall asks: of all the X’s that were out there, how many
      of them did you find?4 Formally, precision and recall are defined as:       4
                                                                                   A colleague make the analogy to the
                                                                                  US court system’s saying “Do you
               I                                                                  promise to tell the whole truth and
           P=                                                             (4.8)   nothing but the truth?” In this case, the
               S
                                                                                  “whole truth” means high recall and
                I
           R=                                                             (4.9)   “nothing but the truth” means high
               T                                                                  precision.”
           S = number of Xs that your system found                       (4.10)
           T = number of Xs in the data                                  (4.11)
           I = number of correct Xs that your system found               (4.12)

      Here, S is mnemonic for “System,” T is mnemonic for “Truth” and I
      is mnemonic for “Intersection.” It is generally accepted that 0/0 = 1
      in these definitions. Thus, if you system found nothing, your preci-
      sion is always perfect; and if there is nothing to find, your recall is
      always perfect.
         Once you can compute precision and recall, you are often able to
      produce precision/recall curves. Suppose that you are attempting


                                                                                  Figure 4.14: prac:spam: show a bunch
                                                                                  of emails spam/nospam sorted by
                                                                                  model predicion, not perfect
                                                                                           practical issues             61



to identify spam. You run a learning algorithm to make predictions
on a test set. But instead of just taking a “yes/no” answer, you allow
your algorithm to produce its confidence. For instance, in perceptron,
you might use the distance from the hyperplane as a confidence
measure. You can then sort all of your test emails according to this
ranking. You may put the most spam-like emails at the top and the
least spam-like emails at the bottom, like in Figure 4.14.                         How would you get a confidence
   Once you have this sorted list, you can choose how aggressively            ? out of a decision tree or KNN?
you want your spam filter to be by setting a threshold anywhere on
this list. One would hope that if you set the threshold very high, you
are likely to have high precision (but low recall). If you set the thresh-
old very low, you’ll have high recall (but low precision). By consider-
ing every possible place you could put this threshold, you can trace out
a curve of precision/recall values, like the one in Figure 4.15. This
allows us to ask the question: for some fixed precision, what sort of
recall can I get. Obviously, the closer your curve is to the upper-right
corner, the better. And when comparing learning algorithms A and
B you can say that A dominates B if A’s precision/recall curve is
always higher than B’s.
   Precision/recall curves are nice because they allow us to visualize
many ways in which we could use the system. However, sometimes
we like to have a single number that informs us of the quality of the
solution. A popular way of combining precision and recall into a
single number is by taking their harmonic mean. This is known as
the balanced f-measure (or f-score):

         2×P×R                                                               Figure 4.15: prac:prcurve: precision
  F=                                                                (4.13)   recall curve
          P+R
                                                                                     0.0    0.2    0.4    0.6    0.8      1.0
The reason that you want to use a harmonic mean rather than an               0.0    0.00    0.00   0.00   0.00   0.00    0.00
                                                                             0.2    0.00    0.20   0.26   0.30   0.32    0.33
arithmetic mean (the one you’re more used to) is that it favors sys-
                                                                             0.4    0.00    0.26   0.40   0.48   0.53    0.57
tems that achieve roughly equal precision and recall. In the extreme         0.6    0.00    0.30   0.48   0.60   0.68    0.74
case where P = R, then F = P = R. But in the imbalanced case, for            0.8    0.00    0.32   0.53   0.68   0.80    0.88
                                                                             1.0    0.00    0.33   0.57   0.74   0.88    1.00
instance P = 0.1 and R = 0.9, the overall f-measure is a modest 0.18.
                                                                             Table 4.1: Table of f-measures when
Table 4.1 shows f-measures as a function of precision and recall, so         varying precision and recall values.
that you can see how important it is to get balanced values.
   In some cases, you might believe that precision is more impor-
tant than recall. This idea leads to the weighted f-measure, which is
parameterized by a weight β ∈ [0, ∞) (beta):

         (1 + β2 )×P×R
  Fβ =                                                              (4.14)
           β2×P + R

For β = 1, this reduces to the standard f-measure. For β = 0, it
focuses entirely on recall and for β → ∞ it focuses entirely on preci-
sion. The interpretation of the weight is that Fβ measures the perfor-
      62   a course in machine learning



      mance for a user who cares β times as much about precision as about
      recall.
         One thing to keep in mind is that precision and recall (and hence
      f-measure) depend crucially on which class is considered the thing
      you wish to find. In particular, if you take a binary data set if flip
      what it means to be a positive or negative example, you will end
      up with completely difference precision and recall values. It is not
      the case that precision on the flipped task is equal to recall on the
      original task (nor vice versa). Consequently, f-measure is also not the
      same. For some tasks where people are less sure about what they
      want, they will occasionally report two sets of precision/recall/f-
      measure numbers, which vary based on which class is considered the
      thing to spot.
         There are other standard metrics that are used in different com-
      munities. For instance, the medical community is fond of the sensi-
      tivity/specificity metric. A sensitive classifier is one which almost
      always finds everything it is looking for: it has high recall. In fact,
      sensitivity is exactly the same as recall. A specific classifier is one
      which does a good job not finding the things that it doesn’t want to
      find. Specificity is precision on the negation of the task at hand.
         You can compute curves for sensitivity and specificity much like
      those for precision and recall. The typical plot, referred to as the re-
      ceiver operating characteristic (or ROC curve) plots the sensitivity
      against 1 − specificity. Given an ROC curve, you can compute the
      area under the curve (or AUC) metric, which also provides a mean-
      ingful single number for a system’s performance. Unlike f-measures,
      which tend to be low because the require agreement, AUC scores
      tend to be very high, even for not great systems. This is because ran-
      dom chance will give you an AUC of 0.5 and the best possible AUC
      is 1.0.
         The main message for evaluation metrics is that you should choose
      whichever one makes the most sense. In many cases, several might
      make sense. In that case, you should do whatever is more commonly
      done in your field. There is no reason to be an outlier without cause.


4.6   Cross Validation

      In Chapter 1, you learned about using development data (or held-out
      data) to set hyperparameters. The main disadvantage to the develop-
      ment data approach is that you throw out some of your training data,
      just for estimating one or two hyperparameters.
         An alternative is the idea of cross validation. In cross validation,
      you break your training data up into 10 equally-sized partitions. You
      train a learning algorithm on 9 of them and test it on the remaining
                                                                                                    practical issues   63



Algorithm 8 CrossValidate(LearningAlgorithm, Data, K)
  1: ê ← ∞                                         // store lowest error encountered so far
  2: α̂ ← unknown                       // store the hyperparameter setting that yielded it
  3: for all hyperparameter settings α do

  4:    err ← [ ]                              // keep track of the K -many error estimates
  5:    for k = 1 to K do
  6:       train ← {( xn , yn ) ∈ Data : n mod K 6= k − 1}
  7:       test ← {( xn , yn ) ∈ Data : n mod K = k − 1} // test every Kth example
  8:       model ← Run LearningAlgorithm on train
  9:       err ← err ⊕ error of model on test              // add current error to list of errors
 10:    end for
 11:    avgErr ← mean of set err
 12:    if avgErr < ê then
 13:       ê ← avgErr                                            // remember these settings
 14:       α̂ ← α                                           // because they’re the best so far
 15:    end if
 16: end for




1. You do this 10 times, each time holding out a different partition as
the “development” part. You can then average your performance over
all ten parts to get an estimate of how well your model will perform
in the future. You can repeat this process for every possible choice of
hyperparameters to get an estimate of which one performs best. The
general K-fold cross validation technique is shown in Algorithm 4.6,
where K = 10 in the preceeding discussion.
   In fact, the development data approach can be seen as an approxi-
mation to cross validation, wherein only one of the K loops (line 5 in
Algorithm 4.6) is executed.
   Typical choices for K are 2, 5, 10 and N − 1. By far the most com-
mon is K = 10: 10-fold cross validation. Sometimes 5 is used for
efficiency reasons. And sometimes 2 is used for subtle statistical rea-
sons, but that is quite rare. In the case that K = N − 1, this is known
as leave-one-out cross validation (or abbreviated as LOO cross val-
idation). After running cross validation, you have two choices. You
can either select one of the K trained models as your final model to
make predictions with, or you can train a new model on all of the
data, using the hyperparameters selected by cross-validation. If you
have the time, the latter is probably a better options.
   It may seem that LOO cross validation is prohibitively expensive
to run. This is true for most learning algorithms except for K-nearest
neighbors. For KNN, leave-one-out is actually very natural. We loop
through each training point and ask ourselves whether this example
would be correctly classified for all different possible values of K.
This requires only as much computation as computing the K nearest
neighbors for the highest value of K. This is such a popular and
effective approach for KNN classification that it is spelled out in
64   a course in machine learning



Algorithm 9 KNN-Train-LOO(D)
  1: errk ← 0, ∀1 ≤ k ≤ N − 1               // errk stores how well you do with kNN
  2: for n = 1 to N do

  3:    Sm ← h|| xn − xm || , mi, ∀m 6= n       // compute distances to other points
  4:    S ← sort(S)                                // put lowest-distance objects first
  5:    ŷ ← 0                                               // current label prediction
  6:    for k = 1 to N − 1 do
  7:       hdist,mi ← Sk
  8:       ŷ ← ŷ + ym                                   // let kth closest point vote
  9:       if ŷ 6= ym then
 10:           errk ← errk + 1                              // one more error for kNN
 11:       end if
 12:    end for
 13: end for

 14: return argmin err k                    // return the K that achieved lowest error
                       k




Algorithm ??.
    Overall, the main advantage to cross validation over develop-
ment data is robustness. The main advantage of development data is
speed.
    One warning to keep in mind is that the goal of both cross valida-
tion and development data is to estimate how well you will do in the
future. This is a question of statistics, and holds only if your test data
really looks like your training data. That is, it is drawn from the same
distribution. In many practical cases, this is not entirely true.
    For example, in person identification, we might try to classify
every pixel in an image based on whether it contains a person or not.
If we have 100 training images, each with 10, 000 pixels, then we have
a total of 1m training examples. The classification for a pixel in image
5 is highly dependent on the classification for a neighboring pixel in
the same image. So if one of those pixels happens to fall in training
data, and the other in development (or cross validation) data, your
model will do unreasonably well. In this case, it is important that
when you cross validate (or use development data), you do so over
images, not over pixels. The same goes for text problems where you
sometimes want to classify things at a word level, but are handed a
collection of documents. The important thing to keep in mind is that
it is the images (or documents) that are drawn independently from
your data distribution and not the pixels (or words), which are drawn
dependently.
                                                                                practical issues   65



4.7   Hypothesis Testing and Statistical Significance


         V IGNETTE : T HE L ADY D RINKING T EA
        story



         Suppose that you’ve presented a machine learning solution to your
      boss that achieves 7% error on cross validation. Your nemesis, Gabe,
      gives a solution to your boss that achieves 6.9% error on cross vali-
      dation. How impressed should your boss be? It depends. If this 0.1%
      improvement was measured over 1000 examples, perhaps not too
      impressed. It would mean that Gabe got exactly one more example
      right than you did. (In fact, he probably got 15 more right and 14
      more wrong.) If this 0.1% impressed was measured over 1, 000, 000
      examples, perhaps this is more impressive.
         This is one of the most fundamental questions in statistics. You
      have a scientific hypothesis of the form “Gabe’s algorithm is better
      than mine.” You wish to test whether this hypothesis is true. You
      are testing it against the null hypothesis, which is that Gabe’s algo-
      rithm is no better than yours. You’ve collected data (either 1000 or
      1m data points) to measure the strength of this hypothesis. You want
      to ensure that the difference in performance of these two algorithms
      is statistically significant: i.e., is probably not just due to random
      luck. (A more common question statisticians ask is whether one drug
      treatment is better than another, where “another” is either a placebo
      or the competitor’s drug.)
         There are about ∞-many ways of doing hypothesis testing. Like
      evaluation metrics and the number of folds of cross validation, this is
      something that is very discipline specific. Here, we will discuss two
      popular tests: the paired t-test and bootstrapping. These tests, and
      other statistical tests, have underlying assumptions (for instance, as-
      sumptions about the distribution of observations) and strengths (for
      instance, small or large samples). In most cases, the goal of hypoth-
      esis testing is to compute a p-value: namely, the probability that the
      observed difference in performance was by chance. The standard way
      of reporting results is to say something like “there is a 95% chance
      that this difference was not by chance.” The value 95% is arbitrary,
      and occasionally people use weaker (90%) test or stronger (99.5%)
      tests.
         The t-test is an example of a parametric test. It is applicable when
      the null hypothesis states that the difference between two responses
      has mean zero and unknown variance. The t-test actually assumes
      that data is distributed according to a Gaussian distribution, which is
66   a course in machine learning



probably not true of binary responses. Fortunately, for large samples
(at least a few hundred), binary seamples are well approximated by                                t      significance
                                                                                               ≥ 1.28       90.0%
a Gaussian distribution. So long as your sample is sufficiently large,                         ≥ 1.64       95.0%
the t-test is reasonable either for regression or classification problems.                     ≥ 1.96       97.5%
                                                                                               ≥ 2.58       99.5%
   Suppose that you evaluate two algorithm on N-many examples.
                                                                                    Table 4.2: Table of significance values
On each example, you can compute whether the algorithm made                         for the t-test.
the correct prediction. Let a1 , . . . , a N denote the error of the first
algorithm on each example. Let b1 , . . . , b N denote the error of the
second algorithm. You can compute µ a and µb as the means of a and
b, respecitively. Finally, center the data as â = a − µ a and b̂ = b − µb .
The t-statistic is defined as:
                   s
                       N ( N − 1)
   t = (µ a − µb )                                                         (4.15)
                     ∑n ( ân − b̂n )2
After computing the t-value, you can compare it to a list of values
for computing confidence intervals. Assuming you have a lot of data
(N is a few hundred or more), then you can compare your t-value to
Table 4.2 to determine the significance level of the difference.                            What does it mean for the means
   One disadvantage to the t-test is that it cannot easily be applied                       µ a and µb to become further apart?
to evaluation metrics like f-score. This is because f-score is a com-                   ?   How does this affect the t-value?
                                                                                            What happens if the variance of a
puted over an entire test set and does not decompose into a set of                          increases?
individual errors. This means that the t-test cannot be applied.
   Fortunately, cross validation gives you a way around this problem.
When you do K-fold cross validation, you are able to compute K
error metrics over the same data. For example, you might run 5-fold
cross validation and compute f-score for every fold. Perhaps the f-
scores are 92.4, 93.9, 96.1, 92.2 and 94.4. This gives you an average
f-score of 93.8 over the 5 folds. The standard deviation of this set of
f-scores is:
        s
              1
           N−1 ∑
   σ=                ( a i − µ )2                                   (4.16)
                   n
        r
           1
     =       (1.96 + 0.01 + 5.29 + 2.56 + 0.36)                     (4.17)
           4
     = 1.595                                                        (4.18)

You can now assume that the distribution of scores is approximately
Gaussian. If this is true, then approximately 70% of the proba-
bility mass lies in the range [µ − σ, µ + σ ]; 95% lies in the range
[µ − 2σ, µ + 2σ]; and 99.5% lies in the range [µ − 3σ, µ + 3σ]. So, if we
were comparing our algorithm against one whose average f-score was
90.6%, we could be 95% certain that our superior performance was
not due to chance.5                                                                 5
                                                                                     Had we run 10-fold cross validation
   WARNING: A confidence of 95% does not mean “There is a 95%                       we might be been able to get tighter
                                                                                    confidence intervals.
chance that I am better.” All it means is that if I reran the same ex-
                                                                                               practical issues   67



Algorithm 10 BootstrapEvaluate(y, ŷ, NumFolds)
 1:  scores ← [ ]
 2:  for k = 1 to NumFolds do
  3:    truth ← [ ]                                 // list of values we want to predict
  4:    pred ← [ ]                             // list of values we actually predicted
  5:    for n = 1 to N do
  6:       m ← uniform random value from 1 to N                     // sample a test point
  7:       truth ← truth ⊕ ym                                           // add on the truth
  8:       pred ← pred ⊕ ŷm                                      // add on our prediction
  9:    end for
 10:    scores ← scores ⊕ f-score(truth, pred)                                   // evaluate
 11: end for

 12: return (mean(scores), stddev(scores))




periment 100 times, then in 95 of those experiments I would still win.
These are very different statements. If you say the first one, people
who know about statistics will get very mad at you!
   One disadvantage to cross validation is that it is computationally
expensive. More folds typically leads to better estimates, but every
new fold requires training a new classifier. This can get very time
consuming. The technique of bootstrapping (and closely related idea
of jack-knifing can address this problem.
   Suppose that you didn’t want to run cross validation. All you have
is a single held-out test set with 1000 data points in it. You can run
your classifier and get predictions on these 1000 data points. You
would like to be able to compute a metric like f-score on this test set,
but also get confidence intervals. The idea behind bootstrapping is
that this set of 1000 is a random draw from some distribution. We
would like to get multiple random draws from this distribution on
which to evaluate. We can simulate multiple draws by repeatedly
subsampling from these 1000 examples, with replacement.
   To perform a single bootstrap, you will sample 1000 random points
from your test set of 1000 random points. This sampling must be
done with replacement (so that the same example can be sampled
more than once), otherwise you’ll just end up with your original test
set. This gives you a bootstrapped sample. On this sample, you can
compute f-score (or whatever metric you want). You then do this 99
more times, to get a 100-fold bootstrap. For each bootstrapped sam-
ple, you will be a different f-score. The mean and standard deviation
of this set of f-scores can be used to estimate a confidence interval for
your algorithm.
   The bootstrap resampling procedure is sketched in Algorithm 4.7.
This takes three arguments: the true labels y, the predicted labels ŷ
and the number of folds to run. It returns the mean and standard
deviation from which you can compute a confidence interval.
      68   a course in machine learning



4.8   Debugging Learning Algorithms

      Learning algorithms are notoriously hard to debug, as you may have
      already experienced if you have implemented any of the models
      presented so far. The main issue is that when a learning algorithm
      doesn’t learn, it’s unclear if this is because there’s a bug or because
      the learning problem is too hard (or there’s too much noise, or . . . ).
      Moreover, sometimes bugs lead to learning algorithms performing
      better than they should: these are especially hard to catch (and always
      a bit disappointing when you do catch them).
         Obviously if you have a reference implementation, you can at-
      tempt to match its output. Otherwise, there are two things you can
      do to try to help debug. The first is to do everything in your power
      to get the learning algorithm to overfit. If it cannot at least overfit the
      training data, there’s definitely something wrong. The second is to
      feed it some tiny hand-specified two dimensional data set on which
      you know what it should do and you can plot the output.
         The easiest way to try to get a learning algorithm to overfit is to
      add a new feature to it. You can call this feature the CheatingIsFun
      feature6 . The feature value associated with this feature is +1 if this       6
                                                                                     Note: cheating is actually not fun and
      is a positive example and −1 (or zero) if this is a negative example.         you shouldn’t do it!

      In other words, this feature is a perfect indicator of the class of this
      example.
         If you add the CheatingIsFun feature and your algorithm does
      not get near 0% training error, this could be because there are too
      many noisy features confusing it. You could either remove a lot of
      the other features, or make the feature value for CheatingIsFun
      either +100 or −100 so that the algorithm really looks at it. If you
      do this and your algorithm still cannot overfit then you likely have a
      bug. (Remember to remove the CheatingIsFun feature from your
      final implementation!)
         A second thing to try is to hand-craft a data set on which you
      know your algorithm should work. This is also useful if you’ve man-
      aged to get your model to overfit and have simply noticed that it
      does not generalize. For instance, you could run KNN on the XOR
      data. Or you could run perceptron on some easily linearly separable
      data (for instance positive points along the line x2 = x1 + 1 and neg-
      ative points along the line x2 = x1 − 1). Or a decision tree on nice
      axis-aligned data.
         When debugging on hand-crafted data, remember whatever you
      know about the models you are considering. For instance, you know
      that the perceptron should converge on linearly separable data, so
      try it on a linearly separable data set. You know that decision trees
      should do well on data with only a few relevant features, so make
                                                                                               practical issues          69



      your label some easy combination of features, such as y = x1 ∨ ( x2 ∧
      ¬ x3 ). You know that KNN should work well on data sets where the
      classes are well separated, so try such data sets.
         The most important thing to keep in mind is that a lot goes in to
      getting good test set performance. First, the model has to be right for
      the data. So crafting your own data is helpful. Second, the model has
      to fit the training data well, so try to get it to overfit. Third, the model
      has to generalize, so make sure you tune hyperparameters well.
         TODO: answers to image questions


4.9   Exercises

      Exercise 4.1. TODO. . .                                                        Figure 4.16: prac:imageanswers: object
                                                                                     recognition answers
                              5 | B EYOND B INARY C LASSIFICATION
                                                                         –       Learning Objectives:
                                                                                 • Represent complex prediction prob-
                                                                                   lems in a formal learning setting.
                                                                                 • Be able to artifically “balance”
                                                                                   imbalanced data.
                                                                                 • Understand the positive and neg-
                                                                                   ative aspects of several reductions
                                                                                   from multiclass classification to
                                                                                   binary classification.
      In the preceeding chapters, you have learned all about a very              • Recognize the difference between
                                                                                   regression and ordinal regression.
      simple form of prediction: predicting bits. In the real world, however,
                                                                                 • Implement stacking as a method of
      we often need to predict much more complex objects. You may need
                                                                                   collective classification.
      to categorize a document into one of several categories: sports, en-
      tertainment, news, politics, etc. You may need to rank web pages or
      ads based on relevance to a query. You may need to simultaneously
      classify a collection of objects, such as web pages, that have important
      information in the links between them. These problems are all com-
      monly encountered, yet fundamentally more complex than binary
      classification.
         In this chapter, you will learn how to use everything you already
      know about binary classification to solve these more complicated
                                                                                 Dependencies:
      problems. You will see that it’s relatively easy to think of a binary
      classifier as a black box, which you can reuse for solving these more
      complex problems. This is a very useful abstraction, since it allows us
      to reuse knowledge, rather than having to build new learning models
      and algorithms from scratch.


5.1   Learning with Imbalanced Data

      Your boss tells you to build a classifier that can identify fraudulent
      transactions in credit card histories. Fortunately, most transactions
      are legitimate, so perhaps only 0.1% of the data is a positive in-
      stance. The imbalanced data problem refers to the fact that for a
      large number of real world problems, the number of positive exam-
      ples is dwarfed by the number of negative examples (or vice versa).
      This is actually something of a misnomer: it is not the data that is
      imbalanced, but the distribution from which the data is drawn. (And
      since the distribution is imbalanced, so must the data be.)
         Imbalanced data is a problem because machine learning algo-
      rithms are too smart for your own good. For most learning algo-
      rithms, if you give them data that is 99.9% negative and 0.1% posi-
      tive, they will simply learn to always predict negative. Why? Because
                                                                       beyond binary classification   71



they are trying to minimize error, and they can achieve 0.1% error by
doing nothing! If a teacher told you to study for an exam with 1000
true/false questions and only one of them is true, it is unlikely you
will study very long.
   Really, the problem is not with the data, but rather with the way
that you have defined the learning problem. That is to say, what you
care about is not accuracy: you care about something else. If you
want a learning algorithm to do a reasonable job, you have to tell it
what you want!
   Most likely, what you want is not to optimize accuracy, but rather
to optimize some other measure, like f-score or AUC. You want your
algorithm to make some positive predictions, and simply prefer those
to be “good.” We will shortly discuss two heuristics for dealing with
this problem: subsampling and weighting. In subsampling, you throw
out some of you negative examples so that you are left with a bal-
anced data set (50% positive, 50% negative). This might scare you
a bit since throwing out data seems like a bad idea, but at least it
makes learning much more efficient. In weighting, instead of throw-
ing out positive examples, we just given them lower weight. If you
assign an importance weight of 0.00101 to each of the positive ex-
amples, then there will be as much weight associated with positive
examples as negative examples.
   Before formally defining these heuristics, we need to have a mech-
anism for formally defining supervised learning problems. We will
proceed by example, using binary classification as the canonical
learning problem.

   TASK : B INARY C LASSIFICATION
  Given:

  1. An input space X

  2. An unknown distribution D over X×{−1, +1}

  Compute: A function f minimizing: E( x,y)∼D f ( x) 6= y
                                                         




   As in all the binary classification examples you’ve seen, you have
some input space (which has always been RD ). There is some distri-
bution that produces labeled examples over the input space. You do
not have access to that distribution, but can obtain samples from it.
Your goal is to find a classifier that minimizes error on that distribu-
tion.
   A small modification on this definition gives a α-weighted classifi-
cation problem, where you believe that the positive class is α-times as
72     a course in machine learning



Algorithm 11 SubsampleMap(D weighted , α)
 1: while true do

 2:   ( x, y) ∼ D weighted     // draw an example from the weighted distribution
 3:   u ∼ uniform random variable in [0, 1]
 4:   if y = +1 or u < α1 then
 5:       return ( x, y)
 6:   end if
 7: end while




important as the negative class.

     TASK : α -W EIGHTED B INARY C LASSIFICATION
     Given:

     1. An input space X

     2. An unknown distribution D over X×{−1, +1}
                                                h                 i
     Compute: A function f minimizing: E( x,y)∼D αy=1 f ( x) 6= y
                                                     




    The objects given to you in weighted binary classification are iden-
tical to standard binary classification. The only difference is that the
cost of misprediction for y = +1 is α, while the cost of misprediction
for y = −1 is 1. In what follows, we assume that α > 1. If it is not,
you can simply swap the labels and use 1/α.
    The question we will ask is: suppose that I have a good algorithm
for solving the B INARY C LASSIFICATION problem. Can I turn that into
a good algorithm for solving the α -W EIGHTED B INARY C LASSIFICATION
problem?
    In order to do this, you need to define a transformation that maps
a concrete weighted problem into a concrete unweighted problem.
This transformation needs to happen both at training time and at test
time (though it need not be the same transformation!). Algorithm ??
sketches a training-time sub-sampling transformation and Algo-
rithm ?? sketches a test-time transformation (which, in this case, is
trivial). All the training algorithm is doing is retaining all positive ex-
amples and a 1/α fraction of all negative examples. The algorithm is
explicitly turning the distribution over weighted examples into a (dif-
ferent) distribution over binary examples. A vanilla binary classifier
is trained on this induced distribution.
    Aside from the fact that this algorithm throws out a lot of data
(especially for large α), it does seem to be doing a reasonable thing.
In fact, from a reductions perspective, it is an optimal algorithm. You
can prove the following result:
                                                                                     beyond binary classification               73



Theorem 2 (Subsampling Optimality). Suppose the binary classifier
trained in Algorithm ?? achieves a binary error rate of e. Then the error rate
of the weighted predictor is equal to αe.

    This theorem states that if your binary classifier does well (on the
induced distribution), then the learned predictor will also do well
(on the original distribution). Thus, we have successfully converted
a weighted learning problem into a plain classification problem! The
fact that the error rate of the weighted predictor is exactly α times
more than that of the unweighted predictor is unavoidable: the error
metric on which it is evaluated is α times bigger!                                             Why is it unreasonable to expect
    The proof of this theorem is so straightforward that we will prove                         to be able to√achieve, for instance,
                                                                                           ?   an error of αe, or anything that is
it here. It simply involves some algebra on expected values.                                   sublinear in α?

Proof of Theorem ??. Let D w be the original distribution and let D b be
the induced distribution. Let f be the binary classifier trained on data
from D b that achieves a binary error rate of eb on that distribution.
We will compute the expected error ew of f on the weighted problem:
                   h                 i
   ew = E( x,y)∼D w αy=1 f ( x) 6= y
                        
                                                                                  (5.1)

      = ∑ ∑ D w ( x, y)αy=1 f ( x) 6= y
                                       
                                                                                  (5.2)
         x∈X y∈±1
                                                            1            
      =α ∑           D w ( x, +1) f ( x) 6= +1 + D w ( x, −1) f ( x) 6= −1
                                             
           x∈X
                                                             α
                                                                                  (5.3)
                                                                            
      =α ∑           D b ( x, +1) f ( x) 6= +1 + D b ( x, −1) f ( x) 6= −1
                                                          
                                                                                  (5.4)
           x∈X
                               
      = αE(x,y)∼D b f ( x) 6= y                                                   (5.5)
      = αeb                                                                       (5.6)

And we’re done! (We implicitly assumed X is discrete. In the case
of continuous data, you need to replace all the sums over x with
integrals over x, but the result still holds.)

   Instead of subsampling the low-cost class, you could alternatively
oversample the high-cost class. The easiest case is when α is an in-
teger, say 5. Now, whenever you get a positive point, you include 5
copies of it in the induced distribution. Whenever you get a negative
point, you include a single copy.                                                              How can you handle non-integral α,
   This oversampling algorithm achieves exactly the same theoretical                       ? for instance 5.5?
result as the subsampling algorithm. The main advantage to the over-
sampling algorithm is that it does not throw out any data. The main
advantage to the subsampling algorithm is that it is more computa-
tionally efficient.



                                                                                               Modify the proof of optimality
                                                                                               for the subsampling algorithm so
                                                                                           ?   that it applies to the oversampling
                                                                                               algorithm.
      74     a course in machine learning



          You might be asking yourself: intuitively, the oversampling algo-
      rithm seems like a much better idea than the subsampling algorithm,
      at least if you don’t care about computational efficiency. But the the-
      ory tells us that they are the same! What is going on? Of course the
      theory isn’t wrong. It’s just that the assumptions are effectively dif-
      ferent in the two cases. Both theorems state that if you can get error
      of e on the binary problem, you automatically get error of αe on the
      weighted problem. But they do not say anything about how possible
      it is to get error e on the binary problem. Since the oversampling al-
      gorithm produces more data points than the subsampling algorithm
      it is very concievable that you could get lower binary error with over-
      sampling than subsampling.
          The primary drawback to oversampling is computational ineffi-
      ciency. However, for many learning algorithms, it is straightforward
      to include weighted copies of data points at no cost. The idea is to
      store only the unique data points and maintain a counter saying how
      many times they are replicated. This is not easy to do for the percep-
      tron (it can be done, but takes work), but it is easy for both decision
      trees and KNN. For example, for decision trees (recall Algorithm 1.3),
      the only changes are to: (1) ensure that line 1 computes the most fre-
      quent weighted answer, and (2) change lines 10 and 11 to compute
      weighted errors.                                                                      Why is it hard to change the per-
                                                                                         ? ceptron? (Hint: it has to do with the
                                                                                            fact that perceptron is online.)
5.2   Multiclass Classification

      Multiclass classification is a natural extension of binary classification.
      The goal is still to assign a discrete label to examples (for instance,
      is a document about entertainment, sports, finance or world news?).
      The difference is that you have K > 2 classes to choose from.
                                                                                            How would you modify KNN to
           TASK : M ULTICLASS C LASSIFICATION                                            ? take into account weights?
           Given:

           1. An input space X and number of classes K

           2. An unknown distribution D over X×[K ]

           Compute: A function f minimizing: E( x,y)∼D f ( x) 6= y
                                                                  




         Note that this is identical to binary classification, except for the
      presence of K classes. (In the above, [K ] = {1, 2, 3, . . . , K }.) In fact, if
      you set K = 2 you exactly recover binary classification.
         The game we play is the same: someone gives you a binary classi-
      fier and you have to use it to solve the multiclass classification prob-
                                                                                     beyond binary classification                 75



Algorithm 12 OneVersusAllTrain(Dmulticlass , BinaryTrain)
 1: for i = 1 to K do

 2:   Dbin ← relabel Dmulticlass so class i is positive and ¬i is negative
 3:    f i ← BinaryTrain(Dbin )
 4: end for

 5: return f 1 , . . . , f K




Algorithm 13 OneVersusAllTest( f 1 , . . . , f K , x̂)
 1: score ← h0, 0, . . . , 0i                     // initialize K -many scores to zero
 2: for i = 1 to K do
 3:    y ← f i (x̂)
 4:    scorei ← scorei + y
 5: end for

 6: return argmax scorek
                       k



lem. A very common approach is the one versus all technique (also
called OVA or one versus rest). To perform OVA, you train K-many
binary classifiers, f 1 , . . . , f K . Each classifier sees all of the training
data. Classifier f i receives all examples labeled class i as positives
and all other examples as negatives. At test time, whichever classifier
predicts “positive” wins, with ties broken randomly.                                           Suppose that you have N data
   The training and test algorithms for OVA are sketched in Algo-                              points in K classes, evenly divided.
                                                                                               How long does it take to train an
rithms 5.2 and 5.2. In the testing procedure, the prediction of the ith
                                                                                           ?   OVA classifier, if the base binary
classifier is added to the overall score for class i. Thus, if the predic-                     classifier takes O( N ) time to train?
tion is positive, class i gets a vote; if the prdiction is negative, every-                    What if the base classifier takes
                                                                                               O( N 2 ) time?
one else (implicitly) gets a vote. (In fact, if your learning algorithm
can output a confidence, as discussed in Section ??, you can often do
better by using the confidence as y, rather than a simple ±1.)
   OVA is very natural, easy to implement, and quite natural. It also
works very well in practice, so long as you do a good job choosing
a good binary classification algorithm tuning its hyperparameters                              Why would using a confidence
well. Its weakness is that it can be somewhat brittle. Intuitively, it is                  ? help.
not particularly robust to errors in the underlying classifiers. If one
classifier makes a mistake, it eis possible that the entire prediction is
erroneous. In fact, it is entirely possible that none of the K classifiers
predicts positive (which is actually the worst-case scenario from a
theoretical perspective)! This is made explicit in the OVA error bound
below.

Theorem 3 (OVA Error Bound). Suppose the average binary error of the
K binary classifiers is e. Then the error rate of the OVA multiclass predictor
is at most (K − 1)e.

Proof of Theorem 3. The key question is erroneous predictions from
the binary classifiers lead to multiclass errors. We break it down into
false negatives (predicting -1 when the truth is +1) and false positives
76   a course in machine learning



(predicting +1 when the truth is -1).
   When a false negative occurs, then the testing procedure chooses
randomly between available options, which is all labels. This gives a
(K − 1)/K probability of multiclass error. Since only one binary error
is necessary to make this happen, the efficiency of this error mode is
[(K − 1)/K ]/1 = (K − 1)/K.
   Multiple false positives can occur simultaneously. Suppose there
are m false positives. If there is simultaneously a false negative, the
error is 1. In order for this to happen, there have to be m + 1 errors,
so the efficiency is 1/( M + 1). In the case that there is not a simulta-
neous false negative, the error probability is m/(m + 1). This requires
m errors, leading to an efficiency of 1/(m + 1).
   The worse case, therefore, is the false negative case, which gives an
efficiency of (K − 1)/K. Since we have K-many opportunities to err,
we multiply this by K and get a bound of (K − 1)e.

   The constants in this are relatively unimportant: the aspect that
matters is that this scales linearly in K. That is, as the number of
classes grows, so does your expected error.
   To develop alternative approaches, a useful way to think about
turning multiclass classification problems into binary classification
problems is to think of them like tournaments (football, soccer–aka
football, cricket, tennis, or whatever appeals to you). You have K
teams entering a tournament, but unfortunately the sport they are
playing only allows two to compete at a time. You want to set up a
way of pairing the teams and having them compete so that you can
figure out which team is best. In learning, the teams are now the
classes and you’re trying to figure out which class is best.1                    1
                                                                                  The sporting analogy breaks down
   One natural approach is to have every team compete against ev-                a bit for OVA: K games are played,
                                                                                 wherein each team will play simultane-
ery other team. The team that wins the majority of its matches is                ously against all other teams.
declared the winner. This is the all versus all (or AVA) approach
(sometimes called all pairs). The most natural way to think about it
is as training (K2 ) classifiers. Say f ij for 1 ≤ i < j ≤ k is the classifier
that pits class i against class j. This classifier receives all of the class i
examples as “positive” and all of the class j examples as “negative.”
When a test point arrives, it is run through all f ij classifiers. Every
time f ij predicts positive, class i gets a point; otherwise, class j gets a
point. After running all (K2 ) classifiers, the class with the most votes
wins.                                                                                    Suppose that you have N data
   The training and test algorithms for AVA are sketched in Algo-                        points in K classes, evenly divided.
                                                                                         How long does it take to train an
rithms 5.2 and 5.2. In theory, the AVA mapping is more complicated                       AVA classifier, if the base binary
than the weighted binary case. The result is stated below, but the                   ?   classifier takes O( N ) time to train?
proof is omitted.                                                                        What if the base classifier takes
                                                                                         O( N 2 ) time? How does this com-
                                                                                         pare to OVA?
Theorem 4 (AVA Error Bound). Suppose the average binary error of
                                                                                    beyond binary classification               77



Algorithm 14 AllVersusAllTrain(Dmulticlass , BinaryTrain)
  1: f ij ← ∅, ∀1 ≤ i < j ≤ K

  2: for i = 1 to K-1 do

  3:     Dpos ← all x ∈ Dmulticlass labeled i
  4:     for j = i+1 to K do
  5:       Dneg ← all x ∈ Dmulticlass labeled j
  6:       Dbin ← {( x, +1) : x ∈ Dpos } ∪ {( x, −1) : x ∈ Dneg }
  7:        f ij ← BinaryTrain(Dbin )
  8:     end for
  9: end for

 10: return all f ij s




Algorithm 15 AllVersusAllTest(all f i j , x̂)
 1: score ← h0, 0, . . . , 0i                    // initialize K -many scores to zero
 2: for i = 1 to K-1 do
 3:    for j = i+1 to K do
 4:       y ← f i j (x̂)
 5:       scorei ← scorei + y
 6:       score j ← score j - y
 7:    end for
 8: end for

 9: return argmax scorek
                         k



the (K2 ) binary classifiers is e. Then the error rate of the AVA multiclass
predictor is at most 2(K − 1)e.
                                                                                              The bound for AVA is 2(K − 1)e; the
    At this point, you might be wondering if it’s possible to do bet-                         bound for OVA is (K − 1)e. Does
ter than something linear in K. Fortunately, the answer is yes! The                       ?   this mean that OVA is necessarily
                                                                                              better than AVA? Why or why not?
solution, like so much in computer science, is divide and conquer.
The idea is to construct a binary tree of classifiers. The leaves of this
tree correspond to the K labels. Since there are only log2 K decisions
made to get from the root to a leaf, then there are only log2 K chances
to make an error.
    An example of a classification tree for K = 8 classes is shown in
Figure 5.2. At the root, you distinguish between classes {1, 2, 3, 4}
and classes {5, 6, 7, 8}. This means that you will train a binary clas-
sifier whose positive examples are all data points with multiclass
label {1, 2, 3, 4} and whose negative examples are all data points with
multiclass label {5, 6, 7, 8}. Based on what decision is made by this
classifier, you can walk down the appropriate path in the tree. When
K is not a powwr of 2, the tree will not be full. This classification tree
algorithm achieves the following bound.

Theorem 5 (Tree Error Bound). Suppose the average binary classifiers
error is e. Then the error rate of the tree classifier is at most dlog2 K e e.

Proof of Theorem 5. A multiclass error is made if any classifier on
                                                                                         Figure 5.1: data set on which OVA will
                                                                                         do terribly with linear classifiers
                                                                                              Consider the data in Figure 5.1 and
                                                                                              assume that you are using a percep-
                                                                                          ?   tron as the base classifier. How well
                                                                                              will OVA do on this data? What
                                                                                              about AVA?
      78   a course in machine learning



      the path from the root to the correct leaf makes an error. Each has
      probability e of making an error and the path consists of at most
      dlog2 K e binary decisions.

         One think to keep in mind with tree classifiers is that you have
      control over how the tree is defined. In OVA and AVA you have no
      say in what classification problems are created. In tree classifiers,
      the only thing that matters is that, at the root, half of the classes are
      considered positive and half are considered negative. You want to
      split the classes in such a way that this classification decision is as
      easy as possible. You can use whatever you happen to know about
      your classification problem to try to separate the classes out in a
      reasonable way.
         Can you do better than dlog2 K e e? It turns out the answer is yes,
      but the algorithms to do so are relatively complicated. You can actu-
      ally do as well as 2e using the idea of error-correcting tournaments.
      Moreover, you can prove a lower bound that states that the best you
      could possible do is e/2. This means that error-correcting tourna-
      ments are at most a factor of four worse than optimal.


5.3   Ranking

      You start a new web search company called Goohooing. Like other
      search engines, a user inputs a query and a set of documents is re-
      trieved. Your goal is to rank the resulting documents based on rel-
      evance to the query. The ranking problem is to take a collection of
      items and sort them according to some notion of preference. One of
      the trickiest parts of doing ranking through learning is to properly
      define the loss function. Toward the end of this section you will see a
      very general loss function, but before that let’s consider a few special
      cases.
         Continuing the web search example, you are given a collection of
      queries. For each query, you are also given a collection of documents,
      together with a desired ranking over those documents. In the follow-
      ing, we’ll assume that you have N-many queries and for each query
      you have M-many documents. (In practice, M will probably vary
      by query, but for ease we’ll consider the simplified case.) The goal is
      to train a binary classifier to predict a preference function. Given a
      query q and two documents di and d j , the classifier should predict
      whether di should be preferred to d j with respect to the query q.
         As in all the previous examples, there are two things we have to
      take care of: (1) how to train the classifier that predicts preferences;
      (2) how to turn the predicted preferences into a ranking. Unlike the
      previous examples, the second step is somewhat complicated in the
                                                                                      beyond binary classification   79



Algorithm 16 NaiveRankTrain(RankingData, BinaryTrain)
 1:  D←[]
 2:  for n = 1 to N do
  3:   for all i, j = 1 to M and i 6= j do
  4:      if i is prefered to j on query n then
  5:          D ← D ⊕ ( xnij , +1)
  6:      else if j is prefered to i on query n then
  7:          D ← D ⊕ ( xnij , −1)
  8:      end if
  9:   end for
 10: end for

 11: return BinaryTrain(D)




Algorithm 17 NaiveRankTest( f , x̂)
 1: score ← h0, 0, . . . , 0i                    // initialize M-many scores to zero
 2: for all i, j = 1 to M and i 6= j do
 3:    y ← f (x̂ij )                               // get predicted ranking of i and j
 4:    scorei ← scorei + y
 5:    score j ← score j - y
 6: end for

 7: return argsort(score)                              // return queries sorted by score




ranking case. This is because we need to predict an entire ranking of
a large number of documents, somehow assimilating the preference
function into an overall permutation.
   For notationally simplicity, let xnij denote the features associated
with comparing document i to document j on query n. Training is
fairly straightforward. For every n and every pair i 6= j, we will
create a binary classification example based on features xnij . This
example is positive if i is preferred to j in the true ranking. It is neg-
ative if j is preferred to i. (In some cases the true ranking will not
express a preference between two objects, in which case we exclude
the i, j and j, i pair from training.)
   Now, you might be tempted to evaluate the classification perfor-
mance of this binary classifier on its own. The problem with this
approach is that it’s impossible to tell—just by looking at its output
on one i, j pair—how good the overall ranking is. This is because
there is the intermediate step of turning these pairwise predictions
into a coherent ranking. What you need to do is measure how well
the ranking based on your predicted preferences compares to the true
ordering. Algorithms 5.3 and 5.3 show naive algorithms for training
and testing a ranking function.
   These algorithms actually work quite well in the case of bipartite
ranking problems. A bipartite ranking problem is one in which you
are only ever trying to predict a binary response, for instance “is this
80     a course in machine learning



document relevant or not?” but are being evaluated according to a
metric like AUC. This is essentially because the only goal in bipartite
problems to to ensure that all the relevant documents are ahead of
all the irrelevant documents. There is no notion that one relevant
document is more relevant than another.
   For non-bipartite ranking problems, you can do better. First, when
the preferences that you get at training time are more nuanced than
“relevant or not,” you can incorporate these preferences at training
time. Effectively, you want to give a higher weight to binary prob-
lems that are very different in terms of perference than others. Sec-
ond, rather than producing a list of scores and then calling an arbi-
trary sorting algorithm, you can actually use the preference function
as the sorting function inside your own implementation of quicksort.
   We can now formalize the problem. Define a ranking as a function
σ that maps the objects we are ranking (documents) to the desired
position in the list, 1, 2, . . . M. If σu < σv then u is preferred to v (i.e.,
appears earlier on the ranked document list). Given data with ob-
served rankings σ, our goal is to learn to predict rankings for new
objects, σ̂. We define Σ M as the set of all ranking functions over M
objects. We also wish to express the fact that making a mistake on
some pairs is worse than making a mistake on others. This will be
encoded in a cost function ω (omega), where ω (i, j) is the cost for ac-
cidentally putting something in position j when it should have gone
in position i. To be a valid cost function valid, ω must be (1) symmet-
ric, (2) monotonic and (3) satisfy the triangle inequality. Namely: (1)
ω (i, j) = ω ( j, i ); (2) if i < j < k or i > j > k then ω (i, j) ≤ ω (i, k);
(3) ω (i, j) + ω ( j, k) ≥ ω (i, k). With these definitions, we can properly
define the ranking problem.

     TASK : ω -R ANKING
     Given:

     1. An input space X

     2. An unknown distribution D over X×Σ M

     Compute: A function f : X → Σ M minimizing:
               "                               #
       E( x,σ)∼D   ∑ [σu < σv ] [σ̂v < σ̂u ] ω (σu , σv )              (5.7)
                   u6=v

     where σ̂ = f ( x)



  In this definition, the only complex aspect is the loss function 5.7.
This loss sums over all pairs of objects u and v. If the true ranking (σ)
                                                                                  beyond binary classification   81



Algorithm 18 RankTrain(Drank , ω, BinaryTrain)
 1: Dbin ← [ ]
 2: for all (x, σ) ∈ D
                      rank do

 3:   for all u 6= v do
 4:      y ← sign(σv - σu )                        // y is +1 if u is prefered to v
 5:      w ← ω(σu , σv )                    // w is the cost of misclassification
 6:      Dbin ← Dbin ⊕ (y, w, xuv )
 7:   end for
 8: end for

 9: return BinaryTrain(D
                              bin )




prefers u to v, but the predicted ranking (σ̂) prefers v to u, then you
incur a cost of ω (σu , σv ).
   Depending on the problem you care about, you can set ω to many
“standard” options. If ω (i, j) = 1 whenever i 6= j, then you achieve
the Kemeny distance measure, which simply counts the number of
pairwise misordered items. In many applications, you may only care
about getting the top K predictions correct. For instance, your web
search algorithm may only display K = 10 results to a user. In this
case, you can define:
              (
                1 if min{i, j} ≤ K and i 6= j
   ω (i, j) =                                                        (5.8)
                0 otherwise
In this case, only errors in the top K elements are penalized. Swap-
ping items 55 and 56 is irrelevant (for K < 55).
   Finally, in the bipartite ranking case, you can express the area
under the curve (AUC) metric as:
                               
                                              +           +
                     M
                    (2)         1 if i ≤ M and j > M
                               
   ω (i, j) = +                               +
                              × 1 if j ≤ M and i > M      +         (5.9)
              M ( M − M+ )     0 otherwise

Here, M is the total number of objects to be ranked and M+ is the
number that are actually “good.” (Hence, M − M+ is the number
that are actually “bad,” since this is a bipartite problem.) You are
only penalized if you rank a good item in position greater than M+
or if you rank a bad item in a position less than or equal to M+ .
   In order to solve this problem, you can follow a recipe similar to
the naive approach sketched earlier. At training time, the biggest
change is that you can weight each training example by how bad it
would be to mess it up. This change is depicted in Algorithm 5.3,
where the binary classiciation data has weights w provided for saying
how important a given example is. These weights are derived from
the cost function ω.
   At test time, instead of predicting scores and then sorting the list,
you essentially run the quicksort algorith, using f as a comparison
      82    a course in machine learning



      Algorithm 19 RankTest( f , x̂, obj)
       1:  if obj contains 0 or 1 elements then
       2:     return obj
        3: else

        4:    p ← randomly chosen object in obj                                    // pick pivot
        5:    left ← [ ]                              // elements that seem smaller than p
        6:    right ← [ ]                               // elements that seem larger than p
        7:    for all u ∈ obj \{p} do
        8:        ŷ ← f (xup )                 // what is the probability that u precedes p
        9:       if uniform random variable < ŷ then
       10:           left ← left ⊕ u
       11:       else
       12:           right ← right ⊕ u
       13:       end if
       14:    end for
       15:    left ← RankTest( f , x̂, left)                         // sort earlier elements
       16:    right ← RankTest( f , x̂, right)                          // sort later elements
       17:    return left ⊕ hpi ⊕ right
       18: end if




      function. At each step in Algorithm 5.3, a pivot p is chosen. Every
      other object u is compared to p using f . If f thinks u is better, then it
      is sorted on the left; otherwise it is sorted on the right. There is one
      major difference between this algorithmand quicksort: the compari-
      son function is allowed to be probabilistic. If f outputs probabilities,
      for instance it predicts that u has an 80% probability of being better
      than p, then it puts it on the left with 80% probability and on the
      right with 20% probability. (The pseudocode is written in such a way
      that even if f just predicts −1, +1, the algorithm still works.)
         This algorithm is better than the naive algorithm in at least two
      ways. First, it only makes O( M log2 M ) calls to f (in expectation),
      rather than O( M2 ) calls in the naive case. Second, it achieves a better
      error bound, shown below:

      Theorem 6 (Rank Error Bound). Suppose the average binary error of f
      is e. Then the ranking algorithm achieves a test error of at most 2e in the
      general case, and e in the bipartite case.


5.4   Collective Classification

      You are writing new software for a digital camera that does face
      identification. However, instead of simply finding a bounding box
      around faces in an image, you must predict where a face is at the
      pixel level. So your input is an image (say, 100×100 pixels: this is a
      really low resolution camera!) and your output is a set of 100×100
                                                                                                   Figure 5.3: example face finding image
      binary predictions about each pixel. You are given a large collection                        and pixel mask
                                                                          beyond binary classification              83



of training examples. An example input/output pair is shown in
Figure 5.3.
    Your first attempt might be to train a binary classifier to predict
whether pixel (i, j) is part of a face or not. You might feed in features
to this classifier about the RGB values of pixel (i, j) as well as pixels
in a window arround that. For instance, pixels in the region {(i +
k, j + l ) : k ∈ [−5, 5], l ∈ [−5, 5]}.
    You run your classifier and notice that it predicts weird things,
like what you see in Figure 5.4. You then realize that predicting each
pixel independently is a bad idea! If pixel (i, j) is part of a face, then
this significantly increases the chances that pixel (i + 1, j) is also part
of a face. (And similarly for other pixels.) This is a collective classifi-    Figure 5.4: bad pixel mask for previous
cation problem because you are trying to predict multiple, correlated          image
objects at the same time.                                                           Similar problems come up all the
    The most general way to formulate these problems is as (undi-                   time. Cast the following as collec-
                                                                                    tive classification problems: web
rected) graph prediction problems. Our input now takes the form                     page categorization; labeling words
of a graph, where the vertices are input/output pairs and the edges             ?   in a sentence as noun, verb, adjec-
represent the correlations among the putputs. (Note that edges do                   tive, etc.; finding genes in DNA
                                                                                    sequences; predicting the stock
not need to express correlations among the inputs: these can simply                 market.
be encoded on the nodes themselves.) For example, in the face identi-
fication case, each pixel would correspond to an vertex in the graph.
For the vertex that corresponds to pixel (5, 10), the input would be
whatever set of features we want about that pixel (including features
about neighboring pixels). There would be edges between that vertex
and (for instance) vertices (4, 10), (6, 10), (5, 9) and (5, 11). If we are
predicting one of K classes at each vertex, then we are given a graph
whose vertices are labeled by pairs ( x, k) ∈ X×[K ]. We will write
G(X×[K ]) to denote the set of all such graphs. A graph in this set is
denoted as G = (V, E) with vertices V and edges E. Our goal is a
function f that takes as input a graph from G(X ) and predicts a label
from [K ] for each of its vertices.                                                 Formulate the example problems
                                                                                ? above as graph prediction prob-
   TASK : C OLLECTIVE C LASSIFICATION                                               lems.

  Given:

  1. An input space X and number of classes K

  2. An unknown distribution D over G(X×[K ])

  Compute: A function f      :    G(X )     →     G([K ]) minimizing:
  E(V,E)∼D ∑v∈V ŷv 6= yv , where yv is the label associated
                         

  with vertex v in G and ŷv is the label predicted by f ( G ).



  In collective classification, you would like to be able to use the
84   a course in machine learning



labels of neighboring vertices to help predict the label of a given
vertex. For instance, you might want to add features to the predict of
a given vertex based on the labels of each neighbor. At training time,
this is easy: you get to see the true labels of each neighbor. However,
at test time, it is much more difficult: you are, yourself, predicting the
labels of each neighbor.
   This presents a chicken and egg problem. You are trying to predict
a collection of labels. But the prediction of each label depends on the
prediction of other labels. If you remember from before, a general so-
lution to this problem is iteration: you can begin with some guesses,
and then try to improve these guesses over time. 2                           2
                                                                              Alternatively, the fact that we’re using
   This is the idea of stacking for solving collective classification        a graph might scream to you “dynamic
                                                                             programming.” Rest assured that
(see Figure 5.5. You can train 5 classifiers. The first classifier just      you can do this too: skip forward to
predicts the value of each pixel independently, like in Figure 5.4.          Chapter 18 for lots more detail here!
This doesn’t use any of the graph structure at all. In the second level,
you can repeat the classification. However, you can use the outputs
from the first level as initial guesses of labels. In general, for the Kth
level in the stack, you can use the inputs (pixel values) as well as
the predictions for all of the K − 1 previous levels of the stack. This
means training K-many binary classifiers based on different feature
sets.
   The prediction technique for stacking is sketched in Algorithm 5.4.
This takes a list of K classifiers, corresponding to each level in the
stack, and an input graph G. The variable Ŷk,v stores the prediction
of classifier k on vertex v in the graph. You first predict every node
in the vertex using the first layer in the stack, and no neighboring
                                                                             Figure 5.5: a charicature of how stack-
information. For the rest of the layers, you add on features to each         ing works
node based on the predictions made by lower levels in the stack for
neighboring nodes (N (u) denotes the neighbors of u).
   The training procedure follows a similar scheme, sketched in Al-
gorithm 5.4. It largely follows the same schematic as the prediction
algorithm, but with training fed in. After the classifier for the k level
has been trained, it is used to predict labels on every node in the
graph. These labels are used by later levels in the stack, as features.
   One thing to be aware of is that MulticlassTrain could con-
ceivably overfit its training data. For example, it is possible that the
first layer might actually achieve 0% error, in which case there is no
reason to iterate. But at test time, it will probably not get 0% error,
so this is misleading. There are (at least) two ways to address this
issue. The first is to use cross-validation during training, and to use
the predictions obtained during cross-validation as the predictions
from StackTest. This is typically very safe, but somewhat expensive.
The alternative is to simply over-regularize your training algorithm.
In particular, instead of trying to find hyperparameters that get the
                                                                                                      beyond binary classification   85



      Algorithm 20 StackTrain(D cc , K, MulticlassTrain)
       1: Dmc ← [ ]                                            // our generated multiclass data
       2: Ŷk,n,v ← 0, ∀k ∈ [K ], n ∈ [ N ], v ∈ Gn        // initialize predictions for all levels
       3: for k = 1 to K do

       4:     for n = 1 to N do
       5:        for all v ∈ Gn do
       6:           ( x, y) ← features and label for node v
       7:           x ← x ⊕ Ŷl,n,u , ∀u ∈ N (u), ∀l ∈ [k − 1]             // add on features for
       8:                                       // neighboring nodes from lower levels in the stack
       9:             Dmc ← Dmc ⊕ (y, x)                                   // add to multiclass data
       10:        end for
       11:   end for
       12:    f k ← MulticlassTrain(Dbin )                                // train kth level classifier
       13:   for n = 1 to N do
       14:        Ŷk,n,v ← StackTest( f 1 , . . . , f k , Gn )   // predict using kth level classifier
       15:   end for
       16: end for

       17: return f 1 , . . . , f K                                            // return all classifiers


      Algorithm 21 StackTest( f 1 , . . . , f K , G)
       1: Ŷk,v ← 0, ∀k ∈ [K ], v ∈ G                     // initialize predictions for all levels
       2: for k = 1 to K do
       3:     for all v ∈ G do
       4:        x ← features for node v
       5:        x ← x ⊕ Ŷl,u , ∀u ∈ N (u), ∀l ∈ [k − 1]                 // add on features for
       6:                                       // neighboring nodes from lower levels in the stack
       7:       Ŷk,v ← f k (x)                                     // predict according to kth level
        8:   end for
        9: end for

       10: return {ŶK,v : v ∈ G }           // return predictions for every node from the last layer



      best development data performance, try to find hyperparameters that
      make your training performance approximately equal to your devel-
      opment performance. This will ensure that your predictions at the kth
      layer are indicative of how well the algorithm will actually do at test
      time.
         TODO: finish this discussion


5.5   Exercises

      Exercise 5.1. TODO. . .
                                                                       6 | L INEAR M ODELS

        The essence of mathematics is not to make simple things compli-          Learning Objectives:
        cated, but to make complicated things simple.  – Stanley Gudder          • Define and plot four surrogate loss
                                                                                   functions: squared loss, logistic loss,
                                                                                   exponential loss and hinge loss.
                                                                                 • Compare and contrast the optimiza-
                                                                                   tion of 0/1 loss and surrogate loss
                                                                                   functions.
                                                                                 • Solve the optimization problem
                                                                                   for squared loss with a quadratic
                                                                                   regularizer in closed form.
      In Chapter ??, you learned about the perceptron algorithm
                                                                                 • Implement and debug gradient
      for linear classification. This was both a model (linear classifier) and
                                                                                   descent and subgradient descent.
      algorithm (the perceptron update rule) in one. In this section, we
      will separate these two, and consider general ways for optimizing
      linear models. This will lead us into some aspects of optimization
      (aka mathematical programming), but not very far. At the end of
      this chapter, there are pointers to more literature on optimization for
      those who are interested.
         The basic idea of the perceptron is to run a particular algorithm
      until a linear separator is found. You might ask: are there better al-
      gorithms for finding such a linear separator? We will follow this idea
      and formulate a learning problem as an explicit optimization prob-
                                                                                 Dependencies:
      lem: find me a linear separator that is not too complicated. We will
      see that finding an “optimal” separator is actually computationally
      prohibitive, and so will need to “relax” the optimality requirement.
      This will lead us to a convex objective that combines a loss func-
      tion (how well are we doing on the training data?) and a regularizer
      (how complicated is our learned model?). This learning framework
      is known as both Tikhonov regularization and structural risk mini-
      mization.


6.1   The Optimization Framework for Linear Models

      You have already seen the perceptron as a way of finding a weight
      vector w and bias b that do a good job of separating positive train-
      ing examples from negative training examples. The perceptron is a
      model and algorithm in one. Here, we are interested in separating
      these issues. We will focus on linear models, like the perceptron.
      But we will think about other, more generic ways of finding good
      parameters of these models.
         The goal of the perceptron was to find a separating hyperplane
      for some training data set. For simplicity, you can ignore the issue
      of overfitting (but just for now!). Not all data sets are linearly sepa-
                                                                                           linear models            87



rable. In the case that your training data isn’t linearly separable, you
might want to find the hyperplane that makes the fewest errors on
the training data. We can write this down as a formal mathematics
optimization problem as follows:


  min
   w,b
         ∑ 1[ y n ( w · x n + b ) > 0]                                (6.1)
          n


In this expression, you are optimizing over two variables, w and b.
The objective function is the thing you are trying to minimize. In
this case, the objective function is simply the error rate (or 0/1 loss) of
the linear classifier parameterized by w, b. In this expression, 1[·] is
the indicator function: it is one when (·) is true and zero otherwise.            You should remember the yw ·          x
   We know that the perceptron algorithm is guaranteed to find                    trick from the perceptron discus-
                                                                              ?   sion. If not, re-convince yourself
parameters for this model if the data is linearly separable. In other             that this is doing the right thing.
words, if the optimum of Eq (6.1) is zero, then the perceptron will
efficiently find parameters for this model. The notion of “efficiency”
depends on the margin of the data for the perceptron.
   You might ask: what happens if the data is not linearly separable?
Is there an efficient algorithm for finding an optimal setting of the
parameters? Unfortunately, the answer is no. There is no polynomial
time algorithm for solving Eq (6.1), unless P=NP. In other words,
this problem is NP-hard. Sadly, the proof of this is quite complicated
and beyond the scope of this book, but it relies on a reduction from a
variant of satisfiability. The key idea is to turn a satisfiability problem
into an optimization problem where a clause is satisfied exactly when
the hyperplane correctly separates the data.
   You might then come back and say: okay, well I don’t really need
an exact solution. I’m willing to have a solution that makes one or
two more errors than it has to. Unfortunately, the situation is really
bad. Zero/one loss is NP-hard to even appproximately minimize. In
other words, there is no efficient algorithm for even finding a solution
that’s a small constant worse than optimal. (The best known constant
at this time is 418/415 ≈ 1.007.)
   However, before getting too disillusioned about this whole enter-
prise (remember: there’s an entire chapter about this framework, so
it must be going somewhere!), you should remember that optimizing
Eq (6.1) perhaps isn’t even what you want to do! In particular, all it
says is that you will get minimal training error. It says nothing about
what your test error will be like. In order to try to find a solution that
will generalize well to test data, you need to ensure that you do not
overfit the data. To do this, you can introduce a regularizer over the
parameters of the model. For now, we will be vague about what this
regularizer looks like, and simply call it an arbitrary function R(w, b).
      88    a course in machine learning



      This leads to the following, regularized objective:

           min
           w,b
                 ∑ 1[yn (w · xn + b) > 0] + λR(w, b)                       (6.2)
                 n

      In Eq (6.2), we are now trying to optimize a trade-off between a so-
      lution that gives low training error (the first term) and a solution
      that is “simple” (the second term). You can think of the maximum
      depth hyperparameter of a decision tree as a form of regularization
      for trees. Here, R is a form of regularization for hyperplanes. In this
      formulation, λ becomes a hyperparameter for the optimization.                     Assuming R does the “right thing,”
         The key remaining questions, given this formalism, are:                        what value(s) of λ will lead to over-
                                                                                    ?   fitting? What value(s) will lead to
      • How can we adjust the optimization problem so that there are                    underfitting?
        efficient algorithms for solving it?

      • What are good regularizers R(w, b) for hyperplanes?

      • Assuming we can adjust the optimization problem appropriately,
        what algorithms exist for efficiently solving this regularized opti-
        mization problem?
      We will address these three questions in the next sections.


6.2   Convex Surrogate Loss Functions

      You might ask: why is optimizing zero/one loss so hard? Intuitively,
      one reason is that small changes to w, b can have a large impact on
      the value of the objective function. For instance, if there is a positive
      training example with w, x · +b = −0.0000001, then adjusting b up-
      wards by 0.00000011 will decrease your error rate by 1. But adjusting
      it upwards by 0.00000009 will have no effect. This makes it really
      difficult to figure out good ways to adjust the parameters.
          To see this more clearly, it is useful to look at plots that relate
      margin to loss. Such a plot for zero/one loss is shown in Figure 6.1.
      In this plot, the horizontal axis measure the margin of a data point
      and the vertical axis measures the loss associated with that margin.
      For zero/one loss, the story is simple. If you get a positive margin         Figure 6.1: plot of zero/one versus
      (i.e., y(w · x + b) > 0) then you get a loss of zero. Otherwise you get      margin
      a loss of one. By thinking about this plot, you can see how changes
      to the parameters that change the margin just a little bit can have an
      enormous effect on the overall loss.
          You might decide that a reasonable way to address this problem is
      to replace the non-smooth zero/one loss with a smooth approxima-
      tion. With a bit of effort, you could probably concoct an “S”-shaped
      function like that shown in Figure 6.2. The benefit of using such an
      S-function is that it is smooth, and potentially easier to optimize. The
      difficulty is that it is not convex.




                                                                                   Figure 6.2: plot of zero/one versus
                                                                                   margin and an S version of it
                                                                                              linear models      89



    If you remember from calculus, a convex function is one that looks
like a happy face (^). (On the other hand, a concave function is one
that looks like a sad face (_); an easy mnemonic is that you can hide
under a concave function.) There are two equivalent definitions of
a concave function. The first is that it’s second derivative is always
non-negative. The second, more geometric, defition is that any chord
of the function lies above it. This is shown in Figure ??. There you
can see a convex function and a non-convex function, both with two
chords drawn in. In the case of the convex function, the chords lie
above the function. In the case of the non-convex function, there are
parts of the chord that lie below the function.
    Convex functions are nice because they are easy to minimize. Intu-
itively, if you drop a ball anywhere in a convex function, it will even-
tually get to the minimum. This is not true for non-convex functions.
For example, if you drop a ball on the very left end of the S-function
from Figure 6.2, it will not go anywhere.
    This leads to the idea of convex surrogate loss functions. Since
zero/one loss is hard to optimize, you want to optimize something
else, instead. Since convex functions are easy to optimize, we want
to approximate zero/one loss with a convex function. This approxi-
mating function will be called a surrogate loss. The surrogate losses
we construct will always be upper bounds on the true loss function:
this guarantees that if you minimize the surrogate loss, you are also
pushing down the real loss.
    There are four common surrogate loss function, each with their
own properties: hinge loss, logistic loss, exponential loss and
squared loss. These are shown in Figure 6.4 and defined below.
These are defined in terms of the true label y (which is just {−1, +1})
and the predicted value ŷ = w · x + b.
                                                                                Figure 6.4: surrogate loss fns
                               (0/1)
     Zero/one:             `           (y, ŷ) = 1[yŷ ≤ 0]             (6.3)
                               (hin)
         Hinge:            `      (y, ŷ) = max{0, 1 − yŷ}             (6.4)
                                              1
       Logistic:           `(log) (y, ŷ) =       log (1 + exp[−yŷ])   (6.5)
                                            log 2
  Exponential:            `(exp) (y, ŷ) = exp[−yŷ]                    (6.6)
                               (sqr)                        2
       Squared:            `           (y, ŷ) = (y − ŷ)               (6.7)

In the definition of logistic loss, the log1 2 term out front is there sim-
ply to ensure that `(log) (y, 0) = 1. This ensures, like all the other
surrogate loss functions, that logistic loss upper bounds the zero/one
loss. (In practice, people typically omit this constant since it does not
affect the optimization.)
   There are two big differences in these loss functions. The first
difference is how “upset” they get by erroneous predictions. In the
      90    a course in machine learning



      case of hinge loss and logistic loss, the growth of the function as ŷ
      goes negative is linear. For squared loss and exponential loss, it is
      super-linear. This means that exponential loss would rather get a few
      examples a little wrong than one example really wrong. The other
      difference is how they deal with very confident correct predictions.
      Once yŷ > 1, hinge loss does not care any more, but logistic and
      exponential still think you can do better. On the other hand, squared
      loss thinks it’s just as bad to predict +3 on a positive example as it is
      to predict −1 on a positive example.


6.3   Weight Regularization

      In our learning objective, Eq (??), we had a term correspond to the
      zero/one loss on the training data, plus a regularizer whose goal
      was to ensure that the learned function didn’t get too “crazy.” (Or,
      more formally, to ensure that the function did not overfit.) If you re-
      place to zero/one loss with a surrogate loss, you obtain the following
      objective:

           min
           w,b
                 ∑ `(yn , w · xn + b) + λR(w, b)                           (6.8)
                  n

      The question is: what should R(w, b) look like?
          From the discussion of surrogate loss function, we would like
      to ensure that R is convex. Otherwise, we will be back to the point
      where optimization becomes difficult. Beyond that, a common desire
      is that the components of the weight vector (i.e., the wd s) should be
      small (close to zero). This is a form of inductive bias.
          Why are small values of wd good? Or, more precisely, why do
      small values of wd correspond to simple functions? Suppose that we
      have an example x with label +1. We might believe that other ex-
      amples, x0 that are nearby x should also have label +1. For example,
      if I obtain x0 by taking x and changing the first component by some
      small value e and leaving the rest the same, you might think that the
      classification would be the same. If you do this, the difference be-
      tween ŷ and ŷ0 will be exactly ew1 . So if w1 is reasonably small, this
      is unlikely to have much of an effect on the classification decision. On
      the other hand, if w1 is large, this could have a large effect.
          Another way of saying the same thing is to look at the derivative
      of the predictions as a function of w1 . The derivative of w, x · +b with
      respect to w1 is:
           ∂w, x · +b   ∂ [ ∑d wd xd + b ]
                      =                    = x1                            (6.9)
             ∂w1               ∂w1
      Interpreting the derivative as the rate of change, we can see that
      the rate of change of the prediction function is proportional to the
                                                                                            linear models           91



individual weights. So if you want the function to change slowly, you
want to ensure that the weights stay small.
   One way to accomplish this is to simply useqthe norm of the
weight vector. Namely R(norm) (w, b) = ||w|| = ∑d w2d . This function
is convex and smooth, which makes it easy to minimize. In prac-
tice, it’s often easier to use the squared norm, namely R(sqr) (w, b) =
||w||2 = ∑d w2d because it removes the ugly square root term and
remains convex. An alternative to using the sum of squared weights
is to use the sum of absolute weights: R(abs) (w, b) = ∑d |wd |. Both of
these norms are convex.                                                            Why do we not regularize the bias
    In addition to small weights being good, you could argue that zero         ? term b?
weights are better. If a weight wd goes to zero, then this means that
feature d is not used at all in the classification decision. If there are a
large number of irrelevant features, you might want as many weights
to go to zero as possible. This suggests an alternative regularizer:
R(cnt) (w, b) = ∑d 1[ xd 6= 0].
    This line of thinking leads to the general concept of p-norms.
(Technically these are called ` p (or “ell p”) norms, but this notation            Why might you not want to use
clashes with the use of ` for “loss.”) This is a family of norms that all      ? R(cnt) as a regularizer?
have the same general flavor. We write ||w|| p to denote the p-norm of
w.
                             !1
                              p

  ||w|| p =   ∑ | wd |   p
                                                                     (6.10)
               d


You can check that the 2-norm exactly corresponds to the usual Eu-
clidean norm, and that the 1-norm corresponds to the “absolute”
regularizer described above.                                                       You can actually identify the R(cnt)
   When p-norms are used to regularize weight vectors, the interest-               regularizer with a p-norm as well.
                                                                               ?   Which value of p gives it to you?
ing aspect is how they trade-off multiple features. To see the behavior
                                                                                   (Hint: you may have to take a limit.)
of p-norms in two dimensions, we can plot their contour (or level-
set). Figure 6.5 shows the contours for the same p norms in two
dimensions. Each line denotes the two-dimensional vectors to which
this norm assignes a total value of 1. By changing the value of p, you
can interpolate between a square (the so-called “max norm”), down
to a circle (2-norm), diamond (1-norm) and pointy-star-shaped-thing
(p < 1 norm).
   In general, smaller values of p “prefer” sparser vectors. You can
see this by noticing that the contours of small p-norms “stretch”
out along the axes. It is for this reason that small p-norms tend to
yield weight vectors with many zero entries (aka sparse weight vec-
tors). Unfortunately, for p < 1 the norm becomes non-convex. As
you might guess, this means that the 1-norm is a popular choice for
sparsity-seeking applications.




                                                                              Figure 6.5: loss:norms2d: level sets of
                                                                              the same p-norms
                                                                                   The max norm corresponds to
                                                                               ? lim p→∞ . Why is this called the max
                                                                                   norm?
      92     a course in machine learning



            M ATH R EVIEW | G RADIENTS
           ... be sure to do enough to do the closed for squared error



                                                                                            Figure 6.6:
      Algorithm 22 GradientDescent(F , K, η1 , . . . )
       1: z(0) ← h0, 0, . . . , 0i               // initialize variable we are optimizing
       2: for k = 1 . . . K do

       3:    g (k) ← ∇z F |z(k-1)               // compute gradient at current location
       4:    z(k) ← z(k-1) − η (k) g (k)               // take a step down the gradient
       5: end for
                    (K)
       6: return z




6.4   Optimization with Gradient Descent

      Envision the following problem. You’re taking up a new hobby:
      blindfolded mountain climbing. Someone blindfolds you and drops
      you on the side of a mountain. Your goal is to get to the peak of the
      mountain as quickly as possible. All you can do is feel the mountain
      where you are standing, and take steps. How would you get to the
      top of the mountain? Perhaps you would feel to find out what direc-
      tion feels the most “upward” and take a step in that direction. If you
      do this repeatedly, you might hope to get the the top of the moun-
      tain. (Actually, if your friend promises always to drop you on purely
      concave mountains, you will eventually get to the peak!)
          The idea of gradient-based methods of optimization is exactly the
      same. Suppose you are trying to find the maximum of a function
      f ( x). The optimizer maintains a current estimate of the parameter of
      interest, x. At each step, it measures the gradient of the function it
      is trying optimize. This measurement occurs at the current location,
      x. Call the gradient g. It then takes a step in the direction of the
      gradient, where the size of the step is controlled by a parameter η
      (eta). The complete step is x ← x + ηg. This is the basic idea of
      gradient ascent.
          The opposite of gradient ascent is gradient descent. All of our
      learning problems will be framed as minimization problems (trying
      to reach the bottom of a ditch, rather than the top of a hill). There-
      fore, descent is the primary approach you will use. One of the major
      conditions for gradient ascent being able to find the true, global min-
      imum, of its objective function is convexity. Without convexity, all is
      lost.
          The gradient descent algorithm is sketched in Algorithm 6.4.
      The function takes as arguments the function F to be minimized,
      the number of iterations K to run and a sequence of learning rates
                                                                                          linear models              93



η1 , . . . , ηK . (This is to address the case that you might want to start
your mountain climbing taking large steps, but only take small steps
when you are close to the peak.)
    The only real work you need to do to apply a gradient descent
method is be able to compute derivatives. For concreteness, suppose
that you choose exponential loss as a loss function and the 2-norm as
a regularizer. Then, the regularized objective function is:
                                    λ
  L(w, b) = ∑ exp − yn (w · xn + b) + ||w||2
                 
                                                                     (6.11)
            n                        2

The only “strange” thing in this objective is that we have replaced
λ with λ2 . The reason for this change is just to make the gradients
cleaner. We can first compute derivatives with respect to b:

   ∂L   ∂                               ∂ λ
          ∑                                  ||w||2
                                  
      =       exp − yn (w · xn + b) +                                (6.12)
   ∂b   ∂b n                            ∂b 2
           ∂
      =∑
                                  
              exp − yn (w · xn + b) + 0                              (6.13)
        n ∂b
                                
             ∂
      =∑
                                                       
                − yn (w · xn + b) exp − yn (w · xn + b)              (6.14)
        n    ∂b
      = − ∑ yn exp − yn (w · xn + b)
                                    
                                                                     (6.15)
             n

Before proceeding, it is worth thinking about what this says. From a
practical perspective, the optimization will operate by updating b ←
b − η ∂∂bL . Consider positive examples: examples with yn = +1. We
would hope for these examples that the current prediction, w · xn + b,
is as large as possible. As this value tends toward ∞, the term in the
exp[] goes to zero. Thus, such points will not contribute to the step.
However, if the current prediction is small, then the exp[] term will
be positive and non-zero. This means that the bias term b will be
increased, which is exactly what you would want. Moreover, once all
points are very well classified, the derivative goes to zero.                    This considered the case of posi-
   Now that we have done the easy case, let’s do the gradient with            ? tive examples. What happens with
                                                                                 negative examples?
respect to w.
                                            λ
  ∇w L = ∇w ∑ exp − yn (w · xn + b) + ∇w ||w||2
                                    
                                                            (6.16)
            n                               2
       = ∑ (∇w − yn (w · xn + b)) exp − yn (w · xn + b) + λw
                                                      
            n
                                                                     (6.17)
         = − ∑ yn xn exp − yn (w · xn + b) + λw
                                             
                                                                     (6.18)
                 n

Now you can repeat the previous exercise. The update is of the form
w ← w − η ∇w L. For well classified points (ones that are tend
toward yn ∞), the gradient is near zero. For poorly classified points,
      94   a course in machine learning



      the gradient points in the direction −yn xn , so the update is of the
      form w ← w + cyn xn , where c is some constant. This is just like
      the perceptron update! Note that c is large for very poorly classified
      points and small for relatively well classified points.
         By looking at the part of the gradient related to the regularizer,
      the update says: w ← w − λw = (1 − λ)w. This has the effect of
      shrinking the weights toward zero. This is exactly what we expect the
      regulaizer to be doing!
         The success of gradient descent hinges on appropriate choices
      for the step size. Figure 6.7 shows what can happen with gradient
      descent with poorly chosen step sizes. If the step size is too big, you
      can accidentally step over the optimum and end up oscillating. If the
      step size is too small, it will take way too long to get to the optimum.
                                                                                         Figure 6.7: good and bad step sizes
      For a well-chosen step size, you can show that gradient descent will
      approach the optimal value at a fast rate. The notion of convergence
      here is that the objective value converges to the true minimum.

      Theorem 7 (Gradient Descent Convergence). Under suitable condi-
      tions1 , for an appropriately chosen constant step size (i.e., η1 = η2 , · · · =   1
                                                                                          Specifically the function to be opti-
      η), the convergence rate of gradient descent is O(1/k ). More specifi-             mized needs to be strongly convex.
                                                                                         This is true for all our problems, pro-
      cally, letting z∗ be the global minimum of F , we have: F (z(k) ) − F (z∗ ) ≤      vided λ > 0. For λ√= 0 the rate could
                     2
      2||z(0) −z∗ ||                                                                     be as bad as O(1/ k ).
           ηk )
                       .
                                                                                                 A naive reading of this theorem
         The proof of this theorem is a bit complicated because it makes                         seems to say that you should choose
      heavy use of some linear algebra. The key is to set the learning rate                  ?   huge values of η. It should be obvi-
      to 1/L, where L is the maximum curvature of the function that is                           ous that this cannot be right. What
                                                                                                 is missing?
      being optimized. The curvature is simply the “size” of the second
      derivative. Functions with high curvature have gradients that change
      quickly, which means that you need to take small steps to avoid
      overstepping the optimum.
         This convergence result suggests a simple approach to decid-
      ing when to stop optimizing: wait until the objective function stops
      changing by much. An alternative is to wait until the parameters stop
      changing by much. A final example is to do what you did for percep-
      tron: early stopping. Every iteration, you can check the performance
      of the current model on some held-out data, and stop optimizing
      when performance plateaus.


6.5   From Gradients to Subgradients

      As a good exercise, you should try deriving gradient descent update
      rules for the different loss functions and different regularizers you’ve
      learned about. However, if you do this, you might notice that hinge
      loss and the 1-norm regularizer are not differentiable everywhere! In
                                                                                          linear models        95



particular, the 1-norm is not differentiable around wd = 0, and the
hinge loss is not differentiable around yŷ = 1.
   The solution to this is to use subgradient optimization. One way
to think about subgradients is just to not think about it: you essen-
tially need to just ignore the fact that you forgot that your function
wasn’t differentiable, and just try to apply gradient descent anyway.
   To be more concrete, consider the hinge function f (z) = max{0, 1 −
z}. This function is differentiable for z > 1 and differentiable for
z < 1, but not differentiable at z = 1. You can derive this using
differentiation by parts:
                  (
     ∂         ∂      0         if z > 1
       f (z) =                                                      (6.19)
    ∂z         ∂z     1 − z if z < 1
               (
                   ∂
                  ∂z 0           if z > 1
             =                                                      (6.20)
                   ∂
                  ∂z ( 1 −  z )  if z<1
               (
                  0      if z ≥ 1
             =                                                      (6.21)
                  −1 if z < 1
   Thus, the derivative is zero for z < 1 and −1 for z > 1, matching
intuition from the Figure. At the non-differentiable point, z = 1,
we can use a subderivative: a generalization of derivatives to non-
differentiable functions. Intuitively, you can think of the derivative
of f at z as the tangent line. Namely, it is the line that touches f at
z that is always below f (for convex functions). The subderivative,          Figure 6.8: hinge loss with sub
denoted ∂ f , is the set of all such lines. At differentiable positions,
this set consists just of the actual derivative. At non-differentiable
positions, this contains all slopes that define lines that always lie
under the function and make contact at the operating point. This is
shown pictorally in Figure 6.8, where example subderivatives are
shown for the hinge loss function. In the particular case of hinge loss,
any value between 0 and −1 is a valid subderivative at z = 0. In fact,
the subderivative is always a closed set of the form [ a, b], where a and
b can be derived by looking at limits from the left and right.
   This gives you a way of computing derivative-like things for non-
differentiable functions. Take hinge loss as an example. For a given
example n, the subgradient of hinge loss can be computed as:
  ∂ w max{0, 1 − yn (w · xn + b)}                                  (6.22)
        (
            0                  if yn (w · xn + b) > 1
   = ∂w                                                            (6.23)
            yn (w · xn + b) otherwise
      (
        ∂w0                   if yn (w · xn + b) > 1
   =                                                               (6.24)
        ∂ w yn (w · xn + b) otherwise
      (
        0       if yn (w · xn + b) > 1
   =                                                               (6.25)
        yn xn otherwise
      96     a course in machine learning



      Algorithm 23 HingeRegularizedGD(D, λ, MaxIter)
       1:  w ← h0, 0, . . . 0i , b ← 0                    // initialize weights and bias
       2:  for iter = 1 . . . MaxIter do
        3:    g ← h0, 0, . . . 0i , g ← 0     // initialize gradient of weights and bias
        4:   for all (x,y) ∈ D do
        5:       if y(w · x + b) ≤ 1 then
        6:          g←g+yx                                   // update weight gradient
        7:          g←g+y                                       // update bias derivative
        8:       end if
        9:   end for
       10:    g ← g − λw                                  // add in regularization term
       11:    w ← w + ηg                                              // update weights
       12:    b ← b + ηg                                                   // update bias
       13: end for

       14: return w, b




            M ATH R EVIEW | M ATRIX MULTIPLICATION AND INVERSION
           ...


                                                                                            Figure 6.9:

         If you plug this subgradient form into Algorithm 6.4, you obtain
      Algorithm 6.5. This is the subgradient descent for regularized hinge
      loss (with a 2-norm regularizer).


6.6   Closed-form Optimization for Squared Loss

      Although gradient descent is a good, generic optimization algorithm,
      there are cases when you can do better. An example is the case of a
      2-norm regularizer and squared error loss function. For this, you can
      actually obtain a closed form solution for the optimal weights. How-
      ever, to obtain this, you need to rewrite the optimization problem in
      terms of matrix operations. For simplicity, we will only consider the
      unbiased version, but the extension is Exercise ??. This is precisely the
      linear regression setting.
         You can think of the training data as a large matrix X of size N×D,
      where Xn,d is the value of the dth feature on the nth example. You
      can think of the labels as a column (“tall”) vector Y of dimension N.
      Finally, you can think of the weights as a column vector w of size
      D. Thus, the matrix-vector product a = Xw has dimension N. In
      particular:

            an = [Xw]n = ∑ Xn,d wd                                               (6.26)
                          d

      This means, in particular, that a is actually the predictions of the
      model. Instead of calling this a, we will call it Ŷ. The squared error
                                                                                              linear models              97



says that we should minimize 12 ∑n (Ŷn − Yn )2 , which can be written
                                                   2
in vector form as a minimization of 12 Ŷ − Y          .                             Verify that the squared error can
   This can be expanded visually as:                                             ? actually be written as this vector
                                                                                     norm.
                                                                      
    x1,1    x1,2 . . .     x1,D      w1       ∑d x1,d wd        y1
   x2,1    x2,2 . . .     x2,D   w2   ∑d x2,d wd   y2 
                                                            
                              ..   ..  =               ≈  .. 
   .         ..                                   ..
                    ..
                                                             
   .
   .          .       .       .  .             .       . 
    x N,1   x N,2 . . .    x N,D     wD       ∑d x N,d wd       yN
  |              {z              } | {z } |       {z      } | {z }
                 X                    w                    Ŷ           Ŷ
                                                                        (6.27)

So, compactly, our optimization problem can be written as:
                     1               λ
  min    L(w) =        ||Xw − Y ||2 + ||w||2                            (6.28)
    w                2               2
If you recall from calculus, you can minimize a function by setting its
derivative to zero. We start with the weights w and take gradients:

  ∇w L(w) = X> (Xw − Y ) + λw                                           (6.29)
                  >          >
            = X Xw − X Y + λw                                           (6.30)
                      
            = X> X + λI w − X> Y                                        (6.31)

We can equate this to zero and solve, yielding:
                   
          X> X + λI w − X> Y = 0                                        (6.32)
                     
  ⇐⇒ X> X + λI D w = X> Y                                               (6.33)
                         
  ⇐⇒ w = X> X + λI D −1 X> Y                                            (6.34)

Thus, the optimal solution of the weights can be computed by a few
matrix multiplications and a matrix inversion. As a sanity check,
you can make sure that the dimensions match. The matrix X> X has
dimension D×D, and therefore so does the inverse term. The inverse
is D×D and X> is D×N, so that product is D×N. Multiplying through
by the N×1 vector Y yields a D×1 vector, which is precisely what we
want for the weights.                                                                For those who are keen on linear
   Note that this gives an exact solution, modulo numerical innacu-                  algebra, you might be worried that
racies with computing matrix inverses. In contrast, gradient descent             ?   the matrix you must invert might
                                                                                     not be invertible. Is this actually a
will give you progressively better solutions and will “eventually”                   problem?
converge to the optimum at a rate of 1/k. This means that if you
want an answer that’s within an accuracy of e = 10−4 , you will need
something on the order of one thousand steps.
   The question is whether getting this exact solution is always more
efficient. To run gradient descent for one step will take O( ND ) time,
with a relatively small constant. You will have to run K iterations,
      98     a course in machine learning



      yielding an overall runtime of O(KND ). On the other hand, the
      closed form solution requires constructing X> X, which takes O( D2 N )
      time. The inversion take O( D3 ) time using standard matrix inver-
      sion routines. The final multiplications take O( ND ) time. Thus, the
      overall runtime is on the order O( D3 + D2 N ). In most standard cases
      (though this is becoming less true over time), N > D, so this is domi-
      nated by O( D2 N ).
         Thus, the overall question is whether you will need to run more
      than D-many iterations of gradient descent. If so, then the matrix
      inversion will be (roughly) faster. Otherwise, gradient descent will
      be (roughly) faster. For low- and medium-dimensional problems (say,
      D ≤ 100), it is probably faster to do the closed form solution via
      matrix inversion. For high dimensional problems (D ≥ 10, 000), it is
      probably faster to do gradient descent. For things in the middle, it’s
      hard to say for sure.


6.7   Support Vector Machines

      At the beginning of this chapter, you may have looked at the convex
      surrogate loss functions and asked yourself: where did these come
      from?! They are all derived from different underlying principles,
      which essentially correspond to different inductive biases.
         Let’s start by thinking back to the original goal of linear classifiers:
      to find a hyperplane that separates the positive training examples
      from the negative ones. Figure 6.10 shows some data and three po-
      tential hyperplanes: red, green and blue. Which one do you like best?
         Most likely you chose the green hyperplane. And most likely you            Figure 6.10: picture of data points with
      chose it because it was furthest away from the closest training points.       three hyperplanes, RGB with G the best
      In other words, it had a large margin. The desire for hyperplanes
      with large margins is a perfect example of an inductive bias. The data
      does not tell us which of the three hyperplanes is best: we have to
      choose one using some other source of information.
         Following this line of thinking leads us to the support vector ma-
      chine (SVM). This is simply a way of setting up an optimization
      problem that attempts to find a separating hyperplane with as large
      a margin as possible. It is written as a constrained optimization
      problem:

                        1
              min                                                          (6.35)
               w,b    γ(w, b)
           subj. to   yn (w · xn + b) ≥ 1                (∀n)

      In this optimization, you are trying to find parameters that maximize
      the margin, denoted γ, (i.e., minimize the reciprocal of the margin)
                                                                                          linear models              99



subject to the constraint that all training examples are correctly classi-
fied.
   The “odd” thing about this optimization problem is that we re-
quire the classification of each point to be greater than one rather than
simply greater than zero. However, the problem doesn’t fundamen-
tally change if you replace the “1” with any other positive constant
(see Exercise ??). As shown in Figure 6.11, the constant one can be
interpreted visually as ensuring that there is a non-trivial margin
between the positive points and negative points.
   The difficulty with the optimization problem in Eq (??) is what
happens with data that is not linearly separable. In that case, there
is no set of parameters w, b that can simultaneously satisfy all the         Figure 6.11: hyperplane with margins
constraints. In optimization terms, you would say that the feasible          on sides

region is empty. (The feasible region is simply the set of all parame-
ters that satify the constraints.) For this reason, this is refered to as
the hard-margin SVM, because enforcing the margin is a hard con-
straint. The question is: how to modify this optimization problem so
that it can handle inseparable data.
   The key idea is the use of slack parameters. The intuition behind
slack parameters is the following. Suppose we find a set of param-
eters w, b that do a really good job on 9999 data points. The points
are perfectly classifed and you achieve a large margin. But there’s
one pesky data point left that cannot be put on the proper side of the
margin: perhaps it is noisy. (See Figure 6.12.) You want to be able
to pretend that you can “move” that point across the hyperplane on           Figure 6.12: one bad point with slack
to the proper side. You will have to pay a little bit to do so, but as
long as you aren’t moving a lot of points around, it should be a good
idea to do this. In this picture, the amount that you move the point is
denoted ξ (xi).
   By introducing one slack parameter for each training example,
and penalizing yourself for having to use slack, you can create an
objective function like the following, soft-margin SVM:
                 1
      min              + C ∑ ξn                                     (6.36)
      w,b,ξ    γ(w, b)     n
               | {z }    | {z }
              large margin   small slack

  subj. to    yn (w · xn + b) ≥ 1 − ξ n              (∀n)
              ξn ≥ 0                                 (∀n)
The goal of this objective function is to ensure that all points are
correctly classified (the first constraint). But if a point n cannot be
correctly classified, then you can set the slack ξ n to something greater
than zero to “move” it in the correct direction. However, for all non-
zero slacks, you have to pay in the objective function proportional to
the amount of slack. The hyperparameter C > 0 controls overfitting
100   a course in machine learning



versus underfitting. The second constraint simply says that you must
not have negative slack.                                                        What values of C will lead to over-
   One major advantage of the soft-margin SVM over the original             ? fitting? What values will lead to
                                                                                underfitting?
hard-margin SVM is that the feasible region is never empty. That is,
there is always going to be some solution, regardless of whether your
training data is linearly separable or not.
   It’s one thing to write down an optimization problem. It’s another
thing to try to solve it. There are a very large number of ways to
optimize SVMs, essentially because they are such a popular learning
model. Here, we will talk just about one, very simple way. More
complex methods will be discussed later in this book once you have a            Suppose I give you a data set.
                                                                                Without even looking at the data,
bit more background.                                                            construct for me a feasible solution
   To make progress, you need to be able to measure the size of the         ?   to the soft-margin SVM. What is
                                                                                the value of the objective for this
margin. Suppose someone gives you parameters w, b that optimize                 solution?
the hard-margin SVM. We wish to measure the size of the margin.
The first observation is that the hyperplane will lie exactly halfway
between the nearest positive point and nearest negative point. If not,
the margin could be made bigger by simply sliding it one way or the
other by adjusting the bias b.
   By this observation, there is some positive example that that lies
exactly 1 unit from the hyperplane. Call it x+ , so that w · x+ + b = 1.
Similarly, there is some negative example, x− , that lies exactly on
the other side of the margin: for which w · x− + b = −1. These two
points, x+ and x− give us a way to measure the size of the margin.
As shown in Figure 6.11, we can measure the size of the margin by
looking at the difference between the lengths of projections of x+
and x− onto the hyperplane. Since projection requires a normalized
vector, we can measure the distances as:
         1
  d+ =       w · x+ + b − 1                                       (6.37)
       || ||
         w
           1
  d− = −       w · x− − b + 1                                     (6.38)   Figure 6.13: copy of figure from p5 of
         ||w||                                                             cs544 svm tutorial

We can then compute the margin by algebra:

        1 +
           d − d−
                  
  γ=                                                              (6.39)
        2                                        
        1     1      +               1      −
      =         w·x +b−1−                w·x −b+1                 (6.40)
        2 ||w||                    ||w||
                                      
        1     1              1
      =         w · x+ −        w · x−                            (6.41)
        2 ||w||           || ||
                            w
                                   
        1     1            1
      =         (+1) −         (−1)                               (6.42)
        2 ||w||         ||w||
          1
      =                                                           (6.43)
        ||w||
                                                                                 linear models   101



This is a remarkable conclusion: the size of the margin is inversely
proportional to the norm of the weight vector. Thus, maximizing the
margin is equivalent to minimizing ||w||! This serves as an addi-
tional justification of the 2-norm regularizer: having small weights
means having large margins!
   However, our goal wasn’t to justify the regularizer: it was to un-
derstand hinge loss. So let us go back to the soft-margin SVM and
plug in our new knowledge about margins:
                    1
         min          ||w||2 + C ∑ ξ n                                 (6.44)
         w,b,ξ      2
                    | {z }       n
                               | {z }
                   large margin
                                  small slack
  subj. to         yn (w · xn + b) ≥ 1 − ξ n               (∀n)
                   ξn ≥ 0                                  (∀n)
Now, let’s play a thought experiment. Suppose someone handed
you a solution to this optimization problem that consisted of weights
(w) and a bias (b), but they forgot to give you the slacks. Could you
recover the slacks from the information you have?
   In fact, the answer is yes! For simplicity, let’s consider positive
examples. Suppose that you look at some positive example xn . You
need to figure out what the slack, ξ n , would have been. There are two
cases. Either w · xn + b is at least 1 or it is not. If it’s large enough,
then you want to set ξ n = 0. Why? It cannot be less than zero by the
second constraint. Moreover, if you set it greater than zero, you will
“pay” unnecessarily in the objective. So in this case, ξ n = 0. Next,
suppose that w · xn + b = 0.2, so it is not big enough. In order to
satisfy the first constraint, you’ll need to set ξ n ≥ 0.8. But because
of the objective, you’ll not want to set it any larger than necessary, so
you’ll set ξ n = 0.8 exactly.
   Following this argument through for both positive and negative
points, if someone gives you solutions for w, b, you can automatically
compute the optimal ξ variables as:
         (
             0                    if yn (w · xn + b) ≥ 1
   ξn =                                                                 (6.45)
             1 − yn (w · xn + b) otherwise
In other words, the optimal value for a slack variable is exactly the
hinge loss on the corresponding example! Thus, we can write the
SVM objective as an unconstrained optimization problem:
                 1
  min              ||w||2 + C ∑ `(hin) (yn , w · xn + b)               (6.46)
   w,b           2
                 | {z }       n
                            |            {z            }
             large margin
                                       small slack
Multiplying this objective through by λ/C, we obtain exactly the reg-
ularized objective from Eq (6.8) with hinge loss as the loss function
and the 2-norm as the regularizer!
      102   a course in machine learning



        TODO: justify in term of one dimensional projections!


6.8   Exercises

      Exercise 6.1. TODO. . .
                                                  7 | P ROBABILISTIC M ODELING
                                                                              –        Learning Objectives:
                                                                                       • Define the generative story for a
                                                                                         naive Bayes classifier.
                                                                                       • Derive relative frequency as the so-
                                                                                         lution to a constrained optimization
                                                                                         problem.
                                                                                       • Compare and contrast generative,
                                                                                         conditional and discriminative
                                                                                         learning.
      Many of the models and algorithms you have learned about                         • Explain when generative models are
                                                                                         likely to fail.
      thus far are relatively disconnected. There is an alternative view of
                                                                                       • Derive logistic loss with an `2
      machine learning that unites and generalizes much of what you have
                                                                                         regularizer from a probabilistic
      already learned. This is the probabilistic modeling framework, in                  perspective.
      which you will explicitly think of learning as a problem of statistical
      inference.
         In this chapter, you will learn about two flavors of probabilistic
      models: generative and conditional. You will see that many of the ap-
      proaches (both supervised and unsupervised) we have seen already
      can be cast as probabilistic models. Through this new view, you will
      be able to develop learning algorithms that have inductive biases
      closer to what you, as a designer, believe. Moreover, the two chap-
                                                                                       Dependencies:
      ters that follow will make heavy use of the probabilistic modeling
      approach to open doors to other learning problems.


7.1   Classification by Density Estimation

      Our underlying assumption for the majority of this book is that
      learning problems are characterized by some unknown probability
      distribution D over input/output pairs ( x, y) ∈ X×Y . Suppose that
      someone told you what D was. In particular, they gave you a Python
      function computeD that took two inputs, x and y, and returned the
      probability of that x, y pair under D . If you had access to such a func-
      tion, classification becomes simple. We can define the Bayes optimal
      classifier as the classifier that, for any test input x̂, simply returns the
      ŷ that maximizes computeD ( x̂, ŷ), or, more formally:

         f (BO) ( x̂ ) = arg max D( x̂, ŷ)                                    (7.1)
                            ŷ∈Y

      This classifier is optimal in one specific sense: of all possible classifiers,
      it achieves the smallest zero/one error.

      Theorem 8 (Bayes Optimal Classifier). The Bayes Optimal Classifier
      f (BO) achieves minimal zero/one error of any deterministic classifier.
104   a course in machine learning



   M ATH R EVIEW | R ULES OF PROBABILITY
  chain rule, marginalization and Bayes’ rule



                                                                               Figure 7.1:

   This theorem assumes that you are comparing against deterministic
classifiers. You can actually prove a stronger result that f (BO) is opti-
mal for randomized classifiers as well, but the proof is a bit messier.
However, the intuition is the same: for a given x, f (BO) chooses the
label with highest probability, thus minimizing the probability that it
makes an error.

Proof of Theorem 8. Consider some other classifier g that claims to be
better than f . Then, there must be some x on which g( x ) 6= f ( x ).
Fix such an x. Now, the probability that f makes an error on this
particular x is 1 − D( x, f (BO) ( x )) and the probability that g makes an
error on this x is 1 − D( x, g( x )). But f (BO) was chosen in such a way
to maximize D( x, f (BO) ( x )), so this must be greater than D( x, g( x )).
Thus, the probability that f errs on this particular x is smaller than
the probability that g errs on it. This applies to any x for which
f ( x ) 6= g( x ) and therefore f achieves smaller zero/one error than
any g.

    The Bayes error rate (or Bayes optimal error rate) is the error rate
of the Bayes optimal classifier. It is the best error rate you can ever
hope to achieve on this classification problem (under zero/one loss).
    The take-home message is that if someone gave you access to
the data distribution, forming an optimal classifier would be trivial.
Unfortunately, no one gave you this distribution, but this analysis
suggests that good way to build a classifier is to try to estimate D . In
other words, you try to learn a distribution D̂ , which you hope to
very similar to D , and then use this distribution for classification. Just
as in the preceding chapters, you can try to form your estimate of D
based on a finite training set.
    The most direct way that you can attempt to construct such a
probability distribution is to select a family of parametric distribu-
tions. For instance, a Gaussian (or Normal) distribution is parametric:
it’s parameters are its mean and covariance. The job of learning is
then to infer which parameters are “best” as far as the observed train-
ing data is concerned, as well as whatever inductive bias you bring.
A key assumption that you will need to make is that the training data
you have access to is drawn independently from D . In particular, as
you draw examples ( x1 , y1 ) ∼ D then ( x2 , y2 ) ∼ D and so on, the
nth draw ( xn , yn ) is drawn from D and does not otherwise depend
                                                                                     probabilistic modeling              105



      on the previous n − 1 samples. This assumption is usually false, but
      is also usually sufficiently close to being true to be useful. Together
      with the assumption that all the training data is drawn from the same
      distribution D leads to the i.i.d. assumption or independently and
      identically distributed assumption. This is a key assumption in al-
      most all of machine learning.


7.2   Statistical Estimation

      Suppose you need to model a coin that is possibly biased (you can
      think of this as modeling the label in a binary classification problem),
      and that you observe data HHTH (where H means a flip came up heads
      and T means it came up tails). You can assume that all the flips came
      from the same coin, and that each flip was independent (hence, the
      data was i.i.d.). Further, you may choose to believe that the coin has
      a fixed probability β of coming up heads (and hence 1 − β of coming
      up tails). Thus, the parameter of your model is simply the scalar β.               Describe a case in which at least
         The most basic computation you might perform is maximum like-                ? one of the assumptions we are
                                                                                         making about the coin flip is false.
      lihood estimation: namely, select the paramter β the maximizes the
      probability of the data under that parameter. In order to do so, you
      need to compute the probability of the data:

         p β ( D ) = p β (HHTH)                           definition of D    (7.2)
                 = p β (H) p β (H) p β (T) p β (H)   data is independent     (7.3)
                 = ββ(1 − β) β                                               (7.4)
                      3
                 = β (1 − β )                                                (7.5)
                 = β3 − β4                                                   (7.6)

      Thus, if you want the parameter β that maximizes the probability of
      the data, you can take the derivative of β3 − β4 with respect to β, set
      it equal to zero and solve for β:

          ∂ h 3    i
             β − β4 = 3β2 − 4β3                                              (7.7)
         ∂β
                          4β3 = 3β2                                          (7.8)
                   ⇐⇒4β = 3                                                  (7.9)
                          3
                   ⇐⇒ β =                                                   (7.10)
                          4
      Thus, the maximum likelihood β is 0.75, which is probably what
      you would have selected by intuition. You can solve this problem
      more generally as follows. If you have H-many heads and T-many
      tails, the probability of your data sequence is β H (1 − β) T . You can
      try to take the derivative of this with respect to β and follow the
      same recipe, but all of the products make things difficult. A more
106       a course in machine learning



friendly solution is to work with the log likelihood or log proba-
bility instead. The log likelihood of this data sequence is H log β +
T log(1 − β). Differentiating with respect to β, you get H/β − T/(1 −
β). To solve, you obtain H/β = T/(1 − β) so H (1 − β) = Tβ.
Thus H − Hβ = Tβ and so H = ( H + T ) β, finally yeilding that
β = H/( H + T ) or, simply, the fraction of observed data that came up
heads. In this case, the maximum likelihood estimate is nothing but
the relative frequency of observing heads!                                          How do you know that the solution
    Now, suppose that instead of flipping a coin, you’re rolling a K-             ? of β = H/( H + T ) is actually a
                                                                                    maximum?
sided die (for instance, to pick the label for a multiclass classification
problem). You might model this by saying that there are parameters
θ1 , θ2 , . . . , θK specifying, respectively, the probabilities that any given
side comes up on a role. Since these are themselves probabilities,
each θk should be at least zero, and the sum of the θk s should be one.
Given a data set that consists of x1 rolls of 1, x2 rolls of 2 and so on,
                                           x
the probability of this data is ∏k θk k , yielding a log probability of
∑k xk log θk . If you pick some particular parameter, say θ3 , the deriva-
tive of this with respect to θ3 is x3 /θ3 , which you want to equate to
zero. This leads to. . . θ3 → ∞.
    This is obviously “wrong.” From the mathematical formulation,
                                                                           x
it’s correct: in fact, setting all of the θk s to ∞ does maximize ∏k θk k for
any (non-negative) xk s. The problem is that you need to constrain the
θs to sum to one. In particular, you have a constraint that ∑k θk = 1
that you forgot to enforce. A convenient way to enforce such con-
straints is through the technique of Lagrange multipliers. To make
this problem consistent with standard minimization problems, it is
convenient to minimize negative log probabilities, instead of maxi-
mizing log probabilities. Thus, the constrainted optimization problem
is:

          min      − ∑ xk log θk                                        (7.11)
           θ
                       k
   subj. to        ∑ θk − 1 = 0
                   k

The Lagrange multiplier approach involves adding a new variable λ
to the problem (called the Lagrange variable) corresponding to the
constraint, and to use that to move the constraint into the objective.
The result, in this case, is:
                                          !
   max min          − ∑ xk log θk − λ   ∑ θk − 1                        (7.12)
      λ        θ
                           k            k

Turning a constrained optimization problem into it’s corresponding
Lagrangian is straightforward. The mystical aspect is why it works.
In this case, the idea is as follows. Think of λ as an adversary: λ
                                                                                                          probabilistic modeling   107



      is trying to maximize this function (you’re trying to minimize it).
      If you pick some parameters θ that actually satisfy the constraint,
      then the green term in Eq (??) goes to zero, and therefore λ does not
      matter: the adversary cannot do anything. On the other hand, if the
      constraint is even slightly unsatisfied, then λ can tend toward +∞
      or −∞ to blow up the objective. So, in order to have a non-infinite
      objective value, the optimizer must find values of θ that satisfy the
      constraint.
          If we solve the inner optimization of Eq (??) by differentiating with
      respect to θ1 , we get x1 /θ1 = λ, yielding θ1 = x1 /λ. In general, the
      solution is θk = xk /λ. Remembering that the goal of λ is to enforce
      the sums-to-one constraint, we can set λ = ∑k xk and verify that
      this is a solution. Thus, our optimal θk = xk / ∑k xk , which again
      completely corresponds to intuition.


7.3   Naive Bayes Models

      Now, consider the binary classification problem. You are looking for
      a parameterized probability distribution that can describe the training
      data you have. To be concrete, your task might be to predict whether
      a movie review is positive or negative (label) based on what words
      (features) appear in that review. Thus, the probability for a single data
      point can be written as:

         pθ ((y, x)) = pθ (y, x1 , x2 , . . . , x D )                                            (7.13)

      The challenge in working with a probability distribution like Eq (7.13)
      is that it’s a distribution over a lot of variables. You can try to sim-
      plify it by applying the chain rule of probabilities:

         pθ ( x1 , x2 , . . . , x D , y) = pθ (y) pθ ( x1 | y) pθ ( x2 | y, x1 ) pθ ( x3 | y, x1 , x2 )
                                           · · · pθ ( x D | y, x1 , x2 , . . . , x D−1 )         (7.14)
                                     = pθ (y) ∏ pθ ( xd | y, x1 , . . . , xd−1 )                 (7.15)
                                                  d

      At this point, this equality is exact for any probability distribution.
      However, it might be difficult to craft a probability distribution for
      the 10000th feature, given the previous 9999. Even if you could, it
      might be difficult to accurately estimate it. At this point, you can
      make assumptions. A classic assumption, called the naive Bayes as-
      sumption, is that the features are independent, conditioned on the label.
      In the movie review example, this is saying that once you know that
      it’s a positive review, the probability that the word “excellent” appears
      is independent of whether “amazing” also appeared. (Note that
      this does not imply that these words are independent when you
108   a course in machine learning



don’t know the label—they most certainly are not.) Formally this
assumption states that:

  Assumption:           p( xd | y, xd0 ) = p( xd | y)      ,    ∀d 6= d0                 (7.16)

Under this assumption, you can simplify Eq (7.15) to:

   pθ ((y, x)) = pθ (y) ∏ pθ ( xd | y)               naive Bayes assumption              (7.17)
                             d

At this point, you can start parameterizing p. Suppose, for now,
that your labels are binary and your features are also binary. In this
case, you could model the label as a biased coin, with probability of
heads (e.g., positive review) given by θ0 . Then, for each label, you
can imagine having one (biased) coin for each feature. So if there are
D-many features, you’ll have 1 + 2D total coins: one for the label
(call it θ0 ) and one for each label/feature combination (call these θ+1
and as θ−1 ). In the movie review example, we might expect θ0 ≈ 0.4
(forty percent of movie reviews are positive) and also that θ+1 might
give high probability to words like “excellent” and “amazing” and
“good” and θ−1 might give high probability to words like “terrible”
and “boring” and “hate”. You can rewrite the probability of a single
example as follows, eventually leading to the log probability of the
entire data set:

   pθ ((y, x)) = pθ (y) ∏ pθ ( xd | y)                                                       naive Bayes assumption
                             d
                                                                                         (7.18)
                                                
                                                     ∏
                        [y=+1]                            [ x =1]
               =       θ0      (1 − θ0 )[y=−1]           θ(yd),d (1 − θ(y),d )[ xd =0]            model assumptions
                                                     d
                                                                                         (7.19)

   Solving for θ0 is identical to solving for the biased coin case from
before: it is just the relative frequency of positive labels in your data
(because θ0 doesn’t depend on x at all). For the other parameters,
you can repeat the same exercise as before for each of the 2D coins
independently. This yields:

               1
               N∑
       θ̂0 =       [ y n = +1]                                                           (7.20)
                 n
             ∑n [yn = +1 ∧ xn,d = 1]
  θ̂(+1),d =                                                                             (7.21)
                  ∑ n [ y n = +1]
             ∑n [yn = −1 ∧ xn,d = 1]
  θ̂(−1),d =                                                                             (7.22)
                  ∑ n [ y n = −1]
In the case that the features are not binary, you need to choose a dif-
ferent model for p( xd | y). The model we chose here is the Bernouilli
distribution, which is effectively a distribution over independent
                                                                                                              probabilistic modeling              109



         M ATH R EVIEW | C OMMON PROBABILITY DISTRIBUTIONS
        remove people about discrete, bernoulli, binomial, multinomial and gaussian distributions



                                                                                                               Figure 7.2:


      coin flips. For other types of data, other distributions become more
      appropriate. The die example from before corresponds to a discrete
      distribution. If the data is continuous, you might choose to use a
      Gaussian distribution (aka Normal distribution). The choice of dis-
      tribution is a form of inductive bias by which you can inject your
      knowledge of the problem into the learning algorithm.



7.4   Prediction

      Consider the predictions made by the naive Bayes model with Bernoulli
      features in Eq (7.18). You can better understand this model by con-
      sidering its decision boundary. In the case of probabilistic models,
      the decision boundary is the set of inputs for which the likelihood of
      y = +1 is precisely 50%. Or, in other words, the set of inputs x for
      which p(y = +1 | x)/p(y = −1 | x) = 1. In order to do this, the
      first thing to notice is that p(y | x) = p(y, x)/p( x). In the ratio, the
      p( x) terms cancel, leaving p(y = +1, x)/p(y = −1, x). Instead of
      computing this ratio, it is easier to compute the log-likelihood ratio
      (or LLR), log p(y = +1, x) − log p(y = −1, x), computed below:

                      "                                              #         "                                                   #
        LLR = log θ0 ∏                                                   − log (1 − θ0 ) ∏
                                    [ x =1]                                                       [ x =1]
                                   θ(+d1),d (1 − θ(+1),d )[ xd =0]                               θ(−d1),d (1 − θ(−1),d )[ xd =0]           model assumptions
                              d                                                              d
                                                                                                 (7.23)
                                                                                                  
             = log θ0 − log(1 − θ0 ) + ∑[ xd = 1] log θ(+1),d − log θ(−1),d
                                                   d
                                                                                       
                     + ∑[ xd = 0] log(1 − θ(+1),d ) − log(1 − θ(−1),d )                                                                take logs and rearrange
                          d
                                                                                                 (7.24)
                                  θ(+1),d                       1 − θ(+1),d                  θ0
             = ∑ xd log                     + ∑(1 − xd ) log                       + log                                                     simplify log terms
                 d
                                  θ(−1),d      d
                                                                1 − θ(−1),d                1 − θ0
                                                                                                 (7.25)
                          "                                          #
                                   θ(+1),d           1 − θ(+1),d                   1 − θ(+1),d              θ0
             = ∑ xd log                      − log                       + ∑ log                  + log                                         group x-terms
                 d
                                   θ(−1),d           1 − θ(−1),d           d
                                                                                   1 − θ(−1),d            1 − θ0
                                                                                                 (7.26)
             = x·w+b                                                                             (7.27)
      110    a course in machine learning



                       θ(+1),d (1 − θ(−1),d )                          1 − θ(+1),d             θ0
            wd = log                               ,   b = ∑ log                     + log
                       θ(−1),d (1 − θ(+1),d )                  d
                                                                       1 − θ(−1),d           1 − θ0
                                                                                         (7.28)

      The result of the algebra is that the naive Bayes model has precisely
      the form of a linear model! Thus, like perceptron and many of the
      other models you’ve previous studied, the decision boundary is
      linear.
         TODO: MBR


7.5   Generative Stories

      A useful way to develop probabilistic models is to tell a generative
      story. This is a fictional story that explains how you believe your
      training data came into existence. To make things interesting, con-
      sider a multiclass classification problem, with continuous features
      modeled by independent Gaussians. Since the label can take values
      1 . . . K, you can use a discrete distribution (die roll) to model it (as
      opposed to the Bernoilli distribution from before):

      1. For each example n = 1 . . . N:

        (a) Choose a label yn ∼ Disc(θ)
        (b) For each feature d = 1 . . . D:
             i. Choose feature value xn,d ∼ Nor(µyn ,d , σy2n ,d )

      This generative story can be directly translated into a likelihood
      function by replacing the “for each”s with products:

                                                for each example
                   z                                   }| "          #{
                                           1              1
         p ( D ) = ∏ θyn          ∏ q 2 exp − 2σ2 (xn,d − µyn ,d )2
                   n    |{z}       d   2πσyn ,d            yn ,d
                     choose label
                                     |                  {z            }
                                                choose feature value
                                  |                  {z               }
                                                         for each feature
                                                                                         (7.29)

      You can take logs to arrive at the log-likelihood:
                       "                                                 #
                                        1                1
        log p( D ) = ∑ log θyn + ∑ − log(σyn ,d ) − 2 ( xn,d − µyn ,d ) + const
                                                2                      2
                     n             d
                                        2              2σyn ,d
                                                                                         (7.30)

      To optimize for θ, you need to add a “sums to one” constraint as
      before. This leads to the previous solution where the θk s are propor-
      tional to the number of examples with label k. In the case of the µs
                                                                                                        probabilistic modeling               111



      you can take a derivative with respect to, say µk,i and obtain:

         ∂ log p( D )    ∂            1
                      =       − ∑ ∑ 2 ( xn,d − µyn ,d )2                      ignore irrelevant terms
             ∂µk,i      ∂µk,i   n d 2σy ,d         n

                                                                                         (7.31)
                            ∂            1
                       =        − ∑       2
                                              ( xn,i − µk,i )2                ignore irrelevant terms
                           ∂µk,i n:y =k 2σk,d
                                           n

                                                                                         (7.32)
                                   1
                       =    ∑     σ 2
                                       ( xn,i − µk,i )                                take derivative
                           n:yn =k k,d
                                                                                         (7.33)

      Setting this equal to zero and solving yields:

                 ∑n:yn =k xn,i
        µk,i =                                                                           (7.34)
                  ∑n:yn =k 1

      Namely, the sample mean of the ith feature of the data points that fall
                                          2 yields:
      in class k. A similar analysis for σk,i
                                    "                                    #
         ∂ log p( D )   ∂             1              1
               2
                      = 2 − ∑                2
                                        log(σk,i ) + 2 ( xn,i − µk,i ) 2
                                                                                            ignore irrelevant terms
             ∂σk,i     ∂σk,i y:yn =k 2              2σk,i
                                                                                         (7.35)
                                       "                                      #
                                            1         1
                       =−        ∑           2
                                           2σk,i
                                                 −    2 )2
                                                   2(σk,i
                                                           ( xn,i − µk,i )2                         take derivative
                             y:yn =k
                                                                                         (7.36)
                            1                  h                    i
                       =     4
                           2σk,i
                                   ∑ (xn,i − µk, i)2 − σk,i2                                                 simplify
                                 y:yn =k
                                                                                         (7.37)

      You can now set this equal to zero and solve, yielding:

         2       ∑n:yn =k ( xn,i − µk,i )2
        σk,i =                                                                           (7.38)
                       ∑n:yn =k 1

      Which is just the sample variance of feature i for class k.                                            What would the estimate be if you
                                                                                                             decided that, for a given class k, all
                                                                                                             features had equal variance? What
7.6   Conditional Models                                                                                 ?   if you assumed feature i had equal
                                                                                                             variance for each class? Under what
                                                                                                             circumstances might it be a good
      In the foregoing examples, the task was formulated as attempting to
                                                                                                             idea to make such assumptions?
      model the joint distribution of ( x, y) pairs. This may seem wasteful:
      at prediction time, all you care about is p(y | x), so why not model it
      directly?
         Starting with the case of regression is actually somewhat simpler
      than starting with classification in this case. Suppose you “believe”
112   a course in machine learning



that the relationship between the real value y and the vector x should
be linear. That is, you expect that y = w · x + b should hold for some
parameters (w, b). Of course, the data that you get does not exactly
obey this: that’s fine, you can think of deviations from y = w · x +
b as noise. To form a probabilistic model, you must assume some
distribution over noise; a convenient choice is zero-mean Gaussian
noise. This leads to the following generative story:

1. For each example n = 1 . . . N:

  (a) Compute tn = w · xn + b
  (b) Choose noise en ∼ Nor(0, σ2 )
  (c) Return yn = tn + en

In this story, the variable tn stands for “target.” It is the noiseless
variable that you do not get to observe. Similarly en is the error
(noise) on example n. The value that you actually get to observe is
yn = tn + en . See Figure 7.3.
   A basic property of the Gaussian distribution is additivity. Namely,
that if a ∼ Nor(µ, σ2 ) and b = a + c, then b ∼ Nor(µ + c, σ2 ). Given
this, from the generative story above, you can derive a shorter gener-
ative story:

1. For each example n = 1 . . . N:                                          Figure 7.3: pictorial view of targets
                                                                            versus labels
  (a) Choose yn ∼ Nor(w · xn + b, σ2 )

Reading off the log likelihood of a dataset from this generative story,
you obtain:
                                                        
                     1             1
  log p( D ) = ∑ − log(σ ) − 2 (w · xn + b − yn )
                            2                          2
                                                             model assumptions
               n     2           2σ
                                                                  (7.39)
                  1
                 2σ2 ∑
            =−         (w · xn + b − yn )2 + const             remove constants
                     n
                                                                  (7.40)

This is precisely the linear regression model you encountered in
Section 6.6! To minimizing the negative log probability, you need only
solve for the regression coefficients w, b as before.
   In the case of binary classification, using a Gaussian noise model
does not make sense. Switching to a Bernoulli model, which de-
scribes binary outcomes, makes more sense. The only remaining
difficulty is that the parameter of a Bernoulli is a value between zero
and one (the probability of “heads”) so your model must produce
such values. A classic approach is to produce a real-valued target, as
before, and then transform this target into a value between zero and
                                                                                          probabilistic modeling                113



      one, so that −∞ maps to 0 and +∞ maps to 1. A function that does
      this is the logistic function1 , defined below and plotted in Figure ??:            1
                                                                                           Also called the sigmoid function
                                                                                          because of it’s “S”-shape.

                                             1          exp z
        Logistic function:     σ(z) =               =                       (7.41)
                                        1 + exp[−z]   1 + exp z

      The logistic function has several nice properties that you can verify
      for yourself: σ (−z) = 1 − σ (z) and ∂σ/∂z = zσ2 (z).
         Using the logistic function, you can write down a generative story
      for binary classification:

      1. For each example n = 1 . . . N:

        (a) Compute tn = σ (w · xn + b)
        (b) Compute zn ∼ Ber(tn )
        (c) Return yn = 2zn − 1 (to make it ±1)                                           Figure 7.4: sketch of logistic function


        The log-likelihood for this model is:
                       h
        log p( D ) = ∑ [yn = +1] log σ (w · xn + b)
                      n
                                                            i
                          + [yn = −1] log σ (−w · xn + b)         model and properties of σ
                                                                            (7.42)
                   = ∑ log σ (yn (w · xn + b))                                    join terms
                      n
                                                                            (7.43)
                   = − ∑ log [1 + exp (−yn (w · xn + b))]                    definition of σ
                           n
                                                                            (7.44)
                   = − ∑ `(log) (yn , w · xn + b)                         definition of `(log)
                           n
                                                                            (7.45)

         As you can see, the log-likelihood is precisely the negative of (a
      scaled version of) the logistic loss from Chapter 6. This model is the
      logistic regression model, and this is where logisitic loss originally
      derived from.
         TODO: conditional versus joint


7.7   Regularization via Priors

      In the foregoing discussion, parameters of the model were selected
      according to the maximum likelihood criteria: find the parameters
      θ that maximize pθ ( D ). The trouble with this approach is easy to
      see even in a simple coin flipping example. If you flip a coin twice
      and it comes up heads both times, the maximum likelihood estimate
114   a course in machine learning



for the bias of the coin is 100%: it will always come up heads. This is
true even if you had only flipped it once! If course if you had flipped
it one million times and it had come up heads every time, then you
might find this to be a reasonable solution.
    This is clearly undesirable behavior, especially since data is expen-
sive in a machine learning setting. One solution (there are others!) is
to seek parameters that balance a tradeoff between the likelihood of
the data and some prior belief you have about what values of those
parameters are likely. Taking the case of the logistic regression, you
might a priori believe that small values of w are more likely than
large values, and choose to represent this as a Gaussian prior on each
component of w.
    The maximum a posteriori principle is a method for incoporat-
ing both data and prior beliefs to obtain a more balanced parameter
estimate. In abstract terms, consider a probabilistic model over data
D that is parameterized by parameters θ. If you think of the pa-
rameters as just another random variable, then you can write this
model as p( D | θ ), and maximum likelihood amounts to choosing θ
to maximize p( D | θ ). However, you might instead with to maximize
the probability of the parameters, given the data. Namely, maximize
p(θ | D ). This term is known as the posterior distribution on θ, and
can be computed by Bayes’ rule:

               prior likelihood
              z}|{ z }| {
              p(θ ) p( D | θ )
                                                       Z
  p(θ | D ) =                     , where   p( D ) =       dθ p(θ ) p( D | θ )
  | {z }           p( D )
   posterior       | {z }
                  evidence
                                                                             (7.46)

This reads: the posterior is equal to the prior times the likelihood di-
vided by the evidence.2 The evidence is a scary-looking term (it has                       2
                                                                                            The evidence is sometimes called the
an integral!) but note that from the perspective of seeking parameters                     marginal likelihood.

θ than maximize the posterior, the evidence is just a constant (it does
not depend on θ) and therefore can be ignored.
   Returning to the logistic regression example with Gaussian priors
on the weights, the log posterior looks like:

                                                      1 2
  log p(θ | D ) = − ∑ `(log) (yn , w · xn + b) − ∑       w + const
                                                        2 d
                                                                                 model definition
                      n                            d
                                                     2σ
                                                                             (7.47)
                                                    1
                = − ∑ `(log) (yn , w · xn + b) −       ||w||2                (7.48)
                      n                            2σ2

and therefore reduces to a regularized logistic function, with a
squared 2-norm regularizer on the weights. (A 1-norm regularizer
                                                                             probabilistic modeling   115



      can be obtained by using a Laplace prior on w rather than a Gaussian
      prior on w.)


7.8   Exercises

      Exercise 7.1. TODO. . .
                                                             8 | N EURAL N ETWORKS
                                                                          –       Learning Objectives:
                                                                                  • Explain the biological inspiration for
                                                                                    multi-layer neural networks.
                                                                                  • Construct a two-layer network that
                                                                                    can solve the XOR problem.
                                                                                  • Implement the back-propogation
                                                                                    algorithm for training multi-layer
                                                                                    networks.
                                                                                  • Explain the trade-off between depth
      The first learning models you learned about (decision trees                   and breadth in network structure.
      and nearest neighbor models) created complex, non-linear decision           • Contrast neural networks with ra-
                                                                                    dial basis functions with k-nearest
      boundaries. We moved from there to the perceptron, perhaps the
                                                                                    neighbor learning.
      most classic linear model. At this point, we will move back to non-
      linear learning models, but using all that we have learned about
      linear learning thus far.
         This chapter presents an extension of perceptron learning to non-
      linear decision boundaries, taking the biological inspiration of neu-
      rons even further. In the perceptron, we thought of the input data
      point (e.g., an image) as being directly connected to an output (e.g.,
      label). This is often called a single-layer network because there is one
      layer of weights. Now, instead of directly connecting the inputs to
                                                                                  Dependencies:
      the outputs, we will insert a layer of “hidden” nodes, moving from
      a single-layer network to a multi-layer network. But introducing
      a non-linearity at inner layers, this will give us non-linear decision
      boundaires. In fact, such networks are able to express almost any
      function we want, not just linear functions. The trade-off for this flex-
      ibility is increased complexity in parameter tuning and model design.


8.1   Bio-inspired Multi-Layer Networks

      One of the major weaknesses of linear models, like perceptron and
      the regularized linear models from the previous chapter, is that they
      are linear! Namely, they are unable to learn arbitrary decision bound-
      aries. In contrast, decision trees and KNN could learn arbitrarily
      complicated decision boundaries.
         One approach to doing this is to chain together a collection of
      perceptrons to build more complex neural networks. An example of
      a two-layer network is shown in Figure 8.1. Here, you can see five
      inputs (features) that are fed into two hidden units. These hidden
      units are then fed in to a single output unit. Each edge in this figure     Figure 8.1: picture of a two-layer
                                                                                  network with 5 inputs and two hidden
      corresponds to a different weight. (Even though it looks like there are
                                                                                  units
      three layers, this is called a two-layer network because we don’t count
                                                                                        neural networks                    117



the inputs as a real layer. That is, it’s two layers of trained weights.)
    Prediction with a neural network is a straightforward generaliza-
tion of prediction with a perceptron. First you compute activations
of the nodes in the hidden unit based on the inputs and the input
weights. Then you compute activations of the output unit given the
hidden unit activations and the second layer of weights.
    The only major difference between this computation and the per-
ceptron computation is that the hidden units compute a non-linear
function of their inputs. This is usually called the activation function
or link function. More formally, if wi,d is the weights on the edge
connecting input d to hidden unit i, then the activation of hidden unit
i is computed as:

  hi = f ( wi · x )                                                 (8.1)

Where f is the link function and wi refers to the vector of weights
feeding in to node i.
    One example link function is the sign function. That is, if the
incoming signal is negative, the activation is −1. Otherwise the
activation is +1. This is a potentially useful activiation function,
but you might already have guessed the problem with it: it is non-
differentiable.
    EXPLAIN BIAS!!!
    A more popular link function is the hyperbolic tangent function,
tanh. A comparison between the sign function and the tanh function
is in Figure 8.2. As you can see, it is a reasonable approximation
to the sign function, but is convenient in that it is differentiable.1
                                                                            Figure 8.2: picture of sign versus tanh
Because it looks like an “S” and because the Greek character for “S”        1
                                                                                It’s derivative is just 1 − tanh2 ( x ).
is “Sigma,” such functions are usually called sigmoid functions.
    Assuming for now that we are using tanh as the link function, the
overall prediction made by a two-layer network can be computed
using Algorithm 8.1. This function takes a matrix of weights W cor-
responding to the first layer weights and a vector of weights v corre-
sponding to the second layer. You can write this entire computation
out in one line as:

  ŷ = ∑ vi tanh(wi · x̂)                                           (8.2)
        i
    = v · tanh(Wx̂)                                                 (8.3)

Where the second line is short hand assuming that tanh can take a
vector as input and product a vector as output.                                     Is it necessary to use a link function
                                                                                    at all? What would happen if you
                                                                                ?   just used the identify function as a
                                                                                    link?
118   a course in machine learning



Algorithm 24 TwoLayerNetworkPredict(W, v, x̂)
 1: for i = 1 to number of hidden units do

 2:    hi ← tanh(wi · x̂)                  // compute activation of hidden unit i
 3: end for

 4: return v · h                                          // compute output unit




   The claim is that two-layer neural networks are more expressive                           y     x0     x1    x2
than single layer networks (i.e., perceptrons). To see this, you can                        +1     +1     +1    +1
construct a very small two-layer network for solving the XOR prob-                          +1     +1     -1    -1
lem. For simplicity, suppose that the data set consists of four data                        -1     +1     +1    -1
points, given in Table 8.1. The classification rule is that y = +1 if an                    -1     +1     -1    +1
only if x1 = x2 , where the features are just ±1.                                   Table 8.1: Small XOR data set.

   You can solve this problem using a two layer network with two
hidden units. The key idea is to make the first hidden unit compute
an “or” function: x1 ∨ x2 . The second hidden unit can compute an
“and” function: x1 ∧ x2 . The the output can combine these into a
single prediction that mimics XOR. Once you have the first hidden
unit activate for “or” and the second for “and,” you need only set the
output weights as −2 and +1, respectively.                                              Verify that these output weights
   To achieve the “or” behavior, you can start by setting the bias to                ? will actually give you XOR.
−0.5 and the weights for the two “real” features as both being 1. You
can check for yourself that this will do the “right thing” if the link
function were the sign function. Of course it’s not, it’s tanh. To get
tanh to mimic sign, you need to make the dot product either really
really large or really really small. You can accomplish this by set-
ting the bias to −500, 000 and both of the two weights to 1, 000, 000.
Now, the activation of this unit will be just slightly above −1 for
x = h−1, −1i and just slightly below +1 for the other three examples.                   This shows how to create an “or”
   At this point you’ve seen that one-layer networks (aka percep-                    ? function. How can you create an
                                                                                        “and” function?
trons) can represent any linear function and only linear functions.
You’ve also seen that two-layer networks can represent non-linear
functions like XOR. A natural question is: do you get additional
representational power by moving beyond two layers? The answer
is partially provided in the following Theorem, due originally to
George Cybenko for one particular type of link function, and ex-
tended later by Kurt Hornik to arbitrary link functions.

Theorem 9 (Two-Layer Networks are Universal Function Approxima-
tors). Let F be a continuous function on a bounded subset of D-dimensional
space. Then there exists a two-layer neural network F̂ with a finite number
of hidden units that approximate F arbitrarily well. Namely, for all x in the
domain of F, F ( x) − F̂ ( x) < e.

   Or, in colloquial terms “two-layer networks can approximate any
                                                                                 neural networks   119



      function.”
         This is a remarkable theorem. Practically, it says that if you give
      me a function F and some error tolerance parameter e, I can construct
      a two layer network that computes F. In a sense, it says that going
      from one layer to two layers completely changes the representational
      capacity of your model.
         When working with two-layer networks, the key question is: how
      many hidden units should I have? If your data is D dimensional
      and you have K hidden units, then the total number of parameters
      is ( D + 2)K. (The first +1 is from the bias, the second is from the
      second layer of weights.) Following on from the heuristic that you
      should have one to two examples for each parameter you are trying
      to estimate, this suggests a method for choosing the number of hid-
      den units as roughly b N D c. In other words, if you have tons and tons
      of examples, you can safely have lots of hidden units. If you only
      have a few examples, you should probably restrict the number of
      hidden units in your network.
         The number of units is both a form of inductive bias and a form
      of regularization. In both view, the number of hidden units controls
      how complex your function will be. Lots of hidden units ⇒ very
      complicated function. Figure ?? shows training and test error for
      neural networks trained with different numbers of hidden units. As
      the number increases, training performance continues to get better.
      But at some point, test performance gets worse because the network
      has overfit the data.


8.2   The Back-propagation Algorithm

      The back-propagation algorithm is a classic approach to training
      neural networks. Although it was not originally seen this way, based
      on what you know from the last chapter, you can summarize back-
      propagation as:

        back-propagation = gradient descent + chain rule                 (8.4)

      More specifically, the set up is exactly the same as before. You are
      going to optimize the weights in the network to minimize some ob-
      jective function. The only difference is that the predictor is no longer
      linear (i.e., ŷ = w · x + b) but now non-linear (i.e., v · tanh(Wx̂)).
      The only question is how to do gradient descent on this more compli-
      cated objective.
         For now, we will ignore the idea of regularization. This is for two
      reasons. The first is that you already know how to deal with regular-
      ization, so everything you’ve learned before applies. The second is
      that historically, neural networks have not been regularized. Instead,
120    a course in machine learning



people have used early stopping as a method for controlling overfit-
ting. Presently, it’s not obvious which is a better solution: both are
valid options.
   To be completely explicit, we will focus on optimizing squared
error. Again, this is mostly for historic reasons. You could easily
replace squared error with your loss function of choice. Our overall
objective is:
                                            !2
              1
   min ∑          y n − ∑ vi f ( wi · x n )                          (8.5)
   W,v     n 2          i
Here, f is some link function like tanh.
    The easy case is to differentiate this with respect to v: the weights
for the output unit. Without even doing any math, you should be
able to guess what this looks like. The way to think about it is that
from vs perspective, it is just a linear model, attempting to minimize
squared error. The only “funny” thing is that its inputs are the activa-
tions h rather than the examples x. So the gradient with respect to v
is just as for the linear case.
    To make things notationally more convenient, let en denote the
error on the nth example (i.e., the blue term above), and let hn denote
the vector of hidden unit activations on that example. Then:
  ∇v = − ∑ en hn                                                       (8.6)
               n
This is exactly like the linear case. One way of interpreting this is:
how would the output weights have to change to make the prediction
better? This is an easy question to answer because they can easily
measure how their changes affect the output.
   The more complicated aspect to deal with is the weights corre-
sponding to the first layer. The reason this is difficult is because the
weights in the first layer aren’t necessarily trying to produce specific
values, say 0 or 5 or −2.1. They are simply trying to produce acti-
vations that get fed to the output layer. So the change they want to
make depends crucially on how the output layer interprets them.
   Thankfully, the chain rule of calculus saves us. Ignoring the sum
over data points, we can compute:
                                       !2
             1
   L(W) =        y − ∑ vi f ( wi · x )                                 (8.7)
             2        i
      ∂L    ∂L ∂ f i
          =                                                            (8.8)
      ∂wi   ∂ f i ∂wi
                                            !
       ∂L
            =−      y − ∑ vi f ( wi · x )       vi = −evi              (8.9)
       ∂ fi                i
      ∂ fi
           = f 0 ( wi · x ) x                                         (8.10)
      ∂wi
                                                                                                      neural networks              121



Algorithm 25 TwoLayerNetworkTrain(D, η, K, MaxIter)
 1:  W ← D×K matrix of small random values                 // initialize input layer weights
 2:  v ← K-vector of small random values                 // initialize output layer weights
  3: for iter = 1 . . . MaxIter do

  4:   G ← D×K matrix of zeros                            // initialize input layer gradient
  5:    g ← K-vector of zeros                           // initialize output layer gradient
  6:   for all (x,y) ∈ D do
  7:       for i = 1 to K do
  8:          ai ← wi · x̂
  9:          hi ← tanh( ai )                    // compute activation of hidden unit i
 10:       end for
 11:       ŷ ← v · h                                                 // compute output unit
 12:       e ← y − ŷ                                                       // compute error
 13:       g ← g − eh                                // update gradient for output layer
 14:       for i = 1 to K do
 15:          Gi ← Gi − evi (1 − tanh2 ( ai )) x        // update gradient for input layer
 16:       end for
 17:   end for
 18:   W ← W − ηG                                            // update input layer weights
 19:    v ← v − ηg                                          // update output layer weights
 20: end for

 21: return W, v




Putting this together, we get that the gradient with respect to wi is:


      ∇wi = −evi f 0 (wi · x) x                                                     (8.11)


Intuitively you can make sense of this. If the overall error of the
predictor (e) is small, you want to make small steps. If vi is small
for hidden unit i, then this means that the output is not particularly
sensitive to the activation of the ith hidden unit. Thus, its gradient
should be small. If vi flips sign, the gradient at wi should also flip
signs. The name back-propagation comes from the fact that you
propagate gradients backward through the network, starting at the
end.
    The complete instantiation of gradient descent for a two layer
network with K hidden units is sketched in Algorithm 8.2. Note that
this really is exactly a gradient descent algorithm; the only different is
that the computation of the gradients of the input layer is moderately
complicated.                                                                                       What would happen to this algo-
    As a bit of practical advice, implementing the back-propagation                                rithm if you wanted to optimize
algorithm can be a bit tricky. Sign errors often abound. A useful trick                        ?   exponential loss instead of squared
                                                                                                   error? What if you wanted to add in
is first to keep W fixed and work on just training v. Then keep v                                  weight regularization?
fixed and work on training W. Then put them together.




                                                                                                   If you like matrix calculus, derive
                                                                                               ? the same algorithm starting from
                                                                                                   Eq (8.3).
      122   a course in machine learning



8.3   Initialization and Convergence of Neural Networks

      Based on what you know about linear models, you might be tempted
      to initialize all the weights in a neural network to zero. You might
      also have noticed that in Algorithm ??, this is not what’s done:
      they’re initialized to small random values. The question is why?
         The answer is because an initialization of W = 0 and v = 0 will
      lead to “uninteresting” solutions. In other words, if you initialize the
      model in this way, it will eventually get stuck in a bad local optimum.
      To see this, first realize that on any example x, the activation hi of the
      hidden units will all be zero since W = 0. This means that on the first
      iteration, the gradient on the output weights (v) will be zero, so they
      will stay put. Furthermore, the gradient w1,d for the dth feature on
      the ith unit will be exactly the same as the gradient w2,d for the same
      feature on the second unit. This means that the weight matrix, after
      a gradient step, will change in exactly the same way for every hidden
      unit. Thinking through this example for iterations 2 . . . , the values of
      the hidden units will always be exactly the same, which means that
      the weights feeding in to any of the hidden units will be exactly the
      same. Eventually the model will converge, but it will converge to a
      solution that does not take advantage of having access to the hidden
      units.
         This shows that neural networks are sensitive to their initialization.
      In particular, the function that they optimize is non-convex, meaning
      that it might have plentiful local optima. (One of which is the trivial
      local optimum described in the preceding paragraph.) In a sense,
      neural networks must have local optima. Suppose you have a two
      layer network with two hidden units that’s been optimized. You have
      weights w1 from inputs to the first hidden unit, weights w2 from in-
      puts to the second hidden unit and weights (v1 , v2 ) from the hidden
      units to the output. If I give you back another network with w1 and
      w2 swapped, and v1 and v2 swapped, the network computes exactly
      the same thing, but with a markedly different weight structure. This
      phenomena is known as symmetric modes (“mode” referring to an
      optima) meaning that there are symmetries in the weight space. It
      would be one thing if there were lots of modes and they were all
      symmetric: then finding one of them would be as good as finding
      any other. Unfortunately there are additional local optima that are
      not global optima.
         Random initialization of the weights of a network is a way to
      address both of these problems. By initializing a network with small
      random weights (say, uniform between −0.1 and 0.1), the network is
      unlikely to fall into the trivial, symmetric local optimum. Moreover,
      by training a collection of networks, each with a different random           Figure 8.3: convergence of randomly
                                                                                   initialized networks
                                                                                       neural networks            123



      initialization, you can often obtain better solutions that with just
      one initialization. In other words, you can train ten networks with
      different random seeds, and then pick the one that does best on held-
      out data. Figure 8.3 shows prototypical test-set performance for ten
      networks with different random initialization, plus an eleventh plot
      for the trivial symmetric network initialized with zeros.
         One of the typical complaints about neural networks is that they
      are finicky. In particular, they have a rather large number of knobs to
      tune:

      1. The number of layers

      2. The number of hidden units per layer

      3. The gradient descent learning rate η

      4. The initialization

      5. The stopping iteration or weight regularization

      The last of these is minor (early stopping is an easy regularization
      method that does not require much effort to tune), but the others
      are somewhat significant. Even for two layer networks, having to
      choose the number of hidden units, and then get the learning rate
      and initialization “right” can take a bit of work. Clearly it can be
      automated, but nonetheless it takes time.
         Another difficulty of neural networks is that their weights can
      be difficult to interpret. You’ve seen that, for linear networks, you
      can often interpret high weights as indicative of positive examples
      and low weights as indicative of negative examples. In multilayer
      networks, it becomes very difficult to try to understand what the
      different hidden units are doing.


8.4   Beyond Two Layers

      The definition of neural networks and the back-propagation algo-
      rithm can be generalized beyond two layers to any arbitrary directed
      acyclic graph. In practice, it is most common to use a layered net-
      work like that shown in Figure 8.4 unless one has a very strong rea-
      son (aka inductive bias) to do something different. However, the
      view as a directed graph sheds a different sort of insight on the back-   Figure 8.4: multi-layer network
      propagation algorithm.
         Suppose that your network structure is stored in some directed
      acyclic graph, like that in Figure 8.5. We index nodes in this graph
      as u, v. The activation before applying non-linearity at a node is au
      and after non-linearity is hu . The graph has a single sink, which is
      the output node y with activation ay (no non-linearity is performed




                                                                                Figure 8.5: DAG network
124   a course in machine learning



Algorithm 26 ForwardPropagation(x)
 1: for all input nodes u do

 2:    hu ← corresponding feature of x
 3: end for

 4: for all nodes v in the network whose parent’s are computed do

 5:    av ← ∑u∈par(v) w(u,v) hu
 6:    hv ← tanh( av )
 7: end for

 8: return a y




Algorithm 27 BackPropagation(x, y)
 1: run ForwardPropagation(x) to compute activations
 2: ey ← y − ay                                       // compute overall network error
 3: for all nodes v in the network whose error e v is computed do

 4:    for all u ∈ par(v) do
 5:       gu,v ← −ev hu                               // compute gradient of this edge
 6:       eu ← eu + ev wu,v (1 − tanh2 ( au )) // compute the “error” of the parent node
 7:    end for
 8: end for

 9: return all gradients ge




on the output unit). The graph has D-many inputs (i.e., nodes with
no parent), whose activations hu are given by an input example. An
edge (u, v) is from a parent to a child (i.e., from an input to a hidden
unit, or from a hidden unit to the sink). Each edge has a weight wu,v .
We say that par(u) is the set of parents of u.
   There are two relevant algorithms: forward-propagation and back-
propagation. Forward-propagation tells you how to compute the
activation of the sink y given the inputs. Back-propagation computes
derivatives of the edge weights for a given input.
   The key aspect of the forward-propagation algorithm is to iter-
atively compute activations, going deeper and deeper in the DAG.
Once the activations of all the parents of a node u have been com-
puted, you can compute the activation of node u. This is spelled out
in Algorithm 8.4. This is also explained pictorially in Figure 8.6.
                                                                                           Figure 8.6: picture of forward prop
   Back-propagation (see Algorithm 8.4) does the opposite: it com-
putes gradients top-down in the network. The key idea is to compute
an error for each node in the network. The error at the output unit is
the “true error.” For any input unit, the error is the amount of gradi-
ent that we see coming from our children (i.e., higher in the network).
These errors are computed backwards in the network (hence the
name back-propagation) along with the gradients themselves. This is
also explained pictorially in Figure 8.7.
   Given the back-propagation algorithm, you can directly run gradi-
ent descent, using it as a subroutine for computing the gradients.



                                                                                           Figure 8.7: picture of back prop
                                                                                            neural networks             125



8.5   Breadth versus Depth

      At this point, you’ve seen how to train two-layer networks and how
      to train arbitrary networks. You’ve also seen a theorem that says
      that two-layer networks are universal function approximators. This
      begs the question: if two-layer networks are so great, why do we care
      about deeper networks?
         To understand the answer, we can borrow some ideas from CS
      theory, namely the idea of circuit complexity. The goal is to show
      that there are functions for which it might be a “good idea” to use a
      deep network. In other words, there are functions that will require a
      huge number of hidden units if you force the network to be shallow,
      but can be done in a small number of units if you allow it to be deep.
      The example that we’ll use is the parity function which, ironically
      enough, is just a generalization of the XOR problem. The function is
      defined over binary inputs as:
        parity( x) = ∑ xd     mod 2                                        (8.12)
                      d
                      (
                          1   if the number of 1s in x is odd
                  =                                                        (8.13)
                          0   if the number of 1s in x is even
          It is easy to define a circuit of depth O(log2 D ) with O( D )-many
      gates for computing the parity function. Each gate is an XOR, ar-
      ranged in a complete binary tree, as shown in Figure 8.8. (If you
      want to disallow XOR as a gate, you can fix this by allowing the
      depth to be doubled and replacing each XOR with an AND, OR and
                                                                                    Figure 8.8: nnet:paritydeep: deep
      NOT combination, like you did at the beginning of this chapter.)              function for computing parity
          This shows that if you are allowed to be deep, you can construct a
      circuit with that computes parity using a number of hidden units that
      is linear in the dimensionality. So can you do the same with shallow
      circuits? The answer is no. It’s a famous result of circuit complexity
      that parity requires exponentially many gates to compute in constant
      depth. The formal theorem is below:
      Theorem 10 (Parity Function Complexity). Any circuit of depth K <
      log2 D that computes the parity function of D input bits must contain O e D
      gates.
         This is a very famous result because it shows that constant-depth
      circuits are less powerful that deep circuits. Although a neural net-
      work isn’t exactly the same as a circuit, the is generally believed that
      the same result holds for neural networks. At the very least, this
      gives a strong indication that depth might be an important considera-
      tion in neural networks.                                                           What is it about neural networks
         One way of thinking about the issue of breadth versus depth has                 that makes it so that the theorem
                                                                                     ?   about circuits does not apply di-
      to do with the number of parameters that need to be estimated. By                  rectly?
      126   a course in machine learning



      the heuristic that you need roughly one or two examples for every
      parameter, a deep model could potentially require exponentially
      fewer examples to train than a shallow model!
         This now flips the question: if deep is potentially so much better,
      why doesn’t everyone use deep networks? There are at least two
      answers. First, it makes the architecture selection problem more
      significant. Namely, when you use a two-layer network, the only
      hyperparameter to choose is how many hidden units should go in
      the middle layer. When you choose a deep network, you need to
      choose how many layers, and what is the width of all those layers.
      This can be somewhat daunting.
         A second issue has to do with training deep models with back-
      propagation. In general, as back-propagation works its way down
      through the model, the sizes of the gradients shrink. You can work
      this out mathematically, but the intuition is simpler. If you are the
      beginning of a very deep network, changing one single weight is
      unlikely to have a significant effect on the output, since it has to
      go through so many other units before getting there. This directly
      implies that the derivatives are small. This, in turn, means that back-
      propagation essentially never moves far from its initialization when
      run on very deep networks.                                                        While these small derivatives might
         Finding good ways to train deep networks is an active research                 make training difficult, they might
                                                                                    ?   be good for other reasons: what
      area. There are two general strategies. The first is to attempt to ini-           reasons?
      tialize the weights better, often by a layer-wise initialization strategy.
      This can be often done using unlabeled data. After this initializa-
      tion, back-propagation can be run to tweak the weights for whatever
      classification problem you care about. A second approach is to use a
      more complex optimization procedure, rather than gradient descent.
      You will learn about some such procedures later in this book.


8.6   Basis Functions

      At this point, we’ve seen that: (a) neural networks can mimic linear
      functions and (b) they can learn more complex functions. A rea-
      sonable question is whether they can mimic a KNN classifier, and
      whether they can do it efficiently (i.e., with not-too-many hidden
      units).
          A natural way to train a neural network to mimic a KNN classifier
      is to replace the sigmoid link function with a radial basis function
      (RBF). In a sigmoid network (i.e., a network with sigmoid links),
      the hidden units were computed as hi = tanh(wi , x·). In an RBF
      network, the hidden units are computed as:
                h                i
        hi = exp −γi ||wi − x||2                                           (8.14)
                                                                                            neural networks             127



         In other words, the hidden units behave like little Gaussian “bumps”
      centered around locations specified by the vectors wi . A one-dimensional
      example is shown in Figure 8.9. The parameter γi specifies the width
      of the Gaussian bump. If γi is large, then only data points that are
      really close to wi have non-zero activations. To distinguish sigmoid
      networks from RBF networks, the hidden units are typically drawn
      with sigmoids or with Gaussian bumps, as in Figure 8.10.
         Training RBF networks involves finding good values for the Gas-
      sian widths, γi , the centers of the Gaussian bumps, wi and the con-
      nections between the Gaussian bumps and the output unit, v. This
      can all be done using back-propagation. The gradient terms for v re-
      main unchanged from before, the the derivates for the other variables     Figure 8.9: nnet:rbfpicture: a one-D
                                                                                picture of RBF bumps
      differ (see Exercise ??).
         One of the big questions with RBF networks is: where should
      the Gaussian bumps be centered? One can, of course, apply back-
      propagation to attempt to find the centers. Another option is to spec-
      ify them ahead of time. For instance, one potential approach is to
      have one RBF unit per data point, centered on that data point. If you
      carefully choose the γs and vs, you can obtain something that looks
      nearly identical to distance-weighted KNN by doing so. This has the
      added advantage that you can go futher, and use back-propagation
      to learn good Gaussian widths (γ) and “voting” factors (v) for the
      nearest neighbor algorithm.
                                                                                    Figure 8.10: nnet:unitsymbols: picture
                                                                                    of nnet with sigmoid/rbf units
8.7   Exercises
                                                                                         Consider an RBF network with
                                                                                         one hidden unit per training point,
      Exercise 8.1. TODO. . .                                                            centered at that point. What bad
                                                                                     ?   thing might happen if you use back-
                                                                                         propagation to estimate the γs and
                                                                                         v on this data if you’re not careful?
                                                                                         How could you be careful?
                                                                 9 | K ERNEL M ETHODS
                                                                         –        Learning Objectives:
                                                                                  • Explain how kernels generalize
                                                                                    both feature combinations and basis
                                                                                    functions.
                                                                                  • Contrast dot products with kernel
                                                                                    products.
                                                                                  • Implement kernelized perceptron.
                                                                                  • Derive a kernelized version of
                                                                                    regularized least squares regression.
      Linear models are great because they are easy to understand                 • Implement a kernelized version of
      and easy to optimize. They suffer because they can only learn very            the perceptron.
      simple decision boundaries. Neural networks can learn more com-             • Derive the dual formulation of the
                                                                                    support vector machine.
      plex decision boundaries, but lose the nice convexity properties of
      many linear models.
         One way of getting a linear model to behave non-linearly is to
      transform the input. For instance, by adding feature pairs as addi-
      tional inputs. Learning a linear model on such a representation is
      convex, but is computationally prohibitive in all but very low dimen-
      sional spaces. You might ask: instead of explicitly expanding the fea-
      ture space, is it possible to stay with our original data representation
      and do all the feature blow up implicitly? Surprisingly, the answer is      Dependencies:
      often “yes” and the family of techniques that makes this possible are
      known as kernel approaches.


9.1   From Feature Combinations to Kernels

      In Section 4.4, you learned one method for increasing the expressive
      power of linear models: explode the feature space. For instance,
      a “quadratic” feature explosion might map a feature vector x =
      h x1 , x2 , x3 , . . . , x D i to an expanded version denoted φ( x):

        φ( x) = h1, 2x1 , 2x2 , 2x3 , . . . , 2x D ,
                   x12 , x1 x2 , x1 x3 , . . . , x1 x D ,
                   x2 x1 , x22 , x2 x3 , . . . , x2 x D ,
                   x3 x1 , x3 x2 , x32 , . . . , x2 x D ,
                    ...,
                   x D x1 , x D x2 , x D x3 , . . . , x2D i               (9.1)

      (Note that there are repetitions here, but hopefully most learning
      algorithms can deal well with redundant features; in particular, the
      2x1 terms are due to collapsing some repetitions.)
                                                                                                kernel methods   129



        You could then train a classifier on this expanded feature space.
     There are two primary concerns in doing so. The first is computa-
     tional: if your learning algorithm scales linearly in the number of fea-
     tures, then you’ve just squared the amount of computation you need
     to perform; you’ve also squared the amount of memory you’ll need.
     The second is statistical: if you go by the heuristic that you should
     have about two examples for every feature, then you will now need
     quadratically many training examples in order to avoid overfitting.
        This chapter is all about dealing with the computational issue. It
     will turn out in Chapter ?? that you can also deal with the statistical
     issue: for now, you can just hope that regularization will be sufficient
     to attenuate overfitting.
        The key insight in kernel-based learning is that you can rewrite
     many linear models in a way that doesn’t require you to ever ex-
     plicitly compute φ( x). To start with, you can think of this purely
     as a computational “trick” that enables you to use the power of a
     quadratic feature mapping without actually having to compute and
     store the mapped vectors. Later, you will see that it’s actually quite a
     bit deeper. Most algorithms we discuss involve a product of the form
     w · φ( x), after performing the feature mapping. The goal is to rewrite
     these algorithms so that they only ever depend on dot products be-
     tween two examples, say x and z; namely, they depend on φ( x) · φ(z).
     To understand why this is helpful, consider the quadratic expansion
     from above, and the dot-product between two vectors. You get:
       φ( x) · φ(z) = 1 + x1 z1 + x2 z2 + · · · + x D z D + x12 z21 + · · · + x1 x D z1 z D +
                       · · · + x D x1 z D z1 + x D x2 z D z2 + · · · + x2D z2D       (9.2)
                    = 1 + 2 ∑ xd zd + ∑ ∑ xd xe zd ze                                (9.3)
                               d             d   e

                    = 1 + 2x · z + ( x · z)2                                         (9.4)
                                     2
                    = (1 + x · z )                                                   (9.5)
     Thus, you can compute φ( x) · φ(z) in exactly the same amount of
     time as you can compute x · z (plus the time it takes to perform an
     addition and a multiply, about 0.02 nanoseconds on a circa 2011
     processor).
        The rest of the practical challenge is to rewrite your algorithms so
     that they only depend on dot products between examples and not on
     any explicit weight vectors.


9.2 Kernelized Perceptron

        Consider the original perceptron algorithm from Chapter 3, re-
     peated in Algorithm 9.2 using linear algebra notation and using fea-
     ture expansion notation φ( x). In this algorithm, there are two places
130   a course in machine learning



Algorithm 28 PerceptronTrain(D, MaxIter)
  1: w ← 0, b ← 0                           // initialize weights and bias
  2: for iter = 1 . . . MaxIter do

  3:   for all (x,y) ∈ D do
  4:       a ← w · φ( x) + b       // compute activation for this example
  5:       if ya ≤ 0 then
  6:          w ← w + y φ( x)                             // update weights
  7:          b←b+y                                            // update bias
  8:       end if
  9:   end for
 10: end for

 11: return w, b




   M ATH R EVIEW | S PANS AND NULL SPACES
  reminder: if U = {ui }i is a set of vectors in RD , then the span of U is the set of vectors that can be writ-
  ten as linear combinations of ui s; namely: span(U ) = {∑i ai ui : a1 ∈ R, . . . , a I ∈ R}.
  the null space of U is everything that’s left: RD \span(U ).
  TODO pictures



                                                                                Figure 9.1:

where φ( x) is used explicitly. The first is in computing the activation
(line 4) and the second is in updating the weights (line 6). The goal is
to remove the explicit dependence of this algorithm on φ and on the
weight vector.
    To do so, you can observe that at any point in the algorithm, the
weight vector w can be written as a linear combination of expanded
training data. In particular, at any point, w = ∑n αn φ( xn ) for some
parameters α. Initially, w = 0 so choosing α = 0 yields this. If the
first update occurs on the nth training example, then the resolution
weight vector is simply yn φ( xn ), which is equivalent to setting αn =
yn . If the second update occurs on the mth training example, then all
you need to do is update αm ← αm + ym . This is true, even if you
make multiple passes over the data. This observation leads to the
following representer theorem, which states that the weight vector of
the perceptron lies in the span of the training data.

Theorem 11 (Perceptron Representer Theorem). During a run of
the perceptron algorithm, the weight vector w is always in the span of the
(assumed non-empty) training data, φ( x1 ), . . . , φ( x N ).

Proof of Theorem 11. By induction. Base case: the span of any non-
empty set contains the zero vector, which is the initial weight vec-
tor. Inductive case: suppose that the theorem is true before the kth
update, and suppose that the kth update happens on example n.
By the inductive hypothesis, you can write w = ∑i αi φ( xi ) before
                                                                                              kernel methods   131



      Algorithm 29 KernelizedPerceptronTrain(D, MaxIter)
        1: α ← 0, b ← 0                                // initialize coefficients and bias
        2: for iter = 1 . . . MaxIter do

        3:   for all (xn ,yn ) ∈ D do
        4:       a ← ∑m αm φ( xm ) · φ( xn ) + b // compute activation for this example
        5:       if yn a ≤ 0 then
        6:          αn ← αn + yn                                     // update coefficients
        7:          b←b+y                                                    // update bias
        8:       end if
        9:   end for
       10: end for

       11: return α, b




      the update. The new weight vector is [∑i αi φ( xi )] + yn φ( xn ) =
      ∑i (αi + yn [i = n])φ( xi ), which is still in the span of the training
      data.

        Now that you know that you can always write w = ∑n αn φ( xn ) for
      some αi s, you can additionall compute the activations (line 4) as:
                                     !
         w · φ( x) + b =        ∑ αn φ( xn )   · φ( x) + b              definition of w
                                n
                                                                                     (9.6)
                                    h             i
                        = ∑ αn φ( xn ) · φ( x) + b             dot products are linear
                            n
                                                                                     (9.7)

      This now depends only on dot-products between data points, and
      never explicitly requires a weight vector. You can now rewrite the
      entire perceptron algorithm so that it never refers explicitly to the
      weights and only ever depends on pairwise dot products between
      examples. This is shown in Algorithm 9.2.
         The advantage to this “kernelized” algorithm is that you can per-
      form feature expansions like the quadratic feature expansion from
      the introduction for “free.” For example, for exactly the same cost as
      the quadratic features, you can use a cubic feature map, computed
      as φ(¨x)φ(z) = (1 + x · z)3 , which corresponds to three-way inter-
      actions between variables. (And, in general, you can do so for any
      polynomial degree p at the same computational complexity.)


9.3   Kernelized K-means

      For a complete change of pace, consider the K-means algorithm from
      Section ??. This algorithm is for clustering where there is no notion of
      “training labels.” Instead, you want to partition the data into coher-
      ent clusters. For data in RD , it involves randomly initializing K-many
      132   a course in machine learning



      cluster means µ(1) , . . . , µ(K) . The algorithm then alternates between the
      following two steps until convergence, with x replaced by φ( x) since
      that is the eventual goal:
                                                                                                 2
      1. For each example n, set cluster label zn = arg mink φ( xn ) − µ(k)                          .

      2. For each cluster k, update µ(k) = N1 ∑n:zn =k φ( xn ), where Nk is the
                                             k
         number of n with zn = k.

         The question is whether you can perform these steps without ex-
      plicitly computing φ( xn ). The representer theorem is more straight-
      forward here than in the perceptron. The mean of a set of data is,
      almost by definition, in the span of that data (choose the ai s all to be
      equal to 1/N). Thus, so long as you initialize the means in the span
      of the data, you are guaranteed always to have the means in the span
      of the data. Given this, you know that you can write each mean as an
      expansion of the data; say that µ(k) = ∑n α(k)
                                                  n φ ( xn ) for some parame-
      ters α(k)
             n  (there are N×K-many  such  parameters).
         Given this expansion, in order to execute step (1), you need to
      compute norms. This can be done as follows:
                                              2
         zn = arg min φ( xn ) − µ(k)                                                                            definition of zn
                    k

                                                                                            (9.8)
                                                      2
            = arg min φ( xn ) − ∑ α(k)
                                   m φ( xm )                                                                   definition of µ(k)
                    k                   m
                                                                                            (9.9)
                                                            2                 "               #
            = arg min ||φ( xn )||2 +
                    k
                                             ∑ α(k)
                                                m φ( xm )       + φ( xn ) ·       ∑ α(k)
                                                                                     m φ( xm )           expand quadratic term
                                             m                                    m
                                                                                           (9.10)
            = arg min ∑ ∑ αm αm0 φ( xm ) · φ( xm0 ) + ∑ αm φ( xm ) · φ( xn ) + const
                                   (k) (k)                           (k)
                                                                                                         linearity and constant
                    k       m m0                                m
                                                                                           (9.11)

      This computation can replace the assignments in step (1) of K-means.
      The mean updates are more direct in step (2):
                                             (
                                                 1
               1                                     if zn = k
          (k)
        µ =        ∑
              Nk n:z =k
                        φ( xn ) ⇐⇒ αn = (k)     Nk
                                                0    otherwise
                                                                     (9.12)
                        n



9.4   What Makes a Kernel

      A kernel is just a form of generalized dot product. You can also
      think of it as simply shorthand for φ( x) · φ(z), which is commonly
      written K φ ( x, z). Or, when φ is clear from context, simply K ( x, z).
                                                                                kernel methods   133



This is often refered to as the kernel product between x and z (under
the mapping φ).
    In this view, what you’ve seen in the preceding two sections is
that you can rewrite both the perceptron algorithm and the K-means
algorithm so that they only ever depend on kernel products between data
points, and never on the actual datapoints themselves. This is a very pow-
erful notion, as it has enabled the development of a large number of
non-linear algorithms essentially “for free” (by applying the so-called
kernel trick, that you’ve just seen twice).
    This raises an interesting question. If you have rewritten these
algorithms so that they only depend on the data through a function
K : X×X → R, can you stick any function K in these algorithms,
or are there some K that are “forbidden?” In one sense, you “could”
use any K, but the real question is: for what types of functions K do
these algorithms retain the properties that we expect them to have
(like convergence, optimality, etc.)?
    One way to answer this question is to say that K (·, ·) is a valid
kernel if it corresponds to the inner product between two vectors.
That is, K is valid if there exists a function φ such that K ( x, z) =
φ( x) · φ(z). This is a direct definition and it should be clear that if K
satisfies this, then the algorithms go through as expected (because
this is how we derived them).
    You’ve already seen the general class of polynomial kernels,
which have the form:
                                  d
    K(poly)
      d     ( x, z ) =   1 + x · z                                     (9.13)

where d is a hyperparameter of the kernel. These kernels correspond
to polynomial feature expansions.
   There is an alternative characterization of a valid kernel function
that is more mathematical. It states that K : X×X → R is a kernel if
K is positive semi-definite (or, in shorthand, psd). This property is
also sometimes called Mercer’s condition. In this context, this means
the for all functions f that are square integrable (i.e., f ( x)2 dx < ∞),
                                                         R

other than the zero function, the following property holds:
  ZZ
       f ( x)K ( x, z) f (z)dxdz > 0                                   (9.14)

This likely seems like it came out of nowhere. Unfortunately, the
connection is well beyond the scope of this book, but is covered well
is external sources. For now, simply take it as a given that this is an
equivalent requirement. (For those so inclined, the appendix of this
book gives a proof, but it requires a bit of knowledge of function
spaces to understand.)
   The question is: why is this alternative characterization useful? It
is useful because it gives you an alternative way to construct kernel
134     a course in machine learning



functions. For instance, using it you can easily prove the following,
which would be difficult from the definition of kernels as inner prod-
ucts after feature mappings.

Theorem 12 (Kernel Addition). If K1 and K2 are kernels, the K defined
by K ( x, z) = K1 ( x, z) + K2 ( x, z) is also a kernel.

Proof of Theorem 12. You need to verify the positive semi-definite
property on K. You can do this as follows:
   ZZ                                  ZZ
        f ( x)K ( x, z) f (z)dxdz =         f ( x) [K1 ( x, z) + K2 ( x, z)] f (z)dxdz        definition of K

                                                                                 (9.15)
                                       ZZ
                                   =        f ( x)K1 ( x, z) f (z)dxdz
                                           ZZ
                                       +        f ( x)K2 ( x, z) f (z)dxdz                   distributive rule

                                                                                 (9.16)
                                   > 0+0                                                  K1 and K2 are psd
                                                                                 (9.17)



   More generally, any positive linear combination of kernels is still a
kernel. Specifically, if K1 , . . . , K M are all kernels, and α1 , . . . , α M ≥ 0,
then K ( x, z) = ∑m αm Km ( x, z) is also a kernel.
   You can also use this property to show that the following Gaus-
sian kernel (also called the RBF kernel) is also psd:
                       h                   i
   K(RBF)
    γ     ( x, z) = exp −γ || x − z||2                                          (9.18)

Here γ is a hyperparameter that controls the width of this Gaussian-
like bumps. To gain an intuition for what the RBF kernel is doing,
consider what prediction looks like in the perceptron:

   f ( x) = ∑ αn K ( xn , x) + b                                                 (9.19)
             n
                   h                i
         = ∑ αn exp −γ || xn − z||2                                              (9.20)
             n

In this computation, each training example is getting to “vote” on the
label of the test point x. The amount of “vote” that the nth training
example gets is proportional to the negative exponential of the dis-
tance between the test point and itself. This is very much like an RBF
neural network, in which there is a Gaussian “bump” at each training
example, with variance 1/(2γ), and where the αn s act as the weights
connecting these RBF bumps to the output.
   Showing that this kernel is positive definite is a bit of an exercise
in analysis (particularly, integration by parts), but otherwise not
difficult. Again, the proof is provided in the appendix.
                                                                                       kernel methods   135



         So far, you have seen two bsaic classes of kernels: polynomial
      kernels (K ( x, vz) = (1 + x · z)d ), which includes the linear kernel
      (K ( x, z) = x · z) and RBF kernels (K ( x, z) = exp[−γ || x − z||2 ]). The
      former have a direct connection to feature expansion; the latter to
      RBF networks. You also know how to combine kernels to get new
      kernels by addition. In fact, you can do more than that: the product
      of two kernels is also a kernel.
         As far as a “library of kernels” goes, there are many. Polynomial
      and RBF are by far the most popular. A commonly used, but techni-
      cally invalid kernel, is the hyperbolic-tangent kernel, which mimics
      the behavior of a two-layer neural network. It is defined as:

         K(tanh) = tanh(1 + x · z)                  Warning: not psd         (9.21)

         A final example, which is not very common, but is nonetheless
      interesting, is the all-subsets kernel. Suppose that your D features
      are all binary: all take values 0 or 1. Let A ⊆ {1, 2, . . . D } be a subset
                                      V
      of features, and let f A ( x) = d∈ A xd be the conjunction of all the
      features in A. Let φ( x) be a feature vector over all such As, so that
      there are 2D features in the vector φ. You can compute the kernel
      associated with this feature mapping as:
                                      
         K(subs) ( x, z) = ∏ 1 + xd zd                                        (9.22)
                          d

      Verifying the relationship between this kernel and the all-subsets
      feature mapping is left as an exercise (but closely resembles the ex-
      pansion for the quadratic kernel).


9.5   Support Vector Machines

      Kernelization predated support vector machines, but SVMs are def-
      initely the model that popularized the idea. Recall the definition of
      the soft-margin SVM from Chapter 6.7 and in particular the opti-
      mization problem (6.36), which attempts to balance a large margin
      (small ||w||2 ) with a small loss (small ξ n s, where ξ n is the slack on
      the nth training example). This problem is repeated below:

                    1
            min       ||w||2 + C ∑ ξ n                                       (9.23)
            w,b,ξ   2            n
        subj. to    yn (w · xn + b) ≥ 1 − ξ n                 (∀n)
                    ξn ≥ 0                                    (∀n)

      Previously, you optimized this by explicitly computing the slack
      variables ξ n , given a solution to the decision boundary, w and b.
      However, you are now an expert with using Lagrange multipliers
136   a course in machine learning



to optimize constrained problems! The overall goal is going to be to
rewrite the SVM optimization problem in a way that it no longer ex-
plicitly depends on the weights w and only depends on the examples
xn through kernel products.
   There are 2N constraints in this optimization, one for each slack
constraint and one for the requirement that the slacks are non-
negative. Unlike the last time, these constraints are now inequalities,
which require a slightly different solution. First, you rewrite all the
inequalities so that they read as something ≥ 0 and then add cor-
responding Lagrange multipliers. The main difference is that the
Lagrange multipliers are now constrained to be non-negative, and
their sign in the augmented objective function matters.
   The second set of constraints is already in the proper form; the
first set can be rewritten as yn (w · xn + b) − 1 + ξ n ≥ 0. You’re now
ready to construct the Lagrangian, using multipliers αn for the first
set of constraints and β n for the second set.

                       1
  L(w, b, ξ, α, β) =     ||w||2 + C ∑ ξ n − ∑ β n ξ n             (9.24)
                       2            n       n
                        − ∑ αn [yn (w · xn + b) − 1 + ξ n ]       (9.25)
                            n

The new optimization problem is:

  min max max L(w, b, ξ, α, β)                                    (9.26)
  w,b,ξ α≥0 β≥0


The intuition is exactly the same as before. If you are able to find a
solution that satisfies the constraints (e.g., the purple term is prop-
erly non-negative), then the β n s cannot do anything to “hurt” the
solution. On the other hand, if the purple term is negative, then the
corresponding β n can go to +∞, breaking the solution.
   You can solve this problem by taking gradients. This is a bit te-
dious, but and important step to realize how everything fits together.
Since your goal is to remove the dependence on w, the first step is to
take a gradient with respect to w, set it equal to zero, and solve for w
in terms of the other variables.

  ∇w L = w − ∑ αn yn xn = 0         ⇐⇒      w = ∑ αn yn xn        (9.27)
                  n                               n

At this point, you should immediately recognize a similarity to the
kernelized perceptron: the optimal weight vector takes exactly the
same form in both algorithms.
   You can now take this new expression for w and plug it back in to
the expression for L, thus removing w from consideration. To avoid
subscript overloading, you should replace the n in the expression for
                                                                                      kernel methods   137



w with, say, m. This yields:

                                        2
                   1
   L(b, ξ, α, β) =       ∑ αm ym xm         + C ∑ ξ n − ∑ βn ξ n             (9.28)
                   2      m                      n        n
                                 "     "              #           !              #
                       − ∑ αn yn            ∑ αm ym xm · xn + b       − 1 + ξn
                           n                m
                                                                             (9.29)

At this point, it’s convenient to rewrite these terms; be sure you un-
derstand where the following comes from:

                     1
                     2∑  ∑ αn αm yn ym xn · xm + ∑(C − β n )ξ n
   L(b, ξ, α, β) =                                                           (9.30)
                       n m                       n
                       − ∑ ∑ αn αm yn ym xn · xm − ∑ αn (yn b − 1 + ξ n )
                           n m                            n
                                                                             (9.31)
                       1
                       2∑  ∑ αn αm yn ym xn · xm + ∑(C − β n )ξ n
                =−                                                           (9.32)
                         n m                       n
                       − b ∑ α n y n − ∑ α n ( ξ n − 1)                      (9.33)
                           n            n

Things are starting to look good: you’ve successfully removed the de-
pendence on w, and everything is now written in terms of dot prod-
ucts between input vectors! This might still be a difficult problem to
solve, so you need to continue and attempt to remove the remaining
variables b and ξ.
   The derivative with respect to b is:

   ∂L
      = − ∑ αn yn = 0                                                        (9.34)
   ∂b     n

This doesn’t allow you to substitute b with something (as you did
with w), but it does mean that the fourth term (b ∑n αn yn ) goes to
zero at the optimum.
   The last of the original variables is ξ n ; the derivatives in this case
look like:
   ∂L
        = C − β n − αn         ⇐⇒     C − β n = αn                           (9.35)
   ∂ξ n

Again, this doesn’t allow you to substitute, but it does mean that you
can rewrite the second term, which as ∑n (C − β n )ξ n as ∑n αn ξ n . This
then cancels with (most of) the final term. However, you need to be
careful to remember something. When we optimize, both αn and β n
are constrained to be non-negative. What this means is that since we
are dropping β from the optimization, we need to ensure that αn ≤ C,
otherwise the corresponding β will need to be negative, which is not
      138   a course in machine learning



      allowed. You finally wind up with the following, where xn · xm has
      been replaced by K ( xn , xm ):

                            1
         L(α) = ∑ αn −
                            2∑  ∑ αn αm yn ym K ( xn , xm )                  (9.36)
                   n          n m

      If you are comfortable with matrix notation, this has a very compact
      form. Let 1 denote the N-dimensional vector of all 1s, let y denote
      the vector of labels and let G be the N×N matrix, where Gn,m =
      yn ym K ( xn , xm ), then this has the following form:

                      1
         L(α) = α> 1 − α> Gα                                                 (9.37)
                      2

      The resulting optimization problem is to maximize L(α) as a function
      of α, subject to the constraint that the αn s are all non-negative and
      less than C (because of the constraint added when removing the β
      variables). Thus, your problem is:

                                  1
                                  2∑  ∑ αn αm yn ym K ( xn , xm ) − ∑ αn
            min        − L(α) =                                              (9.38)
              α
                                    n m                             n
        subj. to   0 ≤ αn ≤ C                                              (∀n)

      One way to solve this problem is gradient descent on α. The only
      complication is making sure that the αs satisfy the constraints. In
      this case, you can use a projected gradient algorithm: after each
      gradient update, you adjust your parameters to satisfy the constraints
      by projecting them into the feasible region. In this case, the projection
      is trivial: if, after a gradient step, any αn < 0, simply set it to 0; if any
      αn > C, set it to C.


9.6   Understanding Support Vector Machines

      The prior discussion involved quite a bit of math to derive a repre-
      sentation of the support vector machine in terms of the Lagrange
      variables. This mapping is actually sufficiently standard that every-
      thing in it has a name. The original problem variables (w, b, ξ) are
      called the primal variables; the Lagrange variables are called the
      dual variables. The optimization problem that results after removing
      all of the primal variables is called the dual problem.
         A succinct way of saying what you’ve done is: you found that after
      converting the SVM into its dual, it is possible to kernelize.
         To understand SVMs, a first step is to peek into the dual formula-
      tion, Eq (9.38). The objective has two terms: the first depends on the
      data, and the second depends only on the dual variables. The first
      thing to notice is that, because of the second term, the αs “want” to
                                                                                 kernel methods   139



get as large as possible. The constraint ensures that they cannot ex-
ceed C, which means that the general tendency is for the αs to grow
as close to C as possible.
    To further understand the dual optimization problem, it is useful
to think of the kernel as being a measure of similarity between two
data points. This analogy is most clear in the case of RBF kernels,
but even in the case of linear kernels, if your examples all have unit
norm, then their dot product is still a measure of similarity. Since you
can write the prediction function as f ( x̂) = sign(∑n αn yn K ( xn , x̂)), it
is natural to think of αn as the “importance” of training example n,
where αn = 0 means that it is not used at all at test time.
    Consider two data points that have the same label; namely, yn =
ym . This means that yn ym = +1 and the objective function has a term
that looks like αn αm K ( xn , xm ). Since the goal is to make this term
small, then one of two things has to happen: either K has to be small,
or αn αm has to be small. If K is already small, then this doesn’t affect
the setting of the corresponding αs. But if K is large, then this strongly
encourages at least one of αn or αm to go to zero. So if you have two
data points that are very similar and have the same label, at least one
of the corresponding αs will be small. This makes intuitive sense: if
you have two data points that are basically the same (both in the x
and y sense) then you only need to “keep” one of them around.
    Suppose that you have two data points with different labels:
yn ym = −1. Again, if K ( xn , xm ) is small, nothing happens. But if
it is large, then the corresponding αs are encouraged to be as large as
possible. In other words, if you have two similar examples with dif-
ferent labels, you are strongly encouraged to keep the corresponding
αs as large as C.
    An alternative way of understanding the SVM dual problem is
geometrically. Remember that the whole point of introducing the
variable αn was to ensure that the nth training example was correctly
classified, modulo slack. More formally, the goal of αn is to ensure
that yn (w · xn + b) − 1 + ξ n ≥ 0. Suppose that this constraint it
not satisfied. There is an important result in optimization theory,
called the Karush-Kuhn-Tucker conditions (or KKT conditions, for
short) that states that at the optimum, the product of the Lagrange
multiplier for a constraint, and the value of that constraint, will equal
zero. In this case, this says that at the optimum, you have:
       h                           i
    αn yn (w · xn + b) − 1 + ξ n = 0                                    (9.39)

In order for this to be true, it means that (at least) one of the follow-
ing must be true:

   αn = 0      or     yn (w · xn + b) − 1 + ξ n = 0                    (9.40)
      140   a course in machine learning



      A reasonable question to ask is: under what circumstances will αn
      be non-zero? From the KKT conditions, you can discern that αn can
      be non-zero only when the constraint holds exactly; namely, that
      yn (w · xn + b) − 1 + ξ n = 0. When does that constraint hold ex-
      actly? It holds exactly only for those points precisely on the margin of
      the hyperplane.
         In other words, the only training examples for which αn 6= 0
      are those that lie precisely 1 unit away from the maximum margin
      decision boundary! (Or those that are “moved” there by the corre-
      sponding slack.) These points are called the support vectors because
      they “support” the decision boundary. In general, the number of sup-
      port vectors is far smaller than the number of training examples, and
      therefore you naturally end up with a solution that only uses a subset
      of the training data.
         From the first discussion, you know that the points that wind up
      being support vectors are exactly those that are “confusable” in the
      sense that you have to examples that are nearby, but have different la-
      bels. This is a completely in line with the previous discussion. If you
      have a decision boundary, it will pass between these “confusable”
      points, and therefore they will end up being part of the set of support
      vectors.


9.7   Exercises

      Exercise 9.1. TODO. . .
                                                             10 | L EARNING T HEORY

         For nothing ought to be posited without a reason given, unless it        Learning Objectives:
         is self-evident or known by experience or proved by the authority        • Explain why inductive bias is
                                                                                    necessary.
         of Sacred Scripture.                    – William of Occam, c. 1320
                                                                                  • Define the PAC model and explain
                                                                                    why both the “P” and “A” are
                                                                                    necessary.
                                                                                  • Explain the relationship between
                                                                                    complexity measures and regulariz-
                                                                                    ers.
       By now, you are an expert at building learning algorithms. You             • Identify the role of complexity in
                                                                                    generalization.
       probably understand how they work, intuitively. And you under-
                                                                                  • Formalize the relationship between
       stand why they should generalize. However, there are several basic
                                                                                    margins and complexity.
       questions you might want to know the answer to. Is learning always
       possible? How many training examples will I need to do a good job
       learning? Is my test performance going to be much worse than my
       training performance? The key idea that underlies all these answer is
       that simple functions generalize well.
          The amazing thing is that you can actually prove strong results
       that address the above questions. In this chapter, you will learn
       some of the most important results in learning theory that attempt
       to answer these questions. The goal of this chapter is not theory for
                                                                                  Dependencies:
       theory’s sake, but rather as a way to better understand why learning
       models work, and how to use this theory to build better algorithms.
       As a concrete example, we will see how 2-norm regularization prov-
       ably leads to better generalization performance, thus justifying our
       common practice!


10.1   The Role of Theory

       In contrast to the quote at the start of this chapter, a practitioner
       friend once said “I would happily give up a few percent perfor-
       mance for an algorithm that I can understand.” Both perspectives
       are completely valid, and are actually not contradictory. The second
       statement is presupposing that theory helps you understand, which
       hopefully you’ll find to be the case in this chapter.
          Theory can serve two roles. It can justify and help understand
       why common practice works. This is the “theory after” view. It can
       also serve to suggest new algorithms and approaches that turn out to
       work well in practice. This is the “theory before” view. Often, it turns
       out to be a mix. Practitioners discover something that works surpris-
       ingly well. Theorists figure out why it works and prove something
       about it. And in the process, they make it better or find new algo-
       142   a course in machine learning



       rithms that more directly exploit whatever property it is that made
       the theory go through.
          Theory can also help you understand what’s possible and what’s
       not possible. One of the first things we’ll see is that, in general, ma-
       chine learning can not work. Of course it does work, so this means
       that we need to think harder about what it means for learning algo-
       rithms to work. By understanding what’s not possible, you can focus
       our energy on things that are.
          Probably the biggest practical success story for theoretical machine
       learning is the theory of boosting, which you won’t actually see in
       this chapter. (You’ll have to wait for Chapter 11.) Boosting is a very
       simple style of algorithm that came out of theoretical machine learn-
       ing, and has proven to be incredibly successful in practice. So much
       so that it is one of the de facto algorithms to run when someone gives
       you a new data set. In fact, in 2004, Yoav Freund and Rob Schapire
       won the ACM’s Paris Kanellakis Award for their boosting algorithm
       AdaBoost. This award is given for theoretical accomplishments that
       have had a significant and demonstrable effect on the practice of
       computing.1                                                                1
                                                                                   In 2008, Corinna Cortes and Vladimir
                                                                                  Vapnik won it for support vector
                                                                                  machines.
10.2   Induction is Impossible

       One nice thing about theory is that it forces you to be precise about
       what you are trying to do. You’ve already seen a formal definition
       of binary classification in Chapter 5. But let’s take a step back and
       re-analyze what it means to learn to do binary classification.
          From an algorithmic perspective, a natural question is whether
       there is an “ultimate” learning algorithm, Aawesome , that solves the
       Binary Classification problem above. In other words, have you been
       wasting your time learning about KNN and Perceptron and decision
       trees, when Aawesome is out there.
          What would such an ultimate learning algorithm do? You would
       like it to take in a data set D and produce a function f . No matter
       what D looks like, this function f should get perfect classification on
       all future examples drawn from the same distribution that produced
       D.
          A little bit of introspection should demonstrate that this is impos-
       sible. For instance, there might be label noise in our distribution. As
       a very simple example, let X = {−1, +1} (i.e., a one-dimensional,
       binary distribution. Define the data distribution as:

         D(h+1i, +1) = 0.4                  D(h−1i, −1) = 0.4            (10.1)
         D(h+1i, −1) = 0.1                  D(h−1i, +1) = 0.1            (10.2)

       In other words, 80% of data points in this distrubtion have x = y
                                                                                                     learning theory             143



       and 20% don’t. No matter what function your learning algorithm
       produces, there’s no way that it can do better than 20% error on this
       data.                                                                                     It’s clear that if your algorithm pro-
          Given this, it seems hopeless to have an algorithm Aawesome that                       duces a deterministic function that
       always achieves an error rate of zero. The best that we can hope is                   ?   it cannot do better than 20% error.
                                                                                                 What if it produces a stochastic (aka
       that the error rate is not “too large.”                                                   randomized) function?
          Unfortunately, simply weakening our requirement on the error
       rate is not enough to make learning possible. The second source of
       difficulty comes from the fact that the only access we have to the
       data distribution is through sampling. In particular, when trying to
       learn about a distribution like that in 10.1, you only get to see data
       points drawn from that distribution. You know that “eventually” you
       will see enough data points that your sample is representative of the
       distribution, but it might not happen immediately. For instance, even
       though a fair coin will come up heads only with probability 1/2, it’s
       completely plausible that in a sequence of four coin flips you never
       see a tails, or perhaps only see one tails.
          So the second thing that we have to give up is the hope that
       A awesome  will always work. In particular, if we happen to get a lousy
       sample of data from D , we need to allow Aawesome to do something
       completely unreasonable.
          Thus, we cannot hope that Aawesome will do perfectly, every time.
       We cannot even hope that it will do pretty well, all of the time. Nor
       can we hope that it will do perfectly, most of the time. The best best
       we can reasonably hope of Aawesome is that it it will do pretty well,
       most of the time.


10.3   Probably Approximately Correct Learning

       Probably Approximately Correct (PAC) learning is a formalism
       of inductive learning based on the realization that the best we can
       hope of an algorithm is that it does a good job (i.e., is approximately
       correct), most of the time (i.e., it is probably appoximately correct).2          2
                                                                                          Leslie Valiant invented the notion
           Consider a hypothetical learning algorithm. You run it on ten dif-            of PAC learning in 1984. In 2011,
                                                                                         he received the Turing Award, the
       ferent binary classification data sets. For each one, it comes back with          highest honor in computing for his
       functions f 1 , f 2 , . . . , f 10 . For some reason, whenever you run f 4 on a   work in learning theory, computational
                                                                                         complexity and parallel systems.
       test point, it crashes your computer. For the other learned functions,
       their performance on test data is always at most 5% error. If this
       situtation is guaranteed to happen, then this hypothetical learning
       algorithm is a PAC learning algorithm. It satisfies “probably” because
       it only failed in one out of ten cases, and it’s “approximate” because
       it achieved low, but non-zero, error on the remainder of the cases.
           This leads to the formal definition of an (e, δ) PAC-learning algo-
       rithm. In this definition, e plays the role of measuring accuracy (in
       144   a course in machine learning



       the previous example, e = 0.05) and δ plays the role of measuring
       failure (in the previous, δ = 0.1).

       Definitions 1. An algorithm A is an (e, δ)-PAC learning algorithm if, for
       all distributions D : given samples from D , the probability that it returns a
       “bad function” is at most δ; where a “bad” function is one with test error
       rate more than e on D .

           There are two notions of efficiency that matter in PAC learning. The
       first is the usual notion of computational complexity. You would prefer
       an algorithm that runs quickly to one that takes forever. The second
       is the notion of sample complexity: the number of examples required
       for your algorithm to achieve its goals. Note that the goal of both
       of these measure of complexity is to bound how much of a scarse
       resource your algorithm uses. In the computational case, the resource
       is CPU cycles. In the sample case, the resource is labeled examples.

       Definition: An algorithm A is an efficient (e, δ)-PAC learning al-
       gorithm if it is an (e, δ)-PAC learning algorithm whose runtime is
       polynomial in 1e and 1δ .
         In other words, suppose that you want your algorithm to achieve
       4% error rate rather than 5%. The runtime required to do so should
       no go up by an exponential factor.


10.4   PAC Learning of Conjunctions

       To get a better sense of PAC learning, we will start with a completely
       irrelevant and uninteresting example. The purpose of this example is
       only to help understand how PAC learning works.
          The setting is learning conjunctions. Your data points are binary
       vectors, for instance x = h0, 1, 1, 0, 1i. Someone guarantees for you
       that there is some boolean conjunction that defines the true labeling
       of this data. For instance, x1 ∧ ¬ x2 ∧ x5 (“or” is not allowed). In
       formal terms, we often call the true underlying classification function
       the concept. So this is saying that the concept you are trying to learn
       is a conjunction. In this case, the boolean function would assign a
       negative label to the example above.
          Since you know that the concept you are trying to learn is a con-
       junction, it makes sense that you would represent your function as
       a conjunction as well. For historical reasons, the function that you
       learn is often called a hypothesis and is often denoted h. However,
       in keeping with the other notation in this book, we will continue to
       denote it f .
          Formally, the set up is as follows. There is some distribution D X
       over binary data points (vectors) x = h x1 , x2 , . . . , x D i. There is a fixed
                                                                                     learning theory               145



concept conjunction c that we are trying to learn. There is no noise,              y     x1     x2     x3     x4
so for any example x, its true label is simply y = c( x).                         +1     0      0      1      1
    What is a reasonable algorithm in this case? Suppose that you                 +1     0      1      1      1
observe the example in Table 10.1. From the first example, we know                -1     1      1      0      1
that the true formula cannot include the term x1 . If it did, this exam-    Table 10.1: Data set for learning con-
                                                                            junctions.
ple would have to be negative, which it is not. By the same reason-
ing, it cannot include x2 . By analogous reasoning, it also can neither
include the term ¬ x3 nor the term ¬ x4 .
    This suggests the algorithm in Algorithm 10.4, colloquially the
“Throw Out Bad Terms” algorithm. In this algorith, you begin with a
function that includes all possible 2D terms. Note that this function
will initially classify everything as negative. You then process each
example in sequence. On a negative example, you do nothing. On
a positive example, you throw out terms from f that contradict the
given positive example.                                                          Verify that Algorithm 10.4 main-
    If you run this algorithm on the data in Table 10.1, the sequence of         tains an invariant that it always errs
f s that you cycle through are:                                              ?   on the side of classifying examples
                                                                                 negative and never errs the other
                                                                                 way.
   f 0 ( x ) = x1 ∧ ¬ x1 ∧ x2 ∧ ¬ x2 ∧ x3 ∧ ¬ x3 ∧ x4 ∧ ¬ x4       (10.3)
    1
   f ( x ) = ¬ x1 ∧ ¬ x2 ∧ x3 ∧ x4                                 (10.4)
   f 2 ( x ) = ¬ x1 ∧ x3 ∧ x4                                      (10.5)
    3
   f ( x ) = ¬ x1 ∧ x3 ∧ x4                                        (10.6)

The first thing to notice about this algorithm is that after processing
an example, it is guaranteed to classify that example correctly. This
observation requires that there is no noise in the data.
    The second thing to notice is that it’s very computationally ef-
ficient. Given a data set of N examples in D dimensions, it takes
O( ND ) time to process the data. This is linear in the size of the data
set.
    However, in order to be an efficient (e, δ)-PAC learning algorithm,
you need to be able to get a bound on the sample complexity of this
algorithm. Sure, you know that its run time is linear in the number
of example N. But how many examples N do you need to see in order
to guarantee that it achieves an error rate of at most e (in all but δ-
                                                     D/e
many cases)? Perhaps N has to be gigantic (like 22 ) to (probably)
guarantee a small error.
    The goal is to prove that the number of samples N required to
(probably) achieve a small error is not-too-big. The general proof
technique for this has essentially the same flavor as almost every PAC
learning proof around. First, you define a “bad thing.” In this case,
a “bad thing” is that there is some term (say ¬ x8 ) that should have
been thrown out, but wasn’t. Then you say: well, bad things happen.
Then you notice that if this bad thing happened, you must not have
146   a course in machine learning



Algorithm 30 BinaryConjunctionTrain(D)
  1: f ← x1 ∧ ¬ x1 ∧ x2 ∧ ¬ x2 ∧ · · · ∧ x D ∧ ¬ x D     // initialize function
  2: for all positive examples ( x,+1) in D do

  3:   for d = 1 . . . D do
  4:      if xd = 0 then
  5:          f ← f without term “xd ”
  6:      else
  7:          f ← f without term “¬ xd ”
  8:      end if
  9:   end for
 10: end for

 11: return f




seen any positive training examples with x8 = 0. So example with
x8 = 0 must have low probability (otherwise you would have seen
them). So bad things must not be that common.

Theorem 13. With probability at least (1 − δ): Algorithm 10.4 requires at
most N = . . . examples to achieve an error rate ≤ e.

Proof of Theorem 13. Let c be the concept you are trying to learn and
let D be the distribution that generates the data.
    A learned function f can make a mistake if it contains any term t
that is not in c. There are initially 2D many terms in f , and any (or
all!) of them might not be in c. We want to ensure that the probability
that f makes an error is at most e. It is sufficient to ensure that
    For a term t (e.g., ¬ x5 ), we say that t “negates” an example x if
t( x) = 0. Call a term t “bad” if (a) it does not appear in c and (b) has
probability at least e/2D of appearing (with respect to the unknown
distribution D over data points).
    First, we show that if we have no bad terms left in f , then f has an
error rate at most e.
    We know that f contains at most 2D terms, since is begins with 2D
terms and throws them out.
    The algorithm begins with 2D terms (one for each variable and
one for each negated variable). Note that f will only make one type
of error: it can call positive examples negative, but can never call a
negative example positive. Let c be the true concept (true boolean
formula) and call a term “bad” if it does not appear in c. A specific
bad term (e.g., ¬ x5 ) will cause f to err only on positive examples
that contain a corresponding bad value (e.g., x5 = 1). TODO... finish
this

   What we’ve shown in this theorem is that: if the true underly-
ing concept is a boolean conjunction, and there is no noise, then the
“Throw Out Bad Terms” algorithm needs N ≤ . . . examples in order
                                                                                    learning theory   147



       to learn a boolean conjunction that is (1 − δ)-likely to achieve an er-
       ror of at most e. That is to say, that the sample complexity of “Throw
       Out Bad Terms” is . . . . Moreover, since the algorithm’s runtime is
       linear in N, it is an efficient PAC learning algorithm.


10.5   Occam’s Razor: Simple Solutions Generalize

       The previous example of boolean conjunctions is mostly just a warm-
       up exercise to understand PAC-style proofs in a concrete setting.
       In this section, you get to generalize the above argument to a much
       larger range of learning problems. We will still assume that there is
       no noise, because it makes the analysis much simpler. (Don’t worry:
       noise will be added eventually.)
           William of Occam (c. 1288 – c. 1348) was an English friar and
       philosopher is is most famous for what later became known as Oc-
       cam’s razor and popularized by Bertrand Russell. The principle ba-
       sically states that you should only assume as much as you need. Or,
       more verbosely, “if one can explain a phenomenon without assuming
       this or that hypothetical entity, then there is no ground for assuming
       it i.e. that one should always opt for an explanation in terms of the
       fewest possible number of causes, factors, or variables.” What Occam
       actually wrote is the quote that began this chapter.
           In a machine learning context, a reasonable paraphrase is “simple
       solutions generalize well.” In other words, you have 10, 000 features
       you could be looking at. If you’re able to explain your predictions
       using just 5 of them, or using all 10, 000 of them, then you should just
       use the 5.
           The Occam’s razor theorem states that this is a good idea, theo-
       retically. It essentially states that if you are learning some unknown
       concept, and if you are able to fit your training data perfectly, but you
       don’t need to resort to a huge class of possible functions to do so,
       then your learned function will generalize well. It’s an amazing theo-
       rem, due partly to the simplicity of its proof. In some ways, the proof
       is actually easier than the proof of the boolean conjunctions, though it
       follows the same basic argument.
           In order to state the theorem explicitly, you need to be able to
       think about a hypothesis class. This is the set of possible hypotheses
       that your algorithm searches through to find the “best” one. In the
       case of the boolean conjunctions example, the hypothesis class, H,
       is the set of all boolean formulae over D-many variables. In the case
       of a perceptron, your hypothesis class is the set of all possible linear
       classifiers. The hypothesis class for boolean conjunctions is finite; the
       hypothesis class for linear classifiers is infinite. For Occam’s razor, we
       can only work with finite hypothesis classes.
       148   a course in machine learning



       Theorem 14 (Occam’s Bound). Suppose A is an algorithm that learns
       a function f from some finite hypothesis class H. Suppose the learned
       function always gets zero error on the training data. Then, the sample com-
       plexity of f is at most log |H|.

         TODO COMMENTS

       Proof of Theorem 14. TODO

           This theorem applies directly to the “Throw Out Bad Terms” algo-
       rithm, since (a) the hypothesis class is finite and (b) the learned func-
       tion always achieves zero error on the training data. To apply Oc-
       cam’s Bound, you need only compute the size of the hypothesis class
       H of boolean conjunctions. You can compute this by noticing that
       there are a total of 2D possible terms in any formula in H. Moreover,
       each term may or may not be in a formula. So there are 22D = 4D
       possible formulae; thus, |H| = 4D . Applying Occam’s Bound, we see
       that the sample complexity of this algorithm is N ≤ . . . .
           Of course, Occam’s Bound is general enough to capture other
       learning algorithms as well. In particular, it can capture decision
       trees! In the no-noise setting, a decision tree will always fit the train-
       ing data perfectly. The only remaining difficulty is to compute the
       size of the hypothesis class of a decision tree learner.
           For simplicity’s sake, suppose that our decision tree algorithm
       always learns complete trees: i.e., every branch from root to leaf
       is length D. So the number of split points in the tree (i.e., places
       where a feature is queried) is 2D−1 . (See Figure 10.1.) Each split
       point needs to be assigned a feature: there D-many choices here.              Figure 10.1: thy:dt: picture of full
       This gives D2D−1 trees. The last thing is that there are 2D leaves            decision tree

       of the tree, each of which can take two possible values, depending
       on whether this leaf is classified as +1 or −1: this is 2×2D = 2D+1
       possibilities. Putting this all togeter gives a total number of trees
       |H| = D2D−1 2D+1 = D22D = D4D . Applying Occam’s Bound, we see
       that TODO examples is enough to learn a decision tree!


10.6   Complexity of Infinite Hypothesis Spaces

       Occam’s Bound is a fantastic result for learning over finite hypothesis
       spaces. Unfortunately, it is completely useless when |H| = ∞. This is
       because the proof works by using each of the N training examples to
       “throw out” bad hypotheses until only a small number are left. But if
       |H| = ∞, and you’re throwing out a finite number at each step, there
       will always be an infinite number remaining.
          This means that, if you want to establish sample complexity results
       for infinite hypothesis spaces, you need some new way of measuring
                                                                                           learning theory              149



their “size” or “complexity.” A prototypical way of doing this is to
measure the complexity of a hypothesis class as the number of different
things it can do.
    As a silly example, consider boolean conjunctions again. Your
input is a vector of binary features. However, instead of representing
your hypothesis as a boolean conjunction, you choose to represent
it as a conjunction of inequalities. That is, instead of writing x1 ∧
¬ x2 ∧ x5 , you write [ x1 > 0.2] ∧ [ x2 < 0.77] ∧ [ x5 < π/4]. In this
representation, for each feature, you need to choose an inequality
(< or >) and a threshold. Since the thresholds can be arbitrary real
values, there are now infinitely many possibilities: |H| = 2D×∞ = ∞.
However, you can immediately recognize that on binary features,
there really is no difference between [ x2 < 0.77] and [ x2 < 0.12] and
any other number of infinitely many possibilities. In other words,
even though there are infinitely many hypotheses, there are only finitely
many behaviors.
    The Vapnik-Chernovenkis dimension (or VC dimension) is a
classic measure of complexity of infinite hypothesis classes based on
this intuition3 . The VC dimension is a very classification-oriented no-
tion of complexity. The idea is to look at a finite set of unlabeled ex-
amples, such as those in Figure 10.2. The question is: no matter how             Figure 10.2: thy:vcex: figure with three
these points were labeled, would we be able to find a hypothesis that            and four examples
                                                                                 3
                                                                                   Yes, this is the same Vapnik who
correctly classifies them. The idea is that as you add more points,
                                                                                 is credited with the creation of the
being able to represent an arbitrary labeling becomes harder and                 support vector machine.
harder. For instance, regardless of how the three points are labeled,
you can find a linear classifier that agrees with that classification.
However, for the four points, there exists a labeling for which you
cannot find a perfect classifier. The VC dimension is the maximum
number of points for which you can always find such a classifier.                      What is that labeling? What is it’s
    You can think of VC dimension as a game between you and an                       ? name?
adversary. To play this game, you choose K unlabeled points however
you want. Then your adversary looks at those K points and assigns
binary labels to them them however he wants. You must then find a
hypothesis (classifier) that agrees with his labeling. You win if you
can find such a hypothesis; he wins if you cannot. The VC dimension
of your hypothesis class is the maximum number of points K so that
you can always win this game. This leads to the following formal
definition, where you can interpret there exists as your move and for
all as adversary’s move.


Definitions 2. For data drawn from some space X , the VC dimension of
a hypothesis space H over X is the maximal K such that: there exists a set
X ⊆ X of size | X | = K, such that for all binary labelings of X, there exists
a function f ∈ H that matches this labeling.
       150   a course in machine learning



          In general, it is much easier to show that the VC dimension is at
       least some value; it is much harder to show that it is at most some
       value. For example, following on the example from Figure 10.2, the
       image of three points (plus a little argumentation) is enough to show
       that the VC dimension of linear classifiers in two dimension is at least
       three.
          To show that the VC dimension is exactly three it suffices to show
       that you cannot find a set of four points such that you win this game
       against the adversary. This is much more difficult. In the proof that
       the VC dimension is at least three, you simply need to provide an
       example of three points, and then work through the small number of
       possible labelings of that data. To show that it is at most three, you
       need to argue that no matter what set of four point you pick, you
       cannot win the game.
          VC
          margins
          small norms


10.7   Learning with Noise

10.8   Agnostic Learning

10.9   Error versus Regret

       Despite the fact that there’s no way to get better than 20% error on
       this distribution, it would be nice to say that you can still learn some-
       thing from it. For instance, the predictor that always guesses y = x
       seems like the “right” thing to do. Based on this observation, maybe
       we can rephrase the goal of learning as to find a function that does
       as well as the distribution allows. In other words, on this data, you
       would hope to get 20% error. On some other distribution, you would
       hope to get X% error, where X% is the best you could do.
           This notion of “best you could do” is sufficiently important that
       it has a name: the Bayes error rate. This is the error rate of the best
       possible classifier, the so-called Bayes optimal classifier. If you knew
       the underlying distribution D , you could actually write down the
       exact Bayes optimal classifier explicitly. (This is why learning is unin-
       teresting in the case that you know D .) It simply has the form:
                          (
             Bayes          +1 if D( x, +1) > D( x, −1)
           f       ( x) =                                                   (10.7)
                            −1 otherwise
       The Bayes optimal error rate is the error rate that this (hypothetical)
       classifier achieves:
          eBayes = E( x,y)∼D y 6= f Bayes ( x)
                                              
                                                                          (10.8)
                                 learning theory   151



10.10 Exercises

      Exercise 10.1. TODO. . .
                                                             11 | E NSEMBLE M ETHODS
                                                                                –        Learning Objectives:
                                                                                         • Implement bagging and explain how
                                                                                           it reduces variance in a predictor.
                                                                                         • Explain the difference between a
                                                                                           weak learner and a strong learner.
                                                                                         • Derive the AdaBoost algorithm.
                                                                                         • Understand the relationship between
                                                                                           boosting decision stumps and linear
                                                                                           classification.
       Groups of people can often make better decisions than
       individuals, especially when group members each come in with
       their own biases. The same is true in machine learning. Ensemble
       methods are learning models that achieve performance by combining
       the opinions of multiple learners. In doing so, you can often get away
       with using much simpler learners and still achieve great performance.
       Moreover, ensembles are inherantly parallel, which can make them
       much more efficient at training and test time, if you have access to
       multiple processors.
          In this chapter, you will learn about various ways of combining
       base learners into ensembles. One of the shocking results we will
       see is that you can take a learning model that only ever does slightly
                                                                                         Dependencies:
       better than chance, and turn it into an arbitrarily good learning
       model, though a technique known as boosting. You will also learn
       how ensembles can decrease the variance of predictors as well as
       perform regularization.


11.1   Voting Multiple Classifiers

       All of the learning algorithms you have seen so far are deterministic.
       If you train a decision tree multiple times on the same data set, you
       will always get the same tree back. In order to get an effect out of
       voting multiple classifiers, they need to differ. There are two primary
       ways to get variability. You can either change the learning algorithm
       or change the data set.
          Building an emsemble by training different classifiers is the most
       straightforward approach. As in single-model learning, you are given
       a data set (say, for classification). Instead of learning a single classifier
       (e.g., a decision tree) on this data set, you learn multiple different
       classifiers. For instance, you might train a decision tree, a perceptron,
       a KNN, and multiple neural networks with different architectures.
       Call these classifiers f 1 , . . . , f M . At test time, you can make a predic-
       tion by voting. On a test example x̂, you compute ŷ1 = f 1 ( x̂ ), . . . ,
                                                                                          ensemble methods                 153



ŷ M = f M ( x̂ ). If there are more +1s in the list hy1 , . . . , y M then you
predict +1; otherwise you predict −1.
    The main advantage of ensembles of different classifiers is that it
is unlikely that all classifiers will make the same mistake. In fact, as
long as every error is made by a minority of the classifiers, you will
achieve optimal classification! Unfortunately, the inductive biases of
different learning algorithms are highly correlated. This means that
different algorithms are prone to similar types of errors. In particular,
ensembles tend to reduce the variance of classifiers. So if you have
a classification algorithm that tends to be very sensitive to small
changes in the training data, ensembles are likely to be useful.                         Which of the classifiers you’ve
    Note that the voting scheme naturally extends to multiclass clas-                 ? learned about so far have high
                                                                                         variance?
sification. However, it does not make sense in the contexts of regres-
sion, ranking or collective classification. This is because you will
rarely see the same exact output predicted twice by two different
regression models (or ranking models or collective classification mod-
els). For regression, a simple solution is to take the mean or median
prediction from the different models. For ranking and collective clas-
sification, different approaches are required.
    Instead of training different types of classifiers on the same data
set, you can train a single type of classifier (e.g., decision tree) on
multiple data sets. The question is: where do these multiple data sets
come from, since you’re only given one at training time?
    One option is to fragment your original data set. For instance, you
could break it into 10 pieces and build decision trees on each of these
pieces individually. Unfortunately, this means that each decision tree
is trained on only a very small part of the entire data set and is likely
to perform poorly.
    A better solution is to use bootstrap resampling. This is a tech-
nique from the statistics literature based on the following observa-
tion. The data set we are given, D, is a sample drawn i.i.d. from an
unknown distribution D . If we draw a new data set D̃ by random
sampling from D with replacement1 , then D̃ is also a sample from D .             Figure 11.1: picture of sampling with
Figure 11.1 shows the process of bootstrap resampling of ten objects.             replacement
    Applying this idea to ensemble methods yields a technique known               1
                                                                                   To sample with replacement, imagine
                                                                                  putting all items from D in a hat. To
as bagging. You start with a single data set D that contains N train-             draw a single sample, pick an element
ing examples. From this single data set, you create M-many “boot-                 at random from that hat, write it down,
strapped training sets” D̃1 , . . . D̃ M . Each of these bootstrapped sets        and then put it back.

also contains N training examples, drawn randomly from D with
replacement. You can then train a decision tree (or other model)
seperately on each of these data sets to obtain classifiers f 1 , . . . , f M .
As before, you can use these classifiers to vote on new test points.
    Note that the bootstrapped data sets will be similar. However, they
will not be too similar. For example, if N is large then the number of
       154   a course in machine learning



       examples that are not present in any particular bootstrapped sample
       is relatively large. The probability that the first training example is
       not selected once is (1 − 1/N ). The probability that it is not selected
       at all is (1 − 1/N ) N . As N → ∞, this tends to 1/e ≈ 0.3679. (Already
       for N = 1000 this is correct to four decimal points.) So only about
       63% of the original training examples will be represented in any
       given bootstrapped set.
           Since bagging tends to reduce variance, it provides an alternative
       approach to regularization. That is, even if each of the learned clas-
       sifiers f 1 , . . . , f M are individually overfit, they are likely to be overfit
       to different things. Through voting, you are able to overcome a sig-
       nificant portion of this overfitting. Figure ?? shows this effect by                Figure 11.2: graph depicting overfitting
       comparing regularization via hyperparameters to regularization via                  using regularization versus bagging
       bagging.


11.2   Boosting Weak Learners

       Boosting is the process of taking a crummy learning algorithm (tech-
       nically called a weak learner) and turning it into a great learning
       algorithm (technically, a strong learner). Of all the ideas that origi-
       nated in the theoretical machine learning community, boosting has
       had—perhaps—the greatest practical impact. The idea of boosting
       is reminiscent of what you (like me!) might have thought when you
       first learned about file compression. If I compress a file, and then
       re-compress it, and then re-compress it, eventually I’ll end up with a
       final that’s only one byte in size!
          To be more formal, let’s define a strong learning algorithm L as
       follows. When given a desired error rate e, a failure probability δ
       and access to “enough” labeled examples from some distribution D ,
       then, with high probability (at least 1 − δ), L learns a classifier f that
       has error at most e. This is precisely the definition of PAC learning
       that you learned about in Chapter 10. Building a strong learning
       algorithm might be difficult. We can as if, instead, it is possible to
       build a weak learning algorithm W that only has to achieve an error
       rate of 49%, rather than some arbitrary user-defined parameter e.
       (49% is arbitrary: anything strictly less than 50% would be fine.)
          Boosting is more of a “framework” than an algorithm. It’s a frame-
       work for taking a weak learning algorithm W and turning it into a
       strong learning algorithm. The particular boosting algorithm dis-
       cussed here is AdaBoost, short for “adaptive boosting algorithm.”
       AdaBoost is famous because it was one of the first practical boosting
       algorithms: it runs in polynomial time and does not require you to
       define a large number of hyperparameters. It gets its name from the
       latter benefit: it automatically adapts to the data that you give it.
                                                                                                       ensemble methods              155



Algorithm 31 AdaBoost( W , D , K)
 1: d(0) ← h N1 , N1 , . . . , N1 i           // Initialize uniform importance to each example
 2: for k = 1 . . . K do

 3:     f (k) ← W (D , d(k-1) )                        // Train kth classifier on weighted data
 4:    ŷn ← f (k) ( xn ), ∀n                              // Make predictions on training data
 5:    ê(k) ← ∑n d(k-1)
                       n [yn 6=     ŷn ]                  // Compute weighted training error
                            1−ê(k)
 6:    α(k) ← 12 log         ê(k)
                                                            // Compute “adaptive” parameter
 7:    dn ← Z1 d(k-1)
         (k)
                n     exp[−α(k) yn ŷn ], ∀n             // Re-weight examples and normalize
 8: end for
 9: return f ( x̂ ) = sgn ∑ k α
                                (k) (k)
                                          
                                   f ( x̂)                // Return (weighted) voted classifier




    The intuition behind AdaBoost is like studying for an exam by
using a past exam. You take the past exam and grade yourself. The
questions that you got right, you pay less attention to. Those that you
got wrong, you study more. Then you take the exam again and repeat
this process. You continually down-weight the importance of questions
you routinely answer correctly and up-weight the importance of ques-
tions you routinely answer incorrectly. After going over the exam
multiple times, you hope to have mastered everything.
    The precise AdaBoost training algorithm is shown in Algorithm 11.2.
The basic functioning of the algorithm is to maintain a weight dis-
tribution d, over data points. A weak learner, f (k) is trained on this
weighted data. (Note that we implicitly assume that our weak learner
can accept weighted training data, a relatively mild assumption that
is nearly always true.) The (weighted) error rate of f (k) is used to de-
termine the adaptive parameter α, which controls how “important” f (k)
is. As long as the weak learner does, indeed, achieve < 50% error,
then α will be greater than zero. As the error drops to zero, α grows
without bound.                                                                                        What happens if the weak learn-
    After the adaptive parameter is computed, the weight distibution                                  ing assumption is violated and ê is
is updated for the next iteration. As desired, examples that are cor-                             ?   equal to 50%? What if it is worse
                                                                                                      than 50%? What does this mean, in
rectly classified (for which yn ŷn = +1) have their weight decreased                                 practice?
multiplicatively. Examples that are incorrectly classified (yn ŷn = −1)
have their weight increased multiplicatively. The Z term is a nom-
ralization constant to ensure that the sum of d is one (i.e., d can be
interpreted as a distribution). The final classifier returned by Ad-
aBoost is a weighted vote of the individual classifiers, with weights
given by the adaptive parameters.
    To better understand why α is defined as it is, suppose that our
weak learner simply returns a constant function that returns the
(weighted) majority class. So if the total weight of positive exam-
ples exceeds that of negative examples, f ( x) = +1 for all x; otherwise
f ( x) = −1 for all x. To make the problem moderately interesting,
suppose that in the original training set, there are 80 positive ex-
156   a course in machine learning



amples and 20 negative examples. In this case, f (1) ( x) = +1. It’s
weighted error rate will be ê(1) = 0.2 because it gets every negative
example wrong. Computing, we get α(1) = 21 log 4. Before normaliza-
tion, we get the new weight for each positive (correct) example to be
1 exp[− 12 log 4] = 21 . The weight for each negative (incorrect) example
becomes 1 exp[ 12 log 4] = 2. We can compute Z = 80×12 + 20×2 = 80.
Therefore, after normalization, the weight distribution on any single
                         1                                                1
positive example is 160    and the weight on any negative example is 40     .
However, since there are 80 positive examples and 20 negative exam-
ples, the cumulative weight on all positive examples is 80×160 1
                                                                   = 12 ;
                                                           1
the cumulative weight on all negative examples is 20×40      = 12 . Thus,
after a single boosting iteration, the data has become precisely evenly
weighted. This guarantees that in the next iteration, our weak learner
must do something more interesting than majority voting if it is to
achieve an error rate less than 50%, as required.                                    This example uses concrete num-
    One of the major attractions of boosting is that it is perhaps easy              bers, but the same result holds no
                                                                                     matter what the data distribution
to design computationally efficient weak learners. A very popular
                                                                                 ?   looks like nor how many examples
type of weak learner is a shallow decision tree: a decision tree with a              there are. Write out the general case
small depth limit. Figure 11.3 shows test error rates for decision trees             to see that you will still arrive at an
                                                                                     even weighting after one iteration.
of different maximum depths (the different curves) run for differing
numbers of boosting iterations (the x-axis). As you can see, if you
are willing to boost for many iterations, very shallow trees are quite
effective.
    In fact, a very popular weak learner is a decision decision stump:
a decision tree that can only ask one question. This may seem like a
silly model (and, in fact, it is on it’s own), but when combined with
boosting, it becomes very effective. To understand why, suppose for
a moment that our data consists only of binary features, so that any
question that a decision tree might ask is of the form “is feature 5
on?” By concentrating on decision stumps, all weak functions must
have the form f ( x) = s(2xd − 1), where s ∈ {±1} and d indexes some
feature.
    Now, consider the final form of a function learned by AdaBoost.
We can expand it as follow, where we let f k denote the single feature
selected by the kth decision stump and let sk denote its sign:
                  "                         #
                                                                                Figure 11.3: perf comparison of depth
   f ( x) = sgn       ∑ αk f   (k)
                                     ( x)                              (11.1)   vs # boost
                      k                                                              Why do the functions have this
                  "                                 #                            ? form?
        = sgn         ∑ αk sk (2x fk − 1)                              (11.2)
                      k
                  "                                     #
        = sgn         ∑ 2αk sk x fk − ∑ αk sk                          (11.3)
                      k                         k

        = sgn [w · x + b]                                              (11.4)
                                                                                                ensemble methods   157



       Algorithm 32 RandomForestTrain(D , depth, K)
        1: for k = 1 . . . K do
        2:    t(k) ← complete binary tree of depth depth with random feature splits
        3:    f (k) ← the function computed by t(k) , with leaves filled in by D
        4: end for

        5: return f ( x̂ ) = sgn ∑ k f
                                       (k)
                                                
                                           ( x̂)                   // Return voted classifier



              where wd =        ∑ 2αk sk      and     b = − ∑ αk sk                   (11.5)
                              k: f k =d                        k

       Thus, when working with decision stumps, AdaBoost actually pro-
       vides an algorithm for learning linear classifiers! In fact, this con-
       nection has recently been strengthened: you can show that AdaBoost
       provides an algorithm for optimizing exponential loss. (However,
       this connection is beyond the scope of this book.)
          As a further example, consider the case of boosting a linear classi-
       fier. In this case, if we let the kth weak classifier be parameterized by
       w(k) and b(k) , the overall predictor will have the form:
                        "                            #
                                                  
           f ( x) = sgn ∑ αk sgn w · x + b
                                       (k)     (k)
                                                                            (11.6)
                          k

       You can notice that this is nothing but a two-layer neural network,
       with K-many hidden units! Of course it’s not a classifically trained
       neural network (once you learn w(k) you never go back and update
       it), but the structure is identical.


11.3   Random Ensembles

       One of the most computationally expensive aspects of ensembles of
       decision trees is training the decision trees. This is very fast for de-
       cision stumps, but for deeper trees it can be prohibitively expensive.
       The expensive part is choosing the tree structure. Once the tree struc-
       ture is chosen, it is very cheap to fill in the leaves (i.e., the predictions
       of the trees) using the training data.
          An efficient and surprisingly effective alternative is to use trees
       with fixed structures and random features. Collections of trees are
       called forests, and so classifiers built like this are called random
       forests. The random forest training algorithm, shown in Algo-
       rithm 11.3 is quite short. It takes three arguments: the data, a desired
       depth of the decision trees, and a number K of total decision trees to
       build.
          The algorithm generates each of the K trees independently, which
       makes it very easy to parallelize. For each trees, it constructs a full
       binary tree of depth depth. The features used at the branches of this
       158   a course in machine learning



       tree are selected randomly, typically with replacement, meaning that
       the same feature can appear multiple times, even in one branch. The
       leaves of this tree, where predictions are made, are filled in based on
       the training data. This last step is the only point at which the training
       data is used. The resulting classifier is then just a voting of the K-
       many random trees.
           The most amazing thing about this approach is that it actually
       works remarkably well. It tends to work best when all of the features
       are at least marginally relevant, since the number of features selected
       for any given tree is small. An intuitive reason that it works well
       is the following. Some of the trees will query on useless features.
       These trees will essentially make random predictions. But some
       of the trees will happen to query on good features and will make
       good predictions (because the leaves are estimated based on the
       training data). If you have enough trees, the random ones will wash
       out as noise, and only the good trees will have an effect on the final
       classification.


11.4   Exercises

       Exercise 11.1. TODO. . .
                                                        12 | E FFICIENT L EARNING
                                                                         –       Learning Objectives:
                                                                                 • Understand and be able to imple-
                                                                                   ment stochastic gradient descent
                                                                                   algorithms.
                                                                                 • Compare and contrast small ver-
                                                                                   sus large batch sizes in stochastic
                                                                                   optimization.
                                                                                 • Derive subgradients for sparse
                                                                                   regularizers.
       So far, our focus has been on models of learning and basic al-            • Implement feature hashing.
       gorithms for those models. We have not placed much emphasis on
       how to learn quickly. The basic techniques you learned about so far
       are enough to get learning algorithms running on tens or hundreds
       of thousands of examples. But if you want to build an algorithm for
       web page ranking, you will need to deal with millions or billions
       of examples, in hundreds of thousands of dimensions. The basic
       approaches you have seen so far are insufficient to achieve such a
       massive scale.
          In this chapter, you will learn some techniques for scaling learning
       algorithms. This are useful even when you do not have billions of
       training examples, because it’s always nice to have a program that
                                                                                 Dependencies:
       runs quickly. You will see techniques for speeding up both model
       training and model prediction. The focus in this chapter is on linear
       models (for simplicity), but most of what you will learn applies more
       generally.


12.1   What Does it Mean to be Fast?

       Everyone always wants fast algorithms. In the context of machine
       learning, this can mean many things. You might want fast training
       algorithms, or perhaps training algorithms that scale to very large
       data sets (for instance, ones that will not fit in main memory). You
       might want training algorithms that can be easily parallelized. Or,
       you might not care about training efficiency, since it is an offline
       process, and only care about how quickly your learned functions can
       make classification decisions.
          It is important to separate out these desires. If you care about
       efficiency at training time, then what you are really asking for are
       more efficient learning algorithms. On the other hand, if you care
       about efficiency at test time, then you are asking for models that can
       be quickly evaluated.
          One issue that is not covered in this chapter is parallel learning.
       160   a course in machine learning



       This is largely because it is currently not a well-understood area in
       machine learning. There are many aspects of parallelism that come
       into play, such as the speed of communication across the network,
       whether you have shared memory, etc. Right now, this the general,
       poor-man’s approach to parallelization, is to employ ensembles.


12.2   Stochastic Optimization

       During training of most learning algorithms, you consider the entire
       data set simultaneously. This is certainly true of gradient descent
       algorithms for regularized linear classifiers (recall Algorithm 6.4), in
       which you first compute a gradient over the entire training data (for
       simplicity, consider the unbiased case):

         g = ∑ ∇w `(yn , w · xn ) + λw                                        (12.1)
               n

       where `(y, ŷ) is some loss function. Then you update the weights by
       w ← w − ηg. In this algorithm, in order to make a single update, you
       have to look at every training example.
          When there are billions of training examples, it is a bit silly to look
       at every one before doing anything. Perhaps just on the basis of the
       first few examples, you can already start learning something!
          Stochastic optimization involves thinking of your training data
       as a big distribution over examples. A draw from this distribution
       corresponds to picking some example (uniformly at random) from
       your data set. Viewed this way, the optimization problem becomes a
       stochastic optimization problem, because you are trying to optimize
       some function (say, a regularized linear classifier) over a probability
       distribution. You can derive this intepretation directly as follows:

         w∗ = arg max ∑ `(yn , w · xn ) + R(w)                              definition
                     w   n
                                                                              (12.2)
                                                       
                                               1
             = arg max ∑ `(yn , w · xn ) +       R(w)             move R inside sum
                     w   n                     N
                                                                              (12.3)
                                                             
                                 1                   1
             = arg max ∑           `(yn , w · xn ) + 2 R(w)       divide through by N
                     w   n       N                  N
                                                                              (12.4)
                                                             
                                                   1
             = arg max E(y,x)∼ D `(y, w · x) +       R(w)         write as expectation
                     w                             N
                                                                              (12.5)
             where D is the training data distribution                        (12.6)

         Given this framework, you have the following general form of an
                                                                                     efficient learning   161



Algorithm 33 StochasticGradientDescent(F , D , S, K, η1 , . . . )
 1: z(0) ← h0, 0, . . . , 0i              // initialize variable we are optimizing
 2: for k = 1 . . . K do

 3:    D(k) ← S-many random data points from D
 4:    g (k) ← ∇z F ( D(k) ) z(k-1)                // compute gradient on sample
 5:    z(k) ← z(k-1) − η (k) g (k)                // take a step down the gradient
 6: end for
              (K)
 7: return z




optimization problem:

      min   Eζ [F (z, ζ )]                                                 (12.7)
       z

In the example, ζ denotes the random choice of examples over the
dataset, z denotes the weight vector and F (w, ζ ) denotes the loss on
that example plus a fraction of the regularizer.
    Stochastic optimization problems are formally harder than regu-
lar (deterministic) optimization problems because you do not even
get access to exact function values and gradients. The only access
you have to the function F that you wish to optimize are noisy mea-
surements, governed by the distribution over ζ. Despite this lack of
information, you can still run a gradient-based algorithm, where you
simply compute local gradients on a current sample of data.
    More precisely, you can draw a data point at random from your
data set. This is analogous to drawing a single value ζ from its
distribution. You can compute the gradient of F just at that point.
In this case of a 2-norm regularized linear model, this is simply
g = ∇w `(y, w · x) + N1 w, where (y, x) is the random point you
selected. Given this estimate of the gradient (it’s an estimate because
it’s based on a single random draw), you can take a small gradient
step w ← w − ηg.
    This is the stochastic gradient descent algorithm (SGD). In prac-
tice, taking gradients with respect to a single data point might be
too myopic. In such cases, it is useful to use a small batch of data.
Here, you can draw 10 random examples from the training data
and compute a small gradient (estimate) based on those examples:
g = ∑10                            10
         m=1 ∇w `( ym , w · xm ) + N w, where you need to include 10
counts of the regularizer. Popular batch sizes are 1 (single points)
and 10. The generic SGD algorithm is depicted in Algorithm 12.2,
which takes K-many steps over batches of S-many examples.
    In stochastic gradient descent, it is imperative to choose good step
sizes. It is also very important that the steps get smaller over time at
a reasonable slow rate. In particular, convergence can be guaranteed
                                          η
for learning rates of the form: η (k) = √0 , where η0 is a fixed, initial
                                            k
step size, typically 0.01, 0.1 or 1 depending on how quickly you ex-
       162   a course in machine learning



       pect the algorithm to converge. Unfortunately, in comparisong to
       gradient descent, stochastic gradient is quite sensitive to the selection
       of a good learning rate.
          There is one more practical issues related to the use of SGD as a
       learning algorithm: do you really select a random point (or subset
       of random points) at each step, or do you stream through the data
       in order. The answer is akin to the answer of the same question for
       the perceptron algorithm (Chapter 3). If you do not permute your
       data at all, very bad things can happen. If you do permute your data
       once and then do multiple passes over that same permutation, it
       will converge, but more slowly. In theory, you really should permute
       every iteration. If your data is small enough to fit in memory, this
       is not a big deal: you will only pay for cache misses. However, if
       your data is too large for memory and resides on a magnetic disk
       that has a slow seek time, randomly seeking to new data points for
       each example is prohibitivly slow, and you will likely need to forgo
       permuting the data. The speed hit in convergence speed will almost
       certainly be recovered by the speed gain in not having to seek on disk
       routinely. (Note that the story is very different for solid state disks,
       on which random accesses really are quite efficient.)


12.3   Sparse Regularization

       For many learning algorithms, the test-time efficiency is governed
       by how many features are used for prediction. This is one reason de-
       cision trees tend to be among the fastest predictors: they only use a
       small number of features. Especially in cases where the actual com-
       putation of these features is expensive, cutting down on the number
       that are used at test time can yield huge gains in efficiency. Moreover,
       the amount of memory used to make predictions is also typically
       governed by the number of features. (Note: this is not true of kernel
       methods like support vector machines, in which the dominant cost is
       the number of support vectors.) Furthermore, you may simply believe
       that your learning problem can be solved with a very small number
       of features: this is a very reasonable form of inductive bias.
          This is the idea behind sparse models, and in particular, sparse
       regularizers. One of the disadvantages of a 2-norm regularizer for
       linear models is that they tend to never produce weights that are
       exactly zero. They get close to zero, but never hit it. To understand
       why, as a weight wd approaches zero, its gradient also approaches
       zero. Thus, even if the weight should be zero, it will essentially never
       get there because of the constantly shrinking gradient.
          This suggests that an alternative regularizer is required to yield a
       sparse inductive bias. An ideal case would be the zero-norm regular-
                                                                           efficient learning   163



izer, which simply counts the number of non-zero values in a vector:
||w||0 = ∑d [wd 6= 0]. If you could minimize this regularizer, you
would be explicitly minimizing the number of non-zero features. Un-
fortunately, not only is the zero-norm non-convex, it’s also discrete.
Optimizing it is NP-hard.
    A reasonable middle-ground is the one-norm: ||w||1 = ∑d |wd |.
It is indeed convex: in fact, it is the tighest ` p norm that is convex.
Moreover, its gradients do not go to zero as in the two-norm. Just as
hinge-loss is the tightest convex upper bound on zero-one error, the
one-norm is the tighest convex upper bound on the zero-norm.
    At this point, you should be content. You can take your subgradi-
ent optimizer for arbitrary functions and plug in the one-norm as a
regularizer. The one-norm is surely non-differentiable at wd = 0, but
you can simply choose any value in the range [−1, +1] as a subgradi-
ent at that point. (You should choose zero.)
    Unfortunately, this does not quite work the way you might expect.
The issue is that the gradient might “overstep” zero and you will
never end up with a solution that is particularly sparse. For example,
at the end of one gradient step, you might have w3 = 0.6. Your
gradient might have g6 = 0.8 and your gradient step (assuming
η = 1) will update so that the new w3 = −0.2. In the subsequent
iteration, you might have g6 = −0.3 and step to w3 = 0.1.
    This observation leads to the idea of trucated gradients. The idea
is simple: if you have a gradient that would step you over wd = 0,
then just set wd = 0. In the easy case when the learning rate is 1, this
means that if the sign of wd − gd is different than the sign of wd then
you truncate the gradient step and simply set wd = 0. In other words,
gd should never be larger than wd Once you incorporate learning
rates, you can express this as:

                if wd > 0 and gd ≤ η1(k) wd
       
        gd
       
  gd ←   gd     if wd < 0 and gd ≥ η1(k) wd                       (12.8)
       
       
         0      otherwise

This works quite well in the case of subgradient descent. It works
somewhat less well in the case of stochastic subgradient descent. The
problem that arises in the stochastic case is that wherever you choose
to stop optimizing, you will have just touched a single example (or
small batch of examples), which will increase the weights for a lot of
features, before the regularizer “has time” to shrink them back down
to zero. You will still end up with somewhat sparse solutions, but not
as sparse as they could be. There are algorithms for dealing with this
situation, but they all have a heuristic flavor to them and are beyond
the scope of this book.
       164    a course in machine learning



12.4   Feature Hashing

       As much as speed is a bottleneck in prediction, so often is memory
       usage. If you have a very large number of features, the amount of
       memory that it takes to store weights for all of them can become
       prohibitive, especially if you wish to run your algorithm on small de-
       vices. Feature hashing is an incredibly simple technique for reducing
       the memory footprint of linear models, with very small sacrifices in
       accuracy.
           The basic idea is to replace all of your features with hashed ver-
       sions of those features, thus reducing your space from D-many fea-
       ture weights to P-many feature weights, where P is the range of
       the hash function. You can actually think of hashing as a (random-
       ized) feature mapping φ : RD → RP , for some P  D. The idea
       is as follows. First, you choose a hash function h whose domain is
       [ D ] = {1, 2, . . . , D } and whose range is [ P]. Then, when you receive a
       feature vector x ∈ RD , you map it to a shorter feature vector x̂ ∈ RP .
       Algorithmically, you can think of this mapping as follows:


       1. Initialize x̂ = h0, 0, . . . , 0i


       2. For each d = 1 . . . D:


          (a) Hash d to position p = h(d)

          (b) Update the pth position by adding xd : x̂ p ← x̂ p + xd


       3. Return x̂


       Mathematically, the mapping looks like:

          φ( x) p = ∑[ h(d) = p] xd           =       ∑            xd        (12.9)
                      d                           d ∈ h −1 ( p )


       where h−1 ( p) = {d : h(d) = p}.
          In the (unrealistic) case where P = D and h simply encodes a per-
       mutation, then this mapping does not change the learning problem
       at all. All it does is rename all of the features. In practice, P  D
       and there will be collisions. In this context, a collision means that
       two features, which are really different, end up looking the same to
       the learning algorithm. For instance, “is it sunny today?” and “did
       my favorite sports team win last night?” might get mapped to the
       same location after hashing. The hope is that the learning algorithm
       is sufficiently robust to noise that it can handle this case well.
                                                                                                 efficient learning   165



         Consider the kernel defined by this hash mapping. Namely:

         K(hash) ( x, z) = φ( x) · φ(z)                                                (12.10)
                                                          !                        !
                       =∑      ∑[ h(d) = p] xd                    ∑[ h(d) = p]zd       (12.11)
                           p       d                              d

                       = ∑ ∑[h(d) = p][h(e) = p] xd ze                                 (12.12)
                           p d,e

                       =∑              ∑       xd ze                                   (12.13)
                           d e∈h−1 (h(d))

                       = x·z+∑                 ∑          xd ze                        (12.14)
                                       d      e6=d,
                                           e∈h−1 (h(d))

       This hash kernel has the form of a linear kernel plus a small number
       of quadratic terms. The particular quadratic terms are exactly those
       given by collisions of the hash function.
           There are two things to notice about this. The first is that collisions
       might not actually be bad things! In a sense, they’re giving you a
       little extra representational power. In particular, if the hash function
       happens to select out feature pairs that benefit from being paired,
       then you now have a better representation. The second is that even if
       this doesn’t happen, the quadratic term in the kernel has only a small
       effect on the overall prediction. In particular, if you assume that your
       hash function is pairwise independent (a common assumption of
       hash functions), then the expected value of this quadratic term is zero,
       and its variance decreases at a rate of O( P−2 ). In other words, if you
       choose P ≈ 100, then the variance is on the order of 0.0001.


12.5   Exercises

       Exercise 12.1. TODO. . .
                                             13 | U NSUPERVISED L EARNING
                                                                          –        Learning Objectives:
                                                                                   • Explain the difference between
                                                                                     linear and non-linear dimensionality
                                                                                     reduction.
                                                                                   • Relate the view of PCA as maximiz-
                                                                                     ing variance with the view of it as
                                                                                     minimizing reconstruction error.
                                                                                   • Implement latent semantic analysis
                                                                                     for text data.
       If you have access to labeled training data, you know what                  • Motivate manifold learning from the
                                                                                     perspective of reconstruction error.
       to do. This is the “supervised” setting, in which you have a teacher
                                                                                   • Understand K-means clustering as
       telling you the right answers. Unfortunately, finding such a teacher
                                                                                     distance minimization.
       is often difficult, expensive, or down right impossible. In those cases,
                                                                                   • Explain the importance of initial-
       you might still want to be able to analyze your data, even though you         ization in k-means and furthest-first
       do not have labels.                                                           heuristic.

          Unsupervised learning is learning without a teacher. One basic           • Implement agglomerative clustering.

       thing that you might want to do with data is to visualize it. Sadly, it     • Argue whether spectral cluster-
                                                                                     ing is a clustering algorithm or a
       is difficult to visualize things in more than two (or three) dimensions,      dimensionality reduction algorithm.
       and most data is in hundreds of dimensions (or more). Dimension-
       ality reduction is the problem of taking high dimensional data and
       embedding it in a lower dimension space. Another thing you might
                                                                                   Dependencies:
       want to do is automatically derive a partitioning of the data into
       clusters. You’ve already learned a basic approach for doing this: the
       k-means algorithm (Chapter 2). Here you will analyze this algorithm
       to see why it works. You will also learn more advanced clustering
       approaches.


13.1   K-Means Clustering, Revisited

          The K-means clustering algorithm is re-presented in Algorithm 13.1.
       There are two very basic questions about this algorithm: (1) does it
       converge (and if so, how quickly); (2) how sensitive it is to initializa-
       tion? The answers to these questions, detailed below, are: (1) yes it
       converges, and it converges very quickly in practice (though slowly
       in theory); (2) yes it is sensitive to initialization, but there are good
       ways to initialize it.
          Consider the question of convergence. The following theorem
       states that the K-Means algorithm converges, though it does not say
       how quickly it happens. The method of proving the convergence is
       to specify a clustering quality objective function, and then to show
       that the K-Means algorithm converges to a (local) optimum of that
       objective function. The particular objective function that K-Means
                                                                                        unsupervised learning   167



Algorithm 34 K-Means(D, K)
  1: for k = 1 to K do

  2:    µk ← some random location        // randomly initialize mean for kth cluster
  3: end for

  4: repeat

  5:   for n = 1 to N do
  6:       zn ← argmink ||µk − xn ||         // assign example n to closest center
  7:   end for
  8:   for k = 1 to K do
  9:       µk ← mean({ xn : zn = k })                // re-estimate mean of cluster k
 10:   end for
 11: until converged

 12: return z                                          // return cluster assignments



is optimizing is the sum of squared distances from any data point to its
assigned center. This is a natural generalization of the definition of a
mean: the mean of a set of points is the single point that minimizes
the sum of squared distances from the mean to every point in the
data. Formally, the K-Means objective is:
                                 2
   L(z, µ; D) = ∑ xn − µzn              =∑   ∑ ||xn − µk ||2                  (13.1)
                  n                      k n:zn =k

Theorem 15 (K-Means Convergence Theorem). For any dataset D and
any number of clusters K, the K-means algorithm converges in a finite num-
ber of iterations, where convergence is measured by L ceasing the change.
Proof of Theorem 15. The proof works as follows. There are only two
points in which the K-means algorithm changes the values of µ or z:
lines 6 and 9. We will show that both of these operations can never
increase the value of L. Assuming this is true, the rest of the argu-
ment is as follows. After the first pass through the data, there are
are only finitely many possible assignments to z and µ, because z is
discrete and because µ can only take on a finite number of values:
means of some subset of the data. Furthermore, L is lower-bounded
by zero. Together, this means that L cannot decrease more than a
finite number of times. Thus, it must stop decreasing at some point,
and at that point the algorithm has converged.
   It remains to show that lines 6 and 9 decrease L. For line 6, when
looking at example n, suppose that the previous value of zn is a and
the new value is b. It must be the case that || xn − µb || ≤ || xn − µb ||.
Thus, changing from a to b can only decrease L. For line 9, consider
the second form of L. Line 9 computes µk as the mean of the data
points for which zn = k, which is precisely the point that minimizes
squared sitances. Thus, this update to µk can only decrease L.

   There are several aspects of K-means that are unfortunate. First,
the convergence is only to a local optimum of L. In practice, this
168   a course in machine learning



means that you should usually run it 10 times with different initial-
izations and pick the one with minimal resulting L. Second, one
can show that there are input datasets and initializations on which
it might take an exponential amount of time to converge. Fortu-
nately, these cases almost never happen in practice, and in fact it has
recently been shown that (roughly) if you limit the floating point pre-
cision of your machine, K-means will converge in polynomial time
(though still only to a local optimum), using techniques of smoothed
analysis.
   The biggest practical issue in K-means is initialization. If the clus-
ter means are initialized poorly, you often get convergence to uninter-
esting solutions. A useful heuristic is the furthest-first heuristic. This
gives a way to perform a semi-random initialization that attempts to
pick initial means as far from each other as possible. The heuristic is
sketched below:

1. Pick a random example m and set µ1 = xm .

2. For k = 2 . . . K:

   (a) Find the example m that is as far as possible from all previ-
      ously selected means; namely: m = arg maxm mink0 <k || xm − µk0 ||2
      and set µk = xm

In this heuristic, the only bit of randomness is the selection of the
first data point. After that, it is completely deterministic (except in
the rare case that there are multiple equidistant points in step 2a). It
is extremely important that when selecting the 3rd mean, you select
that point that maximizes the minimum distance to the closest other
mean. You want the point that’s as far away from all previous means
as possible.
   The furthest-first heuristic is just that: a heuristic. It works very
well in practice, though can be somewhat sensitive to outliers (which
will often get selected as some of the initial means). However, this
outlier sensitivity is usually reduced after one iteration through the
K-means algorithm. Despite being just a heuristic, it is quite useful in
practice.
   You can turn the heuristic into an algorithm by adding a bit more
randomness. This is the idea of the K-means++ algorithm, which
is a simple randomized tweak on the furthest-first heuristic. The
idea is that when you select the kth mean, instead of choosing the
absolute furthest data point, you choose a data point at random, with
probability proportional to its distance squared. This is made formal
in Algorithm 13.1.
   If you use K-means++ as an initialization for K-means, then you
are able to achieve an approximation guarantee on the final value
                                                                                       unsupervised learning   169



Algorithm 35 K-Means ++(D, K)
 1: µ ← x m for m chosen uniformly at random      // randomly initialize first point
     1
 2: for k = 2 to K do

 3:    dn ← mink0 <k || xn − µk0 ||2 , ∀n                   // compute distances
 4:    p ← ∑ 1nd d                         // normalize to probability distribution
              n
 5:    m ← random sample from p                    // pick an example at random
 6:    µk ← xm
 7: end for

 8: run K-Means using µ as initial centers




of the objective. This doesn’t tell you that you will reach the global
optimum, but it does tell you that you will get reasonably close. In
particular, if L̂ is the value obtained by running K-means++, then this
will not be “too far” from L(opt) , the true global minimum.

Theorem 16 (K-means++ Approximation Guarantee). The expected
value of the objective returned by K-means++ is never more than O(log K )
from optimal and can be as close as O(1) from optimal. Even in the former
case, with 2K random restarts, one restart will be O(1) from optimal (with
high probability). Formally: E L̂ ≤ 8(log K + 2)L(opt) . Moreover, if the
                                 

data is “well suited” for clustering, then E L̂ ≤ O(1)L(opt) .
                                             

   The notion of “well suited” for clustering informally states that
the advantage of going from K − 1 clusters to K clusters is “large.”
Formally, it means that LK (opt) ≤ e2 LK −1 (opt) , where LK (opt) is the
optimal value for clustering with K clusters, and e is the desired
degree of approximation. The idea is that if this condition does not
hold, then you shouldn’t bother clustering the data.
   One of the biggest practical issues with K-means clustering is
“choosing K.” Namely, if someone just hands you a dataset and
asks you to cluster it, how many clusters should you produce? This
is difficult, because increasing K will always decrease LK (opt) (until
K > N), and so simply using L as a notion of goodness is insuffi-
cient (analogous to overfitting in a supervised setting). A number
of “information criteria” have been proposed to try to address this
problem. They all effectively boil down to “regularizing” K so that
the model cannot grow to be too complicated. The two most popular
are the Bayes Information Criteria (BIC) and the Akaike Information
Criteria (AIC), defined below in the context of K-means:

   BIC:    arg min L̂K + K log D                                              (13.2)
                 K
   AIC:    arg min L̂K + 2KD                                                  (13.3)
                 K

The informal intuition behind these criteria is that increasing K is
going to make LK go down. However, if it doesn’t go down “by
enough” then it’s not worth doing. In the case of BIC, “by enough”
       170    a course in machine learning



       means by an amount proportional to log D; in the case of AIC, it’s
       proportional to 2D. Thus, AIC provides a much stronger penalty for
       many clusters than does BIC, especially in high dimensions.
           A more formal intuition for BIC is the following. You ask yourself
       the question “if I wanted to send this data across a network, how
       many bits would I need to send?” Clearly you could simply send
       all of the N examples, each of which would take roughly log D bits
       to send. This gives N log D to send all the data. Alternatively, you
       could first cluster the data and send the cluster centers. This will take
       K log D bits. Then, for each data point, you send its center as well as
       its deviation from that center. It turns out this will cost exactly L̂K
       bits. Therefore, the BIC is precisely measuring how many bits it will
       take to send your data using K clusters. The K that minimizes this
       number of bits is the optimal value.




13.2   Linear Dimensionality Reduction

       Dimensionality reduction is the task of taking a dataset in high di-
       mensions (say 10000) and reducing it to low dimensions (say 2) while
       retaining the “important” characteristics of the data. Since this is
       an unsupervised setting, the notion of important characteristics is
       difficult to define.
           Consider the dataset in Figure ??, which lives in high dimensions
       (two) and you want to reduce to low dimensions (one). In the case
       of linear dimensionality reduction, the only thing you can do is to
       project the data onto a vector and use the projected distances as the
       embeddings. Figure ?? shows a projection of this data onto the vector
       that points in the direction of maximal variance of the original dataset.
       Intuitively, this is a reasonable notion of importance, since this is the
       direction in which most information is encoded in the data.
           For the rest of this section, assume that the data is centered:
       namely, the mean of all the data is at the origin. (This will sim-
       ply make the math easier.) Suppose the two dimensional data is
       x1 , . . . , x N and you’re looking for a vector u that points in the direc-
       tion of maximal variance. You can compute this by projecting each
       point onto u and looking at the variance of the result. In order for the
       projection to make sense, you need to constrain ||u||2 = 1. In this
       case, the projections are x1 , u·, . . . , x N , u·. Call these values p1 , . . . , p N .
           The goal is to compute the variance of the { pn }s and then choose
       u to maximize this variance. To compute the variance, you first need
       to compute the mean. Because the mean of the xn s was zero, the
                                                                            unsupervised learning   171



   M ATH R EVIEW | E IGENVALUES AND EIGENVECTORS
  the usual...


                                                                            Figure 13.1:

mean of the ps is also zero. This can be seen as follows:
                              !
  ∑ pn = ∑ xn · u = ∑ xn              ·u = 0·u = 0                 (13.4)
   n           n            n


The variance of the { pn } is then just ∑n p2n . Finding the optimal
u (from the perspective of variance maximization) reduces to the
following optimization problem:

  max
    u
          ∑ ( x n · u )2   subj. to    ||u||2 = 1                  (13.5)
           n

In this problem it becomes apparent why keeping u unit length is
important: if not, u would simply stretch to have infinite length to
maximize the objective.
   It is now helpful to write the collection of datapoints xn as a N×
D matrix X. If you take this matrix X and multiply it by u, which
has dimensions D×1, you end up with a N×1 vector whose values
are exactly the values p. The objective in Eq (13.5) is then just the
squared norm of p. This simplifies Eq (??) to:

  max     ||Xu||2    subj. to    ||u||2 − 1 = 0                    (13.6)
    u

where the constraint has been rewritten to make it amenable to con-
structing the Lagrangian. Doing so and taking gradients yields:
                                  
  L(u, λ) = ||Xu||2 − λ ||u||2 − 1                                 (13.7)

       ∇u L = 2X> Xu − 2λu                                         (13.8)
                       
       =⇒ λu = X> X u                                              (13.9)

You can solve this expression (λu = X> Xu) by computing the first
eigenvector and eigenvalue of the matrix X> X.
   This gives you the solution to a projection into a one-dimensional
space. To get a second dimension, you want to find a new vector v on
which the data has maximal variance. However, to avoid redundancy,
you want v to be orthogonal to u; namely u · v = 0. This gives:

  max     ||Xv||2    subj. to    ||v||2 = 1, and u · v = 0        (13.10)
    v

Following the same procedure as before, you can construct a La-
172      a course in machine learning



Algorithm 36 PCA(D, K)
 1: µ ← mean(X)                                       // compute data mean for centering
                                   
 2:   D←     X − µ1>    >   X − µ1>           // compute covariance, 1 is a vector of ones
 3: {λk , uk } ← top K eigenvalues/eigenvectors of D
 4: return (X − µ1) U                                               // project data using U




grangian and differentiate:
                                             
      L(v, λ1 , λ2 ) = ||Xv||2 − λ1 ||u||2 − 1 − λ2 u · v                         (13.11)

             ∇u L = 2X> Xv − 2λ1 v − 2λ2 u                                        (13.12)
                              
             =⇒ λ1 v = X> X v − λ2 u                                              (13.13)


However, you know that u is the first eigenvector of X> X, so the
solution to this problem for λ1 and v is given by the second eigen-
value/eigenvector pair of X> X.
    Repeating this analysis inductively tells you that if you want to
project onto K mutually orthogonal dimensions, you simply need to
take the first K eigenvectors of the matrix X> X. This matrix is often
called the data covariance matrix because [X> X]i,j = ∑n ∑m xn,i xm,j ,
which is the sample covariance between features i and j.
    This leads to the technique of principle components analysis,
or PCA. For completeness, the is depicted in Algorithm ??. The
important thing to note is that the eigenanalysis only gives you
the projection directions. It does not give you the embedded data.
To embed a data point x you need to compute its embedding as
h x · u1 , x · u2 , . . . , x · uK i. If you write U for the D×K matrix of us, then
this is just XU.
    There is an alternative derivation of PCA that can be informative,
based on reconstruction error. Consider the one-dimensional case
again, where you are looking for a single projection direction u. If
you were to use this direction, your projected data would be Z = Xu.
Each Zn gives the position of the nth datapoint along u. You can
project this one-dimensional data back into the original space by
multiplying it by u> . This gives you reconstructed values Zu> . Instead
of maximizing variance, you might instead want to minimize the
reconstruction error, defined by:

                  2                       2
       X − Zu>        = X − Xuu>                                                         definition of Z
                                                                                  (13.14)
                                               2
                      = ||X||2 + Xuu>              − 2X> Xuu>                            quadratic rule
                                                                                  (13.15)
                                                                                      unsupervised learning   173


                                           2
                       = ||X||2 + Xuu>         − 2u> X> Xu                     quadratic rule
                                                                        (13.16)
                       = ||X||2 + ||X||2 − 2u> X> Xu                       u is a unit vector
                                                                        (13.17)
                       = C − 2 ||Xu||2                       join constants, rewrite last term
                                                                        (13.18)

       Minimizing this final term is equivalent to maximizing ||Xu||2 , which
       is exactly the form of the maximum variance derivation of PCA.
       Thus, you can see that maximizing variance is identical to minimiz-
       ing reconstruction error.
          The same question of “what should K be” arises in dimension-
       ality reduction as in clustering. If the purpose of dimensionality
       reduction is to visualize, then K should be 2 or 3. However, an alter-
       native purpose of dimensionality reduction is to avoid the curse of
       dimensionality. For instance, even if you have labeled data, it might
       be worthwhile to reduce the dimensionality before applying super-
       vised learning, essentially as a form of regularization. In this case,
       the question of an optimal K comes up again. In this case, the same
       criteria (AIC and BIC) that can be used for clustering can be used for
       PCA. The only difference is the quality measure changes from a sum
       of squared distances to means (for clustering) to a sum of squared
       distances to original data points (for PCA). In particular, for BIC you
       get the reconstruction error plus K log D; for AIC, you get the recon-
       struction error plus 2KD.


13.3   Manifolds and Graphs

       what is a manifold?
         graph construction


13.4   Non-linear Dimensionality Reduction

       isomap
          lle
          mvu
          mds?


13.5   Non-linear Clustering: Spectral Methods

       what is a spectrum
         spectral clustering
       174   a course in machine learning



13.6   Exercises

       Exercise 13.1. TODO. . .
                                        14 | E XPECTATION M AXIMIZATION
                                                                           –       Learning Objectives:
                                                                                   • Explain the relationship between
                                                                                     parameters and hidden variables.
                                                                                   • Construct generative stories for
                                                                                     clustering and dimensionality
                                                                                     reduction.
                                                                                   • Draw a graph explaining how EM
                                                                                     works by constructing convex lower
                                                                                     bounds.

       Suppose you were building a naive Bayes model for a text cate-              • Implement EM for clustering with
                                                                                     mixtures of Gaussians, and contrast-
       gorization problem. After you were done, your boss told you that it           ing it with k-means.
       became prohibitively expensive to obtain labeled data. You now have         • Evaluate the differences betweem
       a probabilistic model that assumes access to labels, but you don’t            EM and gradient descent for hidden
                                                                                     variable models.
       have any labels! Can you still do something?
          Amazingly, you can. You can treat the labels as hidden variables,
       and attempt to learn them at the same time as you learn the param-
       eters of your model. A very broad family of algorithms for solving
       problems just like this is the expectation maximization family. In this
       chapter, you will derive expectation maximization (EM) algorithms
       for clustering and dimensionality reduction, and then see why EM
       works.                                                                      Dependencies:



14.1   Clustering with a Mixture of Gaussians

       In Chapter 7, you learned about probabilitic models for classification
       based on density estimation. Let’s start with a fairly simple classifica-
       tion model that assumes we have labeled data. We will shortly remove
       this assumption. Our model will state that we have K classes, and
       data from class k is drawn from a Gaussian with mean µk and vari-
       ance σk2 . The choice of classes is parameterized by θ. The generative
       story for this model is:

       1. For each example n = 1 . . . N:

         (a) Choose a label yn ∼ Disc(θ)

         (b) Choose example xn ∼ Nor(µyn , σy2n )

       This generative story can be directly translated into a likelihood as
       before:

         p( D ) = ∏ Mult(yn | θ)Nor( xn | µyn , σy2n )                    (14.1)
                   n
176   a course in machine learning


                                            for each example
            z                                       }| "                              #{
                              h             i− D           1                      2
          =∏
                                                2
                      θyn          2πσy2n           exp − 2            xn − µyn            (14.2)
              n      |{z}                                2σyn
                  choose label |                        {z                            }
                                                    choose feature values

If you had access to labels, this would be all well and good, and
you could obtain closed form solutions for the maximum likelihood
estimates of all parameters by taking a log and then taking gradients
of the log likelihood:

   θk = fraction of training examples in class k                                           (14.3)
          1
          N∑
      =       [yn = k ]
            n
  µk = mean of training examples in class k                                                (14.4)
          ∑n [yn = k ] xn
      =
           ∑n [yn = k ]
  σk2 = variance of training examples in class k                                           (14.5)
          ∑n [yn = k] || xn − µk ||
      =
                ∑n [yn = k ]
  Suppose that you don’t have labels. Analogously to the K-means                                        You should be able to derive the
algorithm, one potential solution is to iterate. You can start off with                              ? maximum likelihood solution re-
                                                                                                        sults formally by now.
guesses for the values of the unknown variables, and then iteratively
improve them over time. In K-means, the approach was the assign
examples to labels (or clusters). This time, instead of making hard
assignments (“example 10 belongs to cluster 4”), we’ll make soft as-
signments (“example 10 belongs half to cluster 4, a quarter to cluster
2 and a quarter to cluster 5”). So as not to confuse ourselves too
much, we’ll introduce a new variable, zn = hzn,1 , . . . , zn,K (that sums
to one), to denote a fractional assignment of examples to clusters.
   This notion of soft-assignments is visualized in Figure 14.1. Here,
we’ve depicted each example as a pie chart, and it’s coloring denotes
the degree to which it’s been assigned to each (of three) clusters. The
size of the pie pieces correspond to the zn values.
   Formally, zn,k denotes the probability that example n is assigned to
cluster k:

  zn,k = p(yn = k | xn )                                                                   (14.6)
         p(yn = k, xn )
       =                                                                                   (14.7)
            p( xn )
          1
       =    Mult(k | θ)Nor( xn | µk , σk2 )                                                (14.8)   Figure 14.1: em:piecharts: A figure
         Zn                                                                                         showing pie charts

Here, the normalizer Zn is to ensure that zn sums to one.
   Given a set of parameters (the θs, µs and σ2 s), the fractional as-
signments zn,k are easy to compute. Now, akin to K-means, given
                                                                                               expectation maximization   177



Algorithm 37 GMM(X, K)
 1: for k = 1 to K do

 2:    µk ← some random location          // randomly initialize mean for kth cluster
 3:    σk2 ← 1                                                  // initialize variances
 4:    θk ← 1/K                                  // each cluster equally likely a priori
 5: end for

 6: repeat

 7:   for n = 1 to N do
 8:       for k = 1 to K do
                            − D  h                       i
             zn,k ← θk 2πσk2 2 exp − 2σ1 2 || xn − µk ||2
                       
 9:                                                                           // compute
                                               k
                (unnormalized) fractional assignments
 10:        end for
 11:         zn ← ∑ 1zn,k zn                            // normalize fractional assignments
                     k
 12:      end for
 13:      for k = 1 to K do
 14:         θk ← N1 ∑n zn,k                   // re-estimate prior probability of cluster k
              µk ← ∑∑n n,k
                        z   x
                            n
 15:
                       zn,k                               // re-estimate mean of cluster k
                        n

              σk2 ← ∑n n,k
                      z || xn −µk ||
 16:
                       ∑n zn,k
                                                        // re-estimate variance of cluster k
 17:   end for
 18: until converged

 19: return z                                                 // return cluster assignments



fractional assignments, you need to recompute estimates of the
model parameters. In analogy to the maximum likelihood solution
(Eqs (??)-(??)), you can do this by counting fractional points rather
than full points. This gives the following re-estimation updates:

       θk = fraction of training examples in class k                                 (14.9)
              1
              N∑
          =       zn,k
                n
       µk = mean of fractional examples in class k                                 (14.10)
              ∑n zn,k xn
          =
               ∑n zn,k
       σk2 = variance of fractional examples in class k                            (14.11)
              ∑n zn,k || xn − µk ||
          =
                   ∑n zn,k

All that has happened here is that the hard assignments “[yn = k]”
have been replaced with soft assignments “zn,k ”. As a bit of fore-
shadowing of what is to come, what we’ve done is essentially replace
known labels with expected labels, hence the name “expectation maxi-
mization.”
   Putting this together yields Algorithm 14.1. This is the GMM
(“Gaussian Mixture Models”) algorithm, because the probabilitic
model being learned describes a dataset as being drawn from a mix-
ture distribution, where each component of this distribution is a
       178   a course in machine learning



       Gaussian.                                                                                   Aside from the fact that GMMs
          Just as in the K-means algorithm, this approach is succeptible to                        use soft assignments and K-means
       local optima and quality of initialization. The heuristics for comput-                  ?   uses hard assignments, there are
                                                                                                   other differences between the two
       ing better initializers for K-means are also useful here.                                   approaches. What are they?



14.2   The Expectation Maximization Framework

       At this point, you’ve seen a method for learning in a particular prob-
       abilistic model with hidden variables. Two questions remain: (1) can
       you apply this idea more generally and (2) why is it even a reason-
       able thing to do? Expectation maximization is a family of algorithms
       for performing maximum likelihood estimation in probabilistic mod-
       els with hidden variables.
           The general flavor of how we will proceed is as follows. We want
       to maximize the log likelihood L, but this will turn out to be diffi-
       cult to do directly. Instead, we’ll pick a surrogate function L̃ that’s a
       lower bound on L (i.e., L̃ ≤ L everywhere) that’s (hopefully) easier
       to maximize. We’ll construct the surrogate in such a way that increas-
       ing it will force the true likelihood to also go up. After maximizing
       L̃, we’ll construct a new lower bound and optimize that. This process
       is shown pictorially in Figure 14.2.
                                                                                              Figure 14.2: em:lowerbound: A figure
           To proceed, consider an arbitrary probabilistic model p( x, y | θ),                showing successive lower bounds
       where x denotes the observed data, y denotes the hidden data and
       θ denotes the parameters. In the case of Gaussian Mixture Models,
       x was the data points, y was the (unknown) labels and θ included
       the cluster prior probabilities, the cluster means and the cluster vari-
       ances. Now, given access only to a number of examples x1 , . . . , x N ,
       you would like to estimate the parameters (θ) of the model.
           Probabilistically, this means that some of the variables are un-
       known and therefore you need to marginalize (or sum) over their
       possible values. Now, your data consists only of X = h x1 , x2 , . . . , x N i,
       not the ( x, y) pairs in D. You can then write the likelihood as:

          p(X | θ) = ∑ ∑ · · · ∑ p(X, y1 , y2 , . . . y N | θ)              marginalization
                       y1 y2     yN
                                                                              (14.12)
                    = ∑ ∑ · · · ∑ ∏ p( xn , yn | θ)              examples are independent
                       y1 y2     yN n
                                                                              (14.13)
                    = ∏ ∑ p( xn , yn | θ)                                          algebra
                        n yn
                                                                              (14.14)

       At this point, the natural thing to do is to take logs and then start
       taking gradients. However, once you start taking logs, you run into a
                                                                                   expectation maximization             179



problem: the log cannot eat the sum!

   L(X | θ) = ∑ log ∑ p( xn , yn | θ)                                    (14.15)
                 n       yn

Namely, the log gets “stuck” outside the sum and cannot move in to
decompose the rest of the likelihood term!
   The next step is to apply the somewhat strange, but strangely
useful, trick of multiplying by 1. In particular, let q(·) be an arbitrary
probability distribution. We will multiply the p(. . . ) term above by
q(yn )/q(yn ), a valid step so long as q is never zero. This leads to:

                                 p( xn , yn | θ)
   L(X | θ) = ∑ log ∑ q(yn )                                             (14.16)
                 n       yn          q(yn )

We will now construct a lower bound using Jensen’s inequality.
This is a very useful (and easy to prove!) result that states that
f (∑i λi xi ) ≥ ∑i λi f ( xi ), so long as (a) λi ≥ 0 for all i, (b) ∑i λi = 1,
and (c) f is concave. If this looks familiar, that’s just because it’s a
direct result of the definition of concavity. Recall that f is concave if
f ( ax + by) ≥ a f ( x ) + b f ( x ) whenever a + b = 1.                                 Prove Jensen’s inequality using the
    You can now apply Jensen’s inequality to the log likelihood by                    ? definition of concavity and induc-
                                                                                         tion.
identifying the list of q(yn )s as the λs, log as f (which is, indeed,
concave) and each “x” as the p/q term. This yields:

                                  p( xn , yn | θ)
   L(X | θ) ≥ ∑ ∑ q(yn ) log                                             (14.17)
                 n yn                 q(yn )
                  h                                               i
             = ∑ ∑ q(yn ) log p( xn , yn | θ) − q(yn ) log q(yn )        (14.18)
                 n yn

             , L̃(X | θ)                                                 (14.19)

Note that this inequality holds for any choice of function q, so long as
its non-negative and sums to one. In particular, it needn’t even by the
same function q for each n. We will need to take advantage of both of
these properties.
    We have succeeded in our first goal: constructing a lower bound
on L. When you go to optimize this lower bound for θ, the only part
that matters is the first term. The second term, q log q, drops out as a
function of θ. This means that the the maximization you need to be
able to compute, for fixed qn s, is:

   θ(new) ← arg max ∑ ∑ qn (yn ) log p( xn , yn | θ)                     (14.20)
                     θ   n yn

This is exactly the sort of maximization done for Gaussian mixture
models when we recomputed new means, variances and cluster prior
probabilities.
       180   a course in machine learning



          The second question is: what should qn (·) actually be? Any rea-
       sonable q will lead to a lower bound, so in order to choose one q over
       another, we need another criterion. Recall that we are hoping to max-
       imize L by instead maximizing a lower bound. In order to ensure
       that an increase in the lower bound implies an increase in L, we need
       to ensure that L(X | θ) = L̃(X | θ). In words: L̃ should be a lower
       bound on L that makes contact at the current point, θ. This is shown
       in Figure ??, including a case where the lower bound does not make
       contact, and thereby does not guarantee an increase in L with an
       increase in L̃.


14.3   EM versus Gradient Descent

       computing gradients through marginals
         step size


14.4   Dimensionality Reduction with Probabilistic PCA

       derivation
         advantages over pca


14.5   Exercises

       Exercise 14.1. TODO. . .
                                     15 | S EMI -S UPERVISED L EARNING
                                                                         –       Learning Objectives:
                                                                                 • Explain the cluster assumption for
                                                                                   semi-supervised discriminative
                                                                                   learning, and why it is necessary.
                                                                                 • Dervive an EM algorithm for
                                                                                   generative semi-supervised text
                                                                                   categorization.
                                                                                 • Compare and contrast the query by
                                                                                   uncertainty and query by committee
                                                                                   heuristics for active learning.
       You may find yourself in a setting where you have access to some
       labeled data and some unlabeled data. You would like to use the
       labeled data to learn a classifier, but it seems wasteful to throw out
       all that unlabeled data. The key question is: what can you do with
       that unlabeled data to aid learning? And what assumptions do we
       have to make in order for this to be helpful?
          One idea is to try to use the unlabeled data to learn a better deci-
       sion boundary. In a discriminative setting, you can accomplish this
       by trying to find decision boundaries that don’t pass too closely to
       unlabeled data. In a generative setting, you can simply treat some of
       the labels as observed and some as hidden. This is semi-supervised
       learning. An alternative idea is to spend a small amount of money to      Dependencies:
       get labels for some subset of the unlabeled data. However, you would
       like to get the most out of your money, so you would only like to pay
       for labels that are useful. This is active learning.


15.1   EM for Semi-Supervised Learning

       naive bayes model


15.2   Graph-based Semi-Supervised Learning

       key assumption
         graphs and manifolds
         label prop


15.3   Loss-based Semi-Supervised Learning

       density assumption
         loss function
         non-convex
       182   a course in machine learning



15.4   Active Learning

       motivation
         qbc
         qbu


15.5   Dangers of Semi-Supervised Learing

       unlab overwhelms lab
         biased data from active


15.6   Exercises

       Exercise 15.1. TODO. . .
                                  16 | G RAPHICAL M ODELS
                                           Learning Objectives:
                                           • foo




16.1   Exercises

       Exercise 16.1. TODO. . .




                                             Dependencies: None.
                                                             17 | O NLINE L EARNING
                                                                               Learning Objectives:
                                                                               • Explain the experts model, and why
                                                                                 it is hard even to compete with the
                                                                                 single best expert.
                                                                               • Define what it means for an online
                                                                                 learning algorithm to have no regret.
                                                                               • Implement the follow-the-leader
                                                                                 algorithm.
                                                                               • Categorize online learning algo-
                                                                                 rithms in terms of how they measure
       All of the learning algorithms that you know about at this                changes in parameters, and how
       point are based on the idea of training a model on some data, and         they measure error.
       evaluating it on other data. This is the batch learning model. How-
       ever, you may find yourself in a situation where students are con-
       stantly rating courses, and also constantly asking for recommenda-
       tions. Online learning focuses on learning over a stream of data, on
       which you have to make predictions continually.
          You have actually already seen an example of an online learning
       algorithm: the perceptron. However, our use of the perceptron and
       our analysis of its performance have both been in a batch setting. In
       this chapter, you will see a formalization of online learning (which
       differs from the batch learning formalization) and several algorithms     Dependencies:
       for online learning with different properties.



17.1   Online Learning Framework

       regret
          follow the leader
          agnostic learning
          algorithm versus problem



17.2   Learning with Features

       change but not too much
         littlestone analysis for gd and egd



17.3   Passive Agressive Learning

       pa algorithm
         online analysis
                                                   online learning   185



17.4   Learning with Lots of Irrelevant Features

       winnow
         relationship to egd


17.5   Exercises

       Exercise 17.1. TODO. . .
                                  18 | S TRUCTURED L EARNING TASKS
                                                      –   Learning Objectives:
                                                          • TODO. . .




           - Hidden Markov models: viterbi
           - Hidden Markov models: forward-backward
           - Maximum entropy Markov models
           - Structured perceptronn
           - Conditional random fields
           - M3Ns




18.1   Exercises
                                                          Dependencies:
       Exercise 18.1. TODO. . .
                                  19 | BAYESIAN L EARNING
                                           Learning Objectives:
                                           • TODO. . .




19.1   Exercises

       Exercise 19.1. TODO. . .




                                             Dependencies:
                                                C ODE AND DATASETS




Rating   Easy?   AI?   Sys?   Thy?   Morning?
  +2       y      y     n      y        n
  +2       y      y     n      y        n
  +2       n      y     n      n        n
  +2       n      n     n      y        n
  +2       n      y     y      n        y
  +1       y      y     n      n        n
  +1       y      y     n      y        n
  +1       n      y     n      y        n
   0       n      n     n      n        y
   0       y      n     n      y        y
   0       n      y     n      y        n
   0       y      y     y      y        y
  -1       y      y     y      n        y
  -1       n      n     y      y        n
  -1       n      n     y      n        y
  -1       y      n     y      n        y
  -2       n      n     y      y        n
  -2       n      y     y      n        y
  -2       y      n     y      n        n
  -2       y      n     y      n        y
N OTATION
                                                                       B IBLIOGRAPHY




Sergey Brin. Near neighbor search in large metric spaces. In Confer-
ence on Very Large Databases (VLDB), 1995.

Tom M. Mitchell. Machine Learning. McGraw Hill, 1997.

Frank Rosenblatt. The perceptron: A probabilistic model for infor-
mation storage and organization in the brain. Psychological Review,
65:386–408, 1958. Reprinted in Neurocomputing (MIT Press, 1998).
                                                                                                  I NDEX




K-nearest neighbors, 56              circuit complexity, 125                example normalization, 57, 58
e-ball, 37                           clustering, 32, 166                    examples, 9
p-norms, 91                          clustering quality, 166                expectation maximization, 175
0/1 loss, 87                         collective classification, 83          expected loss, 15, 16
                                     complexity, 31                         exponential loss, 89, 157
absolute loss, 14                    concave, 89
activation function, 117             concavity, 179                         feasible region, 99
activations, 39                      concept, 144                           feature combinations, 51
active learning, 181                 confidence intervals, 66               feature mapping, 51
AdaBoost, 154                        constrained optimization problem, 98   feature normalization, 57
algorithm, 86                        contour, 91                            feature scale, 30
all pairs, 76                        convergence rate, 94                   feature space, 28
all versus all, 76                   convex, 86, 88                         feature values, 11, 26
architecture selection, 126          cross validation, 62, 66               feature vector, 26, 28
area under the curve, 62, 81         cubic feature map, 131                 features, 11, 26
AUC, 62, 80, 81                      curvature, 94                          forward-propagation, 124
AVA, 76                                                                     fractional assignments, 176
averaged perceptron, 49              data covariance matrix, 172            furthest-first heuristic, 168
                                     data generating distribution, 15
back-propagation, 121, 124           decision boundary, 31                  Gaussian distribution, 109
bag of words, 54                     decision stump, 156                    Gaussian kernel, 134
bagging, 153                         decision tree, 8, 10                   Gaussian Mixture Models, 177
base learner, 152                    decision trees, 55                     generalize, 9, 16
batch, 161                           development data, 24                   generative story, 110
batch learning, 184                  dimensionality reduction, 166          geometric view, 26
Bayes error rate, 104, 150           discrete distribution, 109             global minimum, 92
Bayes optimal classifier, 103, 150   distance, 28                           GMM, 177
Bayes optimal error rate, 104        dominates, 61                          gradient, 92
Bernouilli distribution, 108         dot product, 43                        gradient ascent, 92
bias, 40                             dual problem, 138                      gradient descent, 92
binary features, 27                  dual variables, 138                    graph, 83
bipartite ranking problems, 79
boosting, 142, 152                   early stopping, 51, 120                hard-margin SVM, 99
bootstrap resampling, 153            embedding, 166                         hash kernel, 165
bootstrapping, 65, 67                ensemble, 152                          held-out data, 24
                                     error driven, 41                       hidden units, 116
categorical features, 27             error rate, 87                         hidden variables, 175
chain rule, 107                      Euclidean distance, 28                 hinge loss, 89
chord, 89                            evidence, 114                          histogram, 12
192   a course in machine learning



hyperbolic tangent, 117               log-likelihood ratio, 109            polynomial kernels, 133
hypercube, 35                         logarithmic transformation, 59       positive semi-definite, 133
hyperparameter, 23, 42, 88            logistic loss, 89                    posterior, 114
hyperplane, 39                        logistic regression, 113             precision, 60
hyperspheres, 35                      LOO cross validation, 63             precision/recall curves, 60
hypothesis, 144                       loss function, 14                    predict, 9
hypothesis class, 147                                                      preference function, 78
hypothesis testing, 65                margin, 46, 98                       primal variables, 138
                                      margin of a data set, 46             principle components analysis, 172
i.i.d. assumption, 105                marginal likelihood, 114             prior, 114
imbalanced data, 70                   maximum a posteriori, 114            probabilistic modeling, 103
importance weight, 71                 maximum depth, 23                    Probably Approximately Correct, 143
independently, 104                    maximum likelihood estimation, 105   projected gradient, 138
independently and identically dis-    Mercer’s condition, 133              psd, 133
      tributed, 105                   model, 86
indicator function, 87                modeling, 22                         radial basis function, 126
induce, 15                            multi-layer network, 116             random forests, 157
induced distribution, 72                                                   RBF kernel, 134
induction, 9                          naive Bayes assumption, 107          RBF network, 126
inductive bias, 18, 28, 30, 90, 109   nearest neighbor, 26, 28             recall, 60
iteration, 32                         neural network, 157                  receiver operating characteristic, 62
                                      neural networks, 52, 116             reconstruction error, 172
jack-knifing, 67                      neurons, 39                          reductions, 72
Jensen’s inequality, 179              noise, 19                            redundant features, 54
joint, 111                            non-convex, 122                      regularized objective, 88
                                      non-linear, 116                      regularizer, 87, 90
K-nearest neighbors, 29               Normal distribution, 109             representer theorem, 130, 132
Karush-Kuhn-Tucker conditions, 139    normalize, 44, 57                    ROC curve, 62
kernel, 128, 132                      null hypothesis, 65
kernel trick, 133                                                          sample complexity, 144, 145, 147
kernels, 52                           objective function, 87               semi-supervised learning, 181
KKT conditions, 139                   one versus all, 75                   sensitivity, 62
                                      one versus rest, 75                  separating hyperplane, 86
label, 11                             online, 41                           SGD, 161
Lagrange multipliers, 106             online learning, 184                 shallow decision tree, 18, 156
Lagrange variable, 106                optimization problem, 87             shape representation, 54
Lagrangian, 106                       output unit, 116                     sigmoid, 117
layer-wise, 126                       OVA, 75                              sigmoid function, 113
leave-one-out cross validation, 63    overfitting, 21                      sigmoid network, 126
level-set, 91                         oversample, 73                       sign, 117
license, 2                                                                 single-layer network, 116
likelihood, 114                       p-value, 65                          slack, 135
linear classifier, 157                PAC, 143, 154                        slack parameters, 99
linear classifiers, 157               paired t-test, 65                    smoothed analysis, 168
linear decision boundary, 39          parametric test, 65                  soft assignments, 176
linear regression, 96                 parity function, 125                 soft-margin SVM, 99
linearly separable, 45                patch representation, 54             span, 130
link function, 117                    PCA, 172                             sparse, 91
log likelihood, 106                   perceptron, 39, 40, 56               specificity, 62
log posterior, 114                    perpendicular, 43                    squared loss, 14, 89
log probability, 106                  pixel representation, 54             stacking, 84
                                                                                             index    193



StackTest, 84                      t-test, 65                        validation data, 24
statistical inference, 103         test data, 22                     Vapnik-Chernovenkis dimension, 149
statistically significant, 65      test error, 22                    variance, 153
stochastic gradient descent, 161   test set, 9                       VC dimension, 149
stochastic optimization, 160       text categorization, 54           vector, 28
strong learner, 154                the curse of dimensionality, 34   visualize, 166
strong learning algorithm, 154     threshold, 40                     vote, 29
strongly convex, 94                Tikhonov regularization, 86       voted perceptron, 49
structural risk minimization, 86   training data, 9, 15, 22          voting, 49
sub-sampling, 72                   training error, 16
subderivative, 95                  trucated gradients, 163
                                                                     weak learner, 154
subgradient, 95                    two-layer network, 116
                                                                     weak learning algorithm, 154
subgradient descent, 96
                                                                     weighted nearest neighbors, 37
support vector machine, 98         unbiased, 45
                                                                     weights, 39
support vectors, 140               underfitting, 21
surrogate loss, 89                 unit hypercube, 36
symmetric modes, 122               unsupervised learning, 32         zero/one loss, 14
